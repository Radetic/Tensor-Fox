{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse.linalg as ssl\n",
    "from scipy import sparse\n",
    "from numba import jit, njit, prange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the problem of approximating a tensor $T \\in \\mathbb{R}^{n+1} \\otimes \\mathbb{R}^{n+1} \\otimes \\mathbb{R}^{n+1}$ by a tensor of rank $r$ given by\n",
    "$$S = \\sum_{\\ell=1}^r \\Lambda_\\ell \\cdot X_{\\ell} \\otimes Y_{\\ell} \\otimes Z_{\\ell},$$\n",
    "where \n",
    "$$X_{\\ell} = (1, X_{\\ell_1}, \\ldots, X_{\\ell_n}),$$\n",
    "$$Y_{\\ell} = (1, Y_{\\ell_1}, \\ldots, Y_{\\ell_n}),$$\n",
    "$$Z_{\\ell} = (1, Z_{\\ell_1}, \\ldots, Z_{\\ell_n}).$$\n",
    "\n",
    "We do this by minimizing the error function\n",
    "$$\\textbf{E}(\\Lambda,X,Y,Z) = \\frac{1}{2}\\|T - S\\|^2 = \\frac{1}{2} \\sum_{i,j,k=0}^n \\left( T_{ijk} - \\sum_{\\ell=1}^r \\Lambda_\\ell \\cdot X_{\\ell_i} Y_{\\ell_j} Z_{\\ell_j} \\right)^2 = \\frac{1}{2} \\sum_{i,j,k=0}^n res_{ijk}^2(\\Lambda, X,Y,Z) = \\frac{1}{2} \\|\\textbf{res}(\\Lambda,X,Y,Z)\\|^2,$$\n",
    "where\n",
    "$$\\Lambda = (\\Lambda_1, \\ldots, \\Lambda_r),$$\n",
    "$$X = (X_1, \\ldots, X_r),$$\n",
    "$$Y = (Y_1, \\ldots, Y_r),$$\n",
    "$$Z = (Z_1, \\ldots, Z_r),$$\n",
    "and $\\textbf{res} = (res_{000}, res_{001}, \\ldots, res_{nnn})$ is the function of the residuals.\n",
    "\n",
    "In the python function called *residuals* the program constructs $\\textbf{res}(\\Lambda,X,Y,Z)$ for a given $(\\Lambda,X,Y,Z)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@njit(nogil=True,parallel=True)\n",
    "def residuals(T,Lambda,X,Y,Z,r,n):\n",
    "    \"\"\"\n",
    "    This function computes the residuals between a 3-D tensor T in R^{n+1}⊗R^{n+1}⊗R^{n+1}\n",
    "    and an approximation S of rank r. The tensor S is of the form\n",
    "    S = Lambda_1*X_1⊗Y_1⊗Z_1 + ... + Lambda_r*X_r⊗Y_r⊗Z_r, where\n",
    "    X_l = (1, X_{l_1}, ..., X_{l_n}),\n",
    "    Y_l = (1, Y_{l_1}, ..., Y_{l_n}),\n",
    "    Z_l = (1, Z_{l_1}, ..., Z_{l_n}).\n",
    "    \n",
    "    The `residual map` is a map res:R^{n+1}->R. For each i,j,k=0...n, the residual r_{i,j,k} \n",
    "    is given by res_{i,j,k} = ( T_{i,j,k} - sum_{l=1}^r Lambda_l*X_{l_i}*Y_{l_j}*Z_{l_k} )^2.\n",
    "    \n",
    "    The entries X_{l_0} = 1, Y_{l_0} = 1, Z_{l_0} = 1 are not passed to the function. Thus,\n",
    "    instead of passing X = (X_1,...,X_r), Y = (Y_1,...,Y_r), Z = (Z_1,...,Z_r) as vectors \n",
    "    with r*(n+1) entries, they are passed with r*n entries.\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    T: float 3-D ndarray\n",
    "    Lambda: float 1-D ndarray with r entries\n",
    "    X: float 1-D ndarray with r*n entries\n",
    "    Y: float 1-D ndarray with r*n entries\n",
    "    Z: float 1-D ndarray with r*n entries\n",
    "    r: int.\n",
    "        The rank of the desired approximating tensor.\n",
    "    n: int\n",
    "        The dimension of the space minus 1.\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    res: float 1-D ndarray with (n+1)**3 entries \n",
    "        Each entry is a residual.\n",
    "    \"\"\"    \n",
    "    \n",
    "    res = np.zeros((n+1)**3)\n",
    "    augX = np.zeros(r*(n+1))\n",
    "    augY = np.zeros(r*(n+1))\n",
    "    augZ = np.zeros(r*(n+1))\n",
    "    \n",
    "    #The program constructs the augmented vectors by considering the 0-th entries, which are equal to 1.\n",
    "    for l in prange(0,r):\n",
    "        augX[l*(n+1)] = 1\n",
    "        augX[l*(n+1)+1 : l*(n+1) + n+1] = X[l*n : l*n + n]\n",
    "        augY[l*(n+1)] = 1\n",
    "        augY[l*(n+1)+1 : l*(n+1) + n+1] = Y[l*n : l*n + n]\n",
    "        augZ[l*(n+1)] = 1\n",
    "        augZ[l*(n+1)+1 : l*(n+1) + n+1] = Z[l*n : l*n + n]\n",
    "        \n",
    "    #Construction of the vector r = (r_{000}, r_{001}, ..., r_{nnn}).\n",
    "    for i in prange(0,n+1):\n",
    "        for j in range(0,n+1):\n",
    "            for k in range(0,n+1):\n",
    "                res[(n+1)**2*i + (n+1)*j + k] = residuals_entries(T,Lambda,augX,augY,augZ,r,n,i,j,k)\n",
    "                \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@njit(nogil=True,cache=True)\n",
    "def residuals_entries(T,Lambda,augX,augY,augZ,r,n,i,j,k):\n",
    "    \"\"\"Computation of each individual residual in the function residuals.\"\"\"\n",
    "    \n",
    "    s = 0\n",
    "    for l in range(0,r):\n",
    "        s += Lambda[l]*augX[l*(n+1)+i]*augY[l*(n+1)+j]*augZ[l*(n+1)+k]\n",
    "        \n",
    "    res_entry = T[i,j,k] - s\n",
    "        \n",
    "    return res_entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the python function *derivative_residuals* the program constructs the Jacobian matrix $D\\textbf{res}$ of $\\textbf{res}$ at $(\\Lambda,X,Y,Z)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@njit(nogil=True,parallel=True)\n",
    "def derivative_residuals(Lambda,X,Y,Z,r,n):\n",
    "    \"\"\"\n",
    "    Computation of the nonzero entries of the Jacobian matrix Dres of the residuals \n",
    "    map at a particular point (Lambda,X,Y,Z). The matrix Dres is sparse, and that is\n",
    "    why we only keep its nonzero entries. This matrix is computed several times\n",
    "    during the program and since the coordinates corresponding to the nonzero \n",
    "    entries never changes, we compute them in another function which is called just \n",
    "    once.\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    Lambda: float 1-D ndarray with r entries\n",
    "    X: float 1-D ndarray with r*n entries\n",
    "    Y: float 1-D array with r*n entries\n",
    "    Z: float 1-D ndarray with r*n entries\n",
    "    r: int. \n",
    "        The rank of the desired approximating tensor.\n",
    "    n: int. \n",
    "        The dimension of the space minus 1.\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    data: float 1-D ndarray \n",
    "        The nonzero entries of Dres.\n",
    "    \"\"\"    \n",
    "    \n",
    "    data = np.zeros(4*(n+1)**3*r, dtype = np.float64)\n",
    "    augX = np.zeros(r*(n+1))\n",
    "    augY = np.zeros(r*(n+1))\n",
    "    augZ = np.zeros(r*(n+1))\n",
    "    s = 0\n",
    "        \n",
    "    #The program constructs the augmented vectors by considering the 0-th entries, which are equal to 1.\n",
    "    for l in prange(0,r):\n",
    "        augX[l*(n+1)] = 1\n",
    "        augX[l*(n+1)+1 : l*(n+1) + n+1] = X[l*n : l*n + n]\n",
    "        augY[l*(n+1)] = 1\n",
    "        augY[l*(n+1)+1 : l*(n+1) + n+1] = Y[l*n : l*n + n]\n",
    "        augZ[l*(n+1)] = 1\n",
    "        augZ[l*(n+1)+1 : l*(n+1) + n+1] = Z[l*n : l*n + n]\n",
    "    \n",
    "    #Computation of all entries of Dres.\n",
    "    for i in range(0,n+1):\n",
    "        for j in range(0,n+1):\n",
    "            for k in range(0,n+1):\n",
    "                for l in range(0,r):\n",
    "                    #Partial derivative with respect to Lambda.\n",
    "                    data[s] = -augX[l*(n+1) + i]*augY[l*(n+1) + j]*augZ[l*(n+1) + k]\n",
    "                    s = s+1\n",
    "                    #Partial derivative with respect to X.\n",
    "                    if i != 0:\n",
    "                        data[s] = -Lambda[l]*augY[l*(n+1) + j]*augZ[l*(n+1) + k]\n",
    "                        s = s+1\n",
    "                    #Partial derivative with respect to Y.\n",
    "                    if j != 0:\n",
    "                        data[s] = -Lambda[l]*augX[l*(n+1) + i]*augZ[l*(n+1) + k]\n",
    "                        s = s+1\n",
    "                    #Partial derivative with respect to Z.\n",
    "                    if k != 0:\n",
    "                        data[s] = -Lambda[l]*augX[l*(n+1) + i]*augY[l*(n+1) + j]\n",
    "                        s = s+1\n",
    "    \n",
    "    data = data[0:s]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function *initialize* creates the arrays *data,row,col*, which are necessary for working with the sparse matrices $D\\textbf{res}$. Since the sparse structure of these matrices is always the same, the arrays *row,col* only need to be initialized one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@njit(nogil=True,cache=True)\n",
    "def initialize(r,n):\n",
    "    \"\"\"\n",
    "    Initialization of the matrix Dres in sparse format, i.e., a triple (data,row,col) \n",
    "    such that data is a 1-D containing the nonzero values of Dres, row is a 1-D ndarray\n",
    "    containing the corresponding rows index of the elements in data and col is a 1-D\n",
    "    ndarray containing the corresponding columns index of the elements in data.\n",
    "    All initial values of data are equal to one. This function doesn't compute any\n",
    "    actual Jacobian matrix, but only initializes its sparse structure for later.\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    r: int\n",
    "        The rank of the desired approximating tensor.\n",
    "    n: int \n",
    "        The dimension of the space minus 1.\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    data: float 1-D ndarray of ones\n",
    "    row: int 1-D ndarray\n",
    "    col: int 1-D ndarray\n",
    "    \"\"\"   \n",
    "    \n",
    "    row = np.zeros(4*(n+1)**3*r, dtype = np.int64)\n",
    "    col = np.zeros(4*(n+1)**3*r, dtype = np.int64)\n",
    "    data = np.zeros(4*(n+1)**3*r, dtype = np.float64)\n",
    "    s = 0\n",
    "        \n",
    "    for i in range(0,n+1):\n",
    "        for j in range(0,n+1):\n",
    "            for k in range(0,n+1):\n",
    "                for l in range(0,r):\n",
    "                    #Partial derivative with respect to Lambda.\n",
    "                    row[s] = (n+1)**2*i + (n+1)*j + k\n",
    "                    col[s] = l\n",
    "                    data[s] = 1\n",
    "                    s = s+1\n",
    "                    #Partial derivative with respect to X.\n",
    "                    if i != 0:\n",
    "                        row[s] = (n+1)**2*i + (n+1)*j + k\n",
    "                        col[s] = r + l*n + i-1\n",
    "                        data[s] = 1\n",
    "                        s = s+1\n",
    "                    #Partial derivative with respect to Y.\n",
    "                    if j != 0:\n",
    "                        row[s] = (n+1)**2*i + (n+1)*j + k\n",
    "                        col[s] = r + r*n + l*n + j-1\n",
    "                        data[s] = 1\n",
    "                        s = s+1\n",
    "                    #Partial derivative with respect to Z.\n",
    "                    if k != 0:\n",
    "                        row[s] = (n+1)**2*i + (n+1)*j + k\n",
    "                        col[s] = r + 2*r*n + l*n + k-1\n",
    "                        data[s] = 1\n",
    "                        s = s+1\n",
    "    \n",
    "    row = row[0:s]\n",
    "    col = col[0:s]\n",
    "    data = data[0:s]\n",
    "    \n",
    "    return(data,row,col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python function *point2tens* constructs the tensor $S = \\sum_{\\ell=1}^r \\Lambda_\\ell \\cdot X_\\ell \\otimes Y_\\ell \\otimes Z_\\ell$ from a point $x = (\\Lambda,X,Y,Z)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@njit(nogil=True,parallel=True)\n",
    "def point2tens(x,r,n):\n",
    "    \"\"\"\n",
    "    Let x = [Lambda,X,Y,Z], where X,Y,Z are described as in the function residual,\n",
    "    i.e., they are 1-D dnarrays with r*n entries each. This function complete these\n",
    "    ndarrays by putting the additional ones and then constructs the 3-D tensor S\n",
    "    associated.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    x: float 1-D ndarray with r+3*r*n entries\n",
    "    r: int \n",
    "        The rank of the desired approximating tensor.\n",
    "    n: int \n",
    "        The dimension of the space minus 1.\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    S: float 3-D ndarray\n",
    "    \"\"\"   \n",
    "    \n",
    "    S = np.zeros((n+1, n+1, n+1))\n",
    "    #The first entries of X,Y,Z are set equal to one. \n",
    "    X = np.ones(r*(n+1))\n",
    "    Y = np.ones(r*(n+1))\n",
    "    Z = np.ones(r*(n+1))\n",
    "    Lambda = x[0:r]\n",
    "    \n",
    "    for l in prange(0,r):\n",
    "        X[l*(n+1) + 1:(l+1)*(n+1)] = x[r + l*n:r + (l+1)*n]\n",
    "        Y[l*(n+1) + 1:(l+1)*(n+1)] = x[r + r*n + l*n:r + r*n + (l+1)*n]\n",
    "        Z[l*(n+1) + 1:(l+1)*(n+1)] = x[r + 2*r*n + l*n:r + 2*r*n + (l+1)*n]\n",
    "    \n",
    "    for i in prange(0,n+1):\n",
    "        for j in range(0,n+1):\n",
    "            for k in range(0,n+1):                \n",
    "                S[i,j,k] = S_entries(Lambda,X,Y,Z,r,n,i,j,k)\n",
    "         \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@njit(nogil=True,cache=True)\n",
    "def S_entries(Lambda,X,Y,Z,r,n,i,j,k):\n",
    "    \"\"\"Computation of each individual entry of S in the function point2tens.\"\"\"\n",
    "    \n",
    "    s = 0\n",
    "    for l in range(0,r):\n",
    "        s += Lambda[l]*X[l*(n+1)+i]*Y[l*(n+1)+j]*Z[l*(n+1)+k]\n",
    "            \n",
    "    S_entry = s\n",
    "    \n",
    "    return S_entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given initial point $x^{(0)} = (\\Lambda^{(0)},X^{(0)},Y^{(0)},Z^{(0)})$, the python function *gauss_newton* tries to minimize the residual function using the damped Gauss-Newton method. The user may choose the maximum number of iterations and the tolerance value to stop the iteration process. The parameter *tol* makes the iteration stops when $\\|T-S\\|/\\|T\\| < tol$ or $\\|x^{(k+1)} - x^{(k)}\\| < tol$. \n",
    "\n",
    "This function returns the approximating tensor $S$, the obtained error $\\|T-S\\|$, and the following additional information:\n",
    "\n",
    "$\\bullet$ The final point $x = (\\Lambda,X,Y,Z) \\in \\mathbb{R}^{r+3rn}$ computed and used to construct the approximating tensor $S$.\n",
    "\n",
    "$\\bullet$ An array $[\\|x^{(1)} - x^{(0)}\\|, \\|x^{(2)} - x^{(1)}\\|, \\ldots ]$ with the distance between the points in each iterarion.\n",
    "\n",
    "$\\bullet$ An array $[\\|T-S^{(0)}\\|, \\|T-S^{(1)}\\|, \\ldots ]$ with the absolute errors in each iteration.\n",
    "\n",
    "$\\bullet$ An array $[x^{(0)}, x^{(1)}, \\ldots]$ with the path of the points computed in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gauss_newton(T,Lambda,X,Y,Z,r,n,maxit=500,tol=10**(-3),condition=False):\n",
    "    \"\"\"\n",
    "    Starting at x = [Lambda,X,Y,Z], this function uses the Damped Gauss-Newton\n",
    "    method to compute an approximation of T with rank r. The result is given in\n",
    "    format both classical formats: as coordinates and as components to form the CPD.\n",
    "    \n",
    "    The Damped Gauss-Newton method is iterative, updating the point x at each iteration.\n",
    "    The last computed x is of the form x = [Lambda,X,Y,Z], and from these we have\n",
    "    the components to form the CPD of S (the approximating tensor). This program also\n",
    "    gives some additional information such as the size of the steps (distance \n",
    "    between each x computed), the errors (distance between T and S at each iteration)\n",
    "    and the path of solutions (the points x computed at each iteration are saved).\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    T: float 3-D ndarray\n",
    "    Lambda: Float 1-D ndarray with r entries\n",
    "    X: float 1-D ndarray with r*n entries\n",
    "    Y: float 1-D ndarray with r*n entries\n",
    "    Z: float 1-D ndarray with r*n entries\n",
    "    r: int \n",
    "        The rank of the desired approximating tensor.\n",
    "    n: int \n",
    "        The dimension of the space minus 1.\n",
    "    maxit: int\n",
    "        Number of maximum iterations permitted. By default this function makes at\n",
    "    most 500 iterations.\n",
    "    tol: float\n",
    "        Tolerance criterium to stop the iteration proccess. Let S^(k) be the approximating \n",
    "    tensor computed at the k-th iteration an x^(k) be the point computed at the k-th \n",
    "    iteration. If we have norm(T-S^(k))/norm(T) < tol or norm(x^(k+1) - x^(k)) < tol, then \n",
    "    the program stops. By default we have tol = 10**(-3).\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    x: float 1-D ndarray with r+3*r*n entries \n",
    "        Each entry represents the components of the approximating tensor in the CPD form.\n",
    "    S: float 3-D ndarray with (n+1)**3 entries \n",
    "        Each entry represents the coordinates of the approximating tensor in coordinate form.\n",
    "    step_sizes: float 1-D ndarray \n",
    "        Distance between the computed points at each iteration.\n",
    "    errors: float 1-D ndarray \n",
    "        Error of the computed approximating tensor at each iteration. \n",
    "    xpath: float 2-D ndarray \n",
    "        Points computed at each iteration. The k-th row represents the point computed at the \n",
    "    k-th iteration. \n",
    "    \"\"\"  \n",
    "        \n",
    "    S = np.zeros((n+1,n+1,n+1))\n",
    "    x = np.concatenate((Lambda,X,Y,Z))\n",
    "    step_sizes = np.zeros(maxit)\n",
    "    errors = np.zeros(maxit)\n",
    "    xpath = np.zeros((maxit,r+3*r*n))\n",
    "    conds = np.zeros(maxit)\n",
    "    error = np.inf\n",
    "    Tsize = np.linalg.norm(T)\n",
    "    #Initialize the row and column indexes for sparse matrix creation.\n",
    "    [data,row,col] = initialize(r,n)\n",
    "    Dres = sparse.csr_matrix((data, (row, col)), shape=((n+1)**3,r+3*r*n))\n",
    "    #u is the damping parameter.\n",
    "    u = 1\n",
    "    atol = 10**(-5)\n",
    "    btol = 10**(-3)\n",
    "    old_residualnorm = 0\n",
    "    #This constant controls the maximum number of iterations of the function lsmr.\n",
    "    lsmr_maxiter = (2 + np.floor(1000/n**2))*max(min(r,n),10)\n",
    "    \n",
    "    #Gauss-Newton iterations starting at x.\n",
    "    for it in range(0,maxit):  \n",
    "        #Computation of r(x) and Dres(x).\n",
    "        res = residuals(T,Lambda,X,Y,Z,r,n)\n",
    "        data = derivative_residuals(Lambda,X,Y,Z,r,n)\n",
    "        Dres[row,col] = data\n",
    "        \n",
    "        #Sets the old values to compare with the new ones.\n",
    "        old_x = x\n",
    "        old_error = error\n",
    "        \n",
    "        #Computation of the Gauss-Newton iteration formula to obtain the new point x.\n",
    "        #The vector a is the solution of min_y |Ay - b|, with A = Dres(x) and b = -res(x). \n",
    "        [y,istop,itn,residualnorm,auxnorm,Dres_norm,estimate_Dres_cond,ynorm] = ssl.lsmr(Dres,-res,u,atol,btol,maxiter=lsmr_maxiter)\n",
    "        x = x + y\n",
    "        \n",
    "        #Computation of the respective tensor S associated to x and its error.\n",
    "        S = point2tens(x,r,n)\n",
    "        error = np.linalg.norm(T-S)\n",
    "                \n",
    "        #Update the damping parameter. \n",
    "        g = 2*(old_error - error)/(old_residualnorm - residualnorm)\n",
    "        if g < 0.25:\n",
    "            u = 2*u\n",
    "        elif g > 0.75:\n",
    "            u = u/3\n",
    "        old_residualnorm = residualnorm\n",
    "        \n",
    "        #Update the arrays with information about the iteration.\n",
    "        step_sizes[it] = np.linalg.norm(x - old_x)   \n",
    "        errors[it] = error\n",
    "        xpath[it,:] = x\n",
    "        if condition == True:\n",
    "            Dres_dense = Dres.toarray()\n",
    "            conds[it] = 1/np.linalg.norm(Dres_dense,-2)\n",
    "        #After 10 iterations, the program starts to verify if the size of the current step or the difference between the errors are smaller than tol.\n",
    "        if it >= 10:\n",
    "            errors_diff = 1/Tsize*np.abs(errors[it] - errors[it-1])\n",
    "            if step_sizes[it] < tol or errors_diff < tol:\n",
    "                break\n",
    "        #Update the vectors L,X,Y,Z for the next iteration.\n",
    "        Lambda = x[0:r]\n",
    "        X = x[r:r+r*n]\n",
    "        Y = x[r+r*n:r+2*r*n]\n",
    "        Z = x[r+2*r*n:r+3*r*n]\n",
    "        \n",
    "    step_sizes = step_sizes[0:it+1]\n",
    "    errors = errors[0:it+1]\n",
    "    xpath = xpath[0:it+1,:]\n",
    "    conds = conds[0:it+1]\n",
    "\n",
    "    return(x,S,step_sizes,errors,xpath,conds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function does the same thing as *gauss_newton*, but with the difference that it measures the computation times of several parts of the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gauss_newton_timing(T,Lambda,X,Y,Z,r,n,maxit=500,tol=10**(-3)):\n",
    "    \"\"\"\n",
    "    This function does the same thing as the `gauss_newton` function, but with the \n",
    "    difference that it measures the computation time of several parts of the\n",
    "    algorithm.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    T: float 3-D ndarray\n",
    "    Lambda: Float 1-D ndarray with r entries\n",
    "    X: float 1-D ndarray with r*n entries\n",
    "    Y: float 1-D ndarray with r*n entries\n",
    "    Z: float 1-D ndarray with r*n entries\n",
    "    r: int \n",
    "        The rank of the desired approximating tensor.\n",
    "    n: int \n",
    "        The dimension of the space minus 1.\n",
    "    maxit: int\n",
    "        Number of maximum iterations permitted. By default this function makes at\n",
    "    most 500 iterations.\n",
    "    tol: float\n",
    "        Tolerance criterium to stop the iteration proccess. Let S^(k) be the approximating \n",
    "    tensor computed at the k-th iteration an x^(k) be the point computed at the k-th \n",
    "    iteration. If we have norm(T-S^(k))/norm(T) < tol or norm(x^(k+1) - x^(k)) < tol, then \n",
    "    the program stops. By default we have tol = 10**(-3).\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    x: float 1-D ndarray with r+3*r*n entries \n",
    "        Each entry represents the components of the approximating tensor in the CPD form.\n",
    "    S: float 3-D ndarray with (n+1)**3 entries \n",
    "        Each entry represents the coordinates of the approximating tensor in coordinate form.\n",
    "    step_sizes: float 1-D ndarray \n",
    "        Distance between the computed points at each iteration.\n",
    "    errors: float 1-D ndarray \n",
    "        Error of the computed approximating tensor at each iteration. \n",
    "    xpath: float 2-D ndarray \n",
    "        Points computed at each iteration. The k-th row represents the point computed at the \n",
    "    k-th iteration. \n",
    "    sparse_time: float 1-D ndarray \n",
    "        The k-th entry is the computation time spent to update the sparse matrix Dres at the \n",
    "    k-th iteration.\n",
    "    gauss_newton_time: float 1-D ndarray \n",
    "        The k-th entry is the computation time spent to compute the lsmr algorithm at the k-th \n",
    "    iteration. We call it gauss_newton because this is the principal part of the Damped \n",
    "    Gauss-Newton method.\n",
    "    rest_time: float 1-D ndarray \n",
    "        The k-th entry is the computation time spent in all the other parts of the k-th iteration.\n",
    "    \"\"\" \n",
    "    \n",
    "    S = np.zeros((n+1,n+1,n+1))\n",
    "    x = np.concatenate((Lambda,X,Y,Z))\n",
    "    step_sizes = np.zeros(maxit)\n",
    "    errors = np.zeros(maxit)\n",
    "    xpath = np.zeros((maxit,r+3*r*n))\n",
    "    error = np.inf\n",
    "    Tsize = np.linalg.norm(T)\n",
    "    #Initialize the row and column indexes for sparse matrix creation.\n",
    "    [data,row,col] = initialize(r,n)\n",
    "    Dres = sparse.csr_matrix((data, (row, col)), shape=((n+1)**3,r+3*r*n))\n",
    "    #u is the damping parameter.\n",
    "    u = 1\n",
    "    atol = 10**(-5)\n",
    "    btol = 10**(-3)\n",
    "    old_residualnorm = 0\n",
    "    #This constant controls the maximum number of iterations of the function lsmr.\n",
    "    lsmr_maxiter = (2 + np.floor(1000/n**2))*max(min(r,n),10)\n",
    "    \n",
    "    sparse_time = np.zeros(maxit)\n",
    "    gauss_newton_time = np.zeros(maxit)\n",
    "    rest_time = np.zeros(maxit)\n",
    "    \n",
    "    #Gauss-Newton iterations starting at x.\n",
    "    for it in range(0,maxit):  \n",
    "        #Computation of r(x) and Dres(x).\n",
    "        start = time.time()\n",
    "        res = residuals(T,Lambda,X,Y,Z,r,n)\n",
    "        data = derivative_residuals(Lambda,X,Y,Z,r,n)\n",
    "        rest_time[it] = time.time() - start\n",
    "        \n",
    "        #Convert the matrix to a scipy sparse matrix.\n",
    "        start = time.time()\n",
    "        Dres[row,col] = data\n",
    "        sparse_time[it] = time.time() - start\n",
    "        \n",
    "        #Sets the old values to compare with the new ones.\n",
    "        start = time.time()\n",
    "        old_x = x\n",
    "        old_error = error\n",
    "        rest_time[it] = rest_time[it] + time.time() - start\n",
    "        \n",
    "        start = time.time()\n",
    "        #Computation of the Gauss-Newton iteration formula to obtain the new point x.\n",
    "        #The vector a is the solution of min_y |Ay - b|, with A = Dr(x) and b = -res(x). \n",
    "        [y,istop,itn,residualnorm,auxnorm,Dres_norm,estimate_Dres_cond,ynorm] = ssl.lsmr(Dres,-res,u,atol,btol,maxiter=lsmr_maxiter)\n",
    "        x = x + y\n",
    "        gauss_newton_time[it] = time.time() - start\n",
    "        \n",
    "        start = time.time()\n",
    "        #Computation of the respective tensor S associated to x and its error.\n",
    "        S = point2tens(x,r,n)\n",
    "        error = np.linalg.norm(T-S)\n",
    "                \n",
    "        #Update the damping parameter. \n",
    "        g = 2*(old_error - error)/(old_residualnorm - residualnorm)\n",
    "        if g < 0.25:\n",
    "            u = 2*u\n",
    "        elif g > 0.75:\n",
    "            u = u/3\n",
    "        old_residualnorm = residualnorm\n",
    "        \n",
    "        #Update the arrays with information about the iteration.\n",
    "        step_sizes[it] = np.linalg.norm(x - old_x)   \n",
    "                \n",
    "        errors[it] = error\n",
    "        xpath[it,:] = x\n",
    "        #After 10 iterations, the program starts to verify if the size of the current step or the difference between the errors are smaller than tol.\n",
    "        if it >= 10:\n",
    "            errors_diff = 1/Tsize*np.abs(errors[it] - errors[it-1])\n",
    "            if step_sizes[it] < tol or errors_diff < tol:\n",
    "                break\n",
    "        #Update the vectors L,X,Y,Z for the next iteration.\n",
    "        Lambda = x[0:r]\n",
    "        X = x[r:r+r*n]\n",
    "        Y = x[r+r*n:r+2*r*n]\n",
    "        Z = x[r+2*r*n:r+3*r*n]\n",
    "        rest_time[it] = rest_time[it] + time.time() - start\n",
    "        \n",
    "    step_sizes = step_sizes[0:it+1]\n",
    "    errors = errors[0:it+1]\n",
    "    xpath = xpath[0:it+1,:]\n",
    "    sparse_time = sparse_time[0:it+1]\n",
    "    gauss_newton_time = gauss_newton_time[0:it+1]\n",
    "    rest_time = rest_time[0:it+1]\n",
    "\n",
    "    return(x,S,step_sizes,errors,xpath,sparse_time,gauss_newton_time,rest_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python function *low_rank* tries to minimize the residual function calling several times the Gauss-Newton method. The user may choose the number of trials the program makes with the parameter *maxtrials*. Also, the user may choose the maximum number of iterations at each trials. Finally, the user may define the tolerance value to stop the iteration process. The parameter *tol* is passed to each Gauss_Newton trial, we also use this parameter to stop the program when $\\|T-S\\|/\\|T\\| < tol$ or the improvement of $\\|T-S\\|/\\|T\\|$ in some trial is less than *tol*. \n",
    "\n",
    "The first outputs of this function are essentially the best output of all functions *gauss_newton* computed. The program returns the arrays $(\\Lambda,X,Y,Z)$, including the 0th entries (equal to 1). Look at the desciption of the function *x2CPD* for more details. The latter outputs are general information about all the trials. These informations are the following:\n",
    "\n",
    "$\\bullet$ The total time spent in each trial.\n",
    "\n",
    "$\\bullet$ The number of steps used in each trial.\n",
    "\n",
    "$\\bullet$ The relative error $\\|T-S\\|/\\|T\\|$ obtained in each trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def low_rank(T,r,n,maxtrials=3,maxit=500,tol=10**(-3)):\n",
    "    \"\"\"\n",
    "    This function searches for the best rank r approximation of T by making several\n",
    "    calls to the gauss_newton function with random initial points. By defalt, the\n",
    "    gauss_newton function is called 3 times and the best result is saved. The user \n",
    "    may choose the number of trials the program makes with the parameter maxtrials. \n",
    "    Also, the user may choose the maximum number of iterations at each trials. Finally, \n",
    "    the user may define the tolerance value to stop the iteration process. The parameter \n",
    "    tol is passed to each Gauss_Newton trial, we also use this parameter to stop the \n",
    "    program when |T-S|/|T| < tol or the improvement of |T-S|/|T| in some trial is less \n",
    "    than tol. \n",
    "\n",
    "    The first outputs of this function are essentially the best output of all functions \n",
    "    gauss_newton computed. The program returns the arrays $(\\Lambda,X,Y,Z)$, including \n",
    "    the 0th entries (equal to 1). Look at the desciption of the function `x2CPD` for more \n",
    "    details. The latter outputs are general information about all the trials. These \n",
    "    informations are the following:\n",
    "\n",
    "    * The total time spent in each trial.\n",
    "\n",
    "    * The number of steps used in each trial.\n",
    "\n",
    "    * The relative error |T-S|/|T| obtained in each trial.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    T: float 3-D ndarray\n",
    "    r: int \n",
    "        The rank of the desired approximating tensor.\n",
    "    n: int \n",
    "        The dimension of the space minus 1.\n",
    "    maxtrials: int \n",
    "        Number of maximum number of times the program calls the gauss_newton function.\n",
    "    maxit: int\n",
    "        Number of maximum iterations permitted in the gauss_newton function. By default \n",
    "    this function makes at most 500 iterations.\n",
    "    tol: float\n",
    "        Tolerance criterium to stop the iteration proccess. Let S^(k) be the approximating \n",
    "    tensor computed at the k-th iteration an x^(k) be the point computed at the k-th \n",
    "    iteration. If we have norm(T-S^(k))/norm(T) < tol or norm(x^(k+1) - x^(k)) < tol, then \n",
    "    the program stops. By default we have tol = 10**(-3).\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    Lambda: float 1-D ndarray with r entries\n",
    "    X: float 2-D ndarray with r*(n+1) entries\n",
    "    Y: float 2-D ndarray with r*(n+1) entries\n",
    "    Z: float 2-D ndarray with r*(n+1) entries\n",
    "    S: float 3-D ndarray\n",
    "    error: float\n",
    "        The value |T-S|, which is the error of the best trials. \n",
    "    step_sizes: float 1-D ndarray \n",
    "        Distance between the computed points at each iteration of the best trial.\n",
    "    errors: float 1-D ndarray \n",
    "        Error of the computed approximating tensor at each iteration of the best trial.\n",
    "    xpath: float 2-D ndarray \n",
    "        Points computed at each iteration. The k-th row represents the point computed at the \n",
    "    k-th iteration of the best trial. \n",
    "    times: float 1-D ndarray\n",
    "        The total time spent in each trial. The i-th entry is the computation time of the\n",
    "    i-th trial.\n",
    "    steps: int 1-D ndarray\n",
    "        The number of steps used in each trial. The i-th entry is the number of steps (number \n",
    "    of iterations) of the i-th trial.\n",
    "    rel_errors: float 1-D ndarray \n",
    "        The relative error |T-S|/|T| obtained in each trial. The i-th entry is the relative \n",
    "    error of the approximation of the i-th trial.\n",
    "    \"\"\" \n",
    "    \n",
    "    times = np.zeros(maxtrials)\n",
    "    steps = np.zeros(maxtrials)\n",
    "    rel_errors = np.zeros(maxtrials)\n",
    "    best_error = np.inf\n",
    "    \n",
    "    #Computation of the Frobenius norm of T.\n",
    "    Tsize = np.linalg.norm(T)\n",
    "    \n",
    "    #At each trial the program generates a random starting point (L,X,Y,Z) to apply the Gauss-Newton method.\n",
    "    for trial in range(0,maxtrials):\n",
    "        Lambda = np.random.randn(r)\n",
    "        X = np.random.randn(r*n)\n",
    "        Y = np.random.randn(r*n)\n",
    "        Z = np.random.randn(r*n)\n",
    "        \n",
    "        #Computation of one Gauss-Newton method starting at (L,X,Y,Z).\n",
    "        start = time.time()\n",
    "        [x,S,step_sizes,errors,xpath] = gauss_newton(T,Lambda,X,Y,Z,r,n,maxit,tol) \n",
    "        \n",
    "        #Update the vectors with general information.\n",
    "        times[trial] = time.time() - start\n",
    "        steps[trial] = step_sizes.shape[0]\n",
    "        rel_errors[trial] = 1/Tsize*errors[-1]\n",
    "                \n",
    "        #We use the subroutine \"pocket\" to save the best tensor S computed  so far.    \n",
    "        error = errors[-1]\n",
    "        if error < best_error:\n",
    "            best_x = x\n",
    "            best_S = S\n",
    "            old_best_error = best_error\n",
    "            best_error = error\n",
    "            best_step_sizes = step_sizes\n",
    "            best_errors = errors\n",
    "            best_xpath = xpath\n",
    "            #The search for better solutions stops when the best relative error is small enough or the improvement is irrelevant.\n",
    "            if trial > 1:\n",
    "                if best_error/Tsize < tol or np.abs(best_error - old_best_error) < Tsize*tol :\n",
    "                    break\n",
    "                \n",
    "    #After everything, we rename all the information related to the best S, for convenience.\n",
    "    [Lambda,X,Y,Z] = x2CPD(best_x,r,n)\n",
    "    error = best_error\n",
    "    step_sizes = best_step_sizes\n",
    "    errors = best_errors\n",
    "    xpath = best_xpath\n",
    "    times = times[0:trial+1]\n",
    "    steps = steps[0:trial+1]\n",
    "    rel_errors = rel_errors[0:trial+1]\n",
    "\n",
    "    return(Lambda,X,Y,Z,S,error,step_sizes,errors,xpath,times,steps,rel_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the point $x = (x_1, \\ldots, x_{r+3rn})$, the function *x2CPD* breaks it in parts, in order to form the CPD of $S$. This program return the arrays\n",
    "\n",
    "$$\\Lambda = \\left[\n",
    "\\begin{array}{c}\n",
    "    \\Lambda_1\\\\\n",
    "    \\vdots\\\\ \n",
    "    \\Lambda_r\n",
    "\\end{array}\n",
    "\\right], \\quad\n",
    "X = \\left[\n",
    "\\begin{array}{c}\n",
    "    X_1\\\\\n",
    "    \\vdots\\\\ \n",
    "    X_r\n",
    "\\end{array}\n",
    "\\right], \\quad\n",
    "Y = \\left[\n",
    "\\begin{array}{c}\n",
    "    Y_1\\\\\n",
    "    \\vdots\\\\ \n",
    "    Y_r\n",
    "\\end{array}\n",
    "\\right], \\quad\n",
    "Z = \\left[\n",
    "\\begin{array}{c}\n",
    "    Z_1\\\\\n",
    "    \\vdots\\\\ \n",
    "    Z_r\n",
    "\\end{array}\n",
    "\\right]$$\n",
    "so we have that \n",
    "$$S = \\sum_{\\ell=1}^r \\Lambda_\\ell \\cdot X_\\ell \\otimes Y_\\ell \\otimes Z_\\ell,$$ \n",
    "where\n",
    "$$X_\\ell = (1, X_{\\ell_1}, \\ldots, X_{\\ell_n}),$$\n",
    "$$Y_\\ell = (1, Y_{\\ell_1}, \\ldots, Y_{\\ell_n}),$$\n",
    "$$Z_\\ell = (1, Z_{\\ell_1}, \\ldots, Z_{\\ell_n}).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@njit(nogil=True,parallel=True)\n",
    "def x2CPD(x,r,n):\n",
    "    \"\"\"\n",
    "    Given the point x = (x_1, \\ldots, x_{r+3rn}), this function breaks it in parts, \n",
    "    in order to form the CPD of S. This program return the arrays (let ^T be the transpose)\n",
    "    Lambda = [Lambda_1,...,Lambda_r]^T,\n",
    "    X = [X_1,...,X_r]^T,\n",
    "    Y = [Y_1,...,Y_r]^T,\n",
    "    Z = [Z_1,...,Z_r]^T\n",
    "    so we have that\n",
    "    S = Lambda_1*X_1⊗Y_1⊗Z_1 + ... + Lambda_r*X_r⊗Y_r⊗Z_r, where\n",
    "    X_l = (1, X_{l_1}, ..., X_{l_n}),\n",
    "    Y_l = (1, Y_{l_1}, ..., Y_{l_n}),\n",
    "    Z_l = (1, Z_{l_1}, ..., Z_{l_n}).\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    x: float 1-D ndarray\n",
    "    r: int \n",
    "        The rank of the approximating tensor.\n",
    "    n: int \n",
    "        The dimension of the space minus 1.\n",
    "        \n",
    "    Outputs\n",
    "    -------\n",
    "    Lambda: float 1-D ndarray with r entries\n",
    "    X: float 2-D ndarray with r*(n+1) entries\n",
    "    Y: float 2-D ndarray with r*(n+1) entries\n",
    "    Z: float 2-D ndarray with r*(n+1) entries\n",
    "    \"\"\"\n",
    "    \n",
    "    Lambda = np.zeros(r)\n",
    "    X = np.ones((r,n+1))\n",
    "    Y = np.ones((r,n+1))\n",
    "    Z = np.ones((r,n+1))\n",
    "    \n",
    "    Lambda = x[0:r]\n",
    "    for l in prange(0,r):\n",
    "        X[l,:] = x[r + l*n:r + (l+1)*n]\n",
    "        Y[l,:] = x[r + r*n + l*n:r + r*n + (l+1)*n]\n",
    "        Z[l,:] = x[r + 2*r*n + l*n:r + 2*r*n + (l+1)*n]\n",
    "        \n",
    "    return(Lambda,X,Y,Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function computes several approximations of $T$ for $r = 1 \\ldots n^2$. We use these computations to determine the (most probable) rank of $T$. The function also returns an array *errors_per_rank* with the relative errors for the rank varying from $1$ to $r+1$, where $r$ is the computed rank of $T$. It is relevant to say that the value $r$ computed can also be the *border rank* of $T$, not the actual rank. \n",
    "\n",
    "The idea is that the minimum of $\\|T-S\\|$, for each rank $r$, stabilizes when $S$ has the same rank as $T$. This function also plots the graph of the errors so the user are able to visualize the moment when the error stabilizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank(T):\n",
    "    \"\"\"\n",
    "    This function computes several approximations of T for r = 1 \\ldots n^2. We use \n",
    "    these computations to determine the (most probable) rank of T. The function also \n",
    "    returns an array `errors_per_rank` with the relative errors for the rank varying \n",
    "    from 1 to r+1, where r is the computed rank of T. It is relevant to say that the \n",
    "    value r computed can also be the `border rank` of T, not the actual rank. \n",
    "\n",
    "    The idea is that the minimum of \\|T-S\\|, for each rank r, stabilizes when S has \n",
    "    the same rank as T. This function also plots the graph of the errors so the user \n",
    "    are able to visualize the moment when the error stabilizes.\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    T: float 3-D ndarray\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    final_rank: int\n",
    "        The computed rank of T.\n",
    "    errors_per_rank: float 1-D ndarray\n",
    "        The error |T-S| computed for each rank.    \n",
    "    \"\"\"\n",
    "    \n",
    "    #R is an upper bound for the rank.\n",
    "    n = T.shape[0]-1\n",
    "    R = n**2\n",
    "    errors_per_rank = np.zeros(R)\n",
    "    Tsize = np.linalg.norm(T)\n",
    "    for r in range(1,R):\n",
    "        maxit = 100\n",
    "        tol = 10**(-3)\n",
    "        Lambda = np.random.randn(r)\n",
    "        X = np.random.randn(r*n)\n",
    "        Y = np.random.randn(r*n)\n",
    "        Z = np.random.randn(r*n)\n",
    "        [x,S,step_sizes,errors,xpath] = gauss_newton(T,Lambda,X,Y,Z,r,n,maxit,tol)\n",
    "        errors_per_rank[r-1] = errors[-1]/Tsize\n",
    "        if r > 1:\n",
    "            #Verification of the stabilization condition.\n",
    "            if np.abs(errors_per_rank[r-1] - errors_per_rank[r-2]) < tol:\n",
    "                break\n",
    "     \n",
    "    final_rank = r-1\n",
    "    errors_per_rank = errors_per_rank[0:r] \n",
    "    \n",
    "    print('R(T) =',r-1)\n",
    "    print('|T-S|/|T| =',errors_per_rank[-2])\n",
    "    plt.plot(range(1,r+1),np.log10(errors_per_rank))\n",
    "    plt.plot(r-1,np.log10(errors_per_rank[-2]),marker = 'o',color = 'k')\n",
    "    plt.title('Rank trials')\n",
    "    plt.xlabel('r')\n",
    "    plt.ylabel('$log10 \\|T - S\\|/|T|$')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "            \n",
    "    return(final_rank,errors_per_rank)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
