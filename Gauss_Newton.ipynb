{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "import scipy.sparse.linalg as ssl\n",
    "from numba import jit, njit, prange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the problem of approximating a tensor $T \\in \\mathbb{R}^{n+1} \\otimes \\mathbb{R}^{n+1} \\otimes \\mathbb{R}^{n+1}$ by a tensor of rank $r$ given by\n",
    "$$S = \\sum_{\\ell=1}^r \\Lambda_\\ell \\cdot X_{\\ell} \\otimes Y_{\\ell} \\otimes Z_{\\ell},$$\n",
    "where \n",
    "$$X_{\\ell} = (1, X_{\\ell_1}, \\ldots, X_{\\ell_n}),$$\n",
    "$$Y_{\\ell} = (1, Y_{\\ell_1}, \\ldots, Y_{\\ell_n}),$$\n",
    "$$Z_{\\ell} = (1, Z_{\\ell_1}, \\ldots, Z_{\\ell_n}).$$\n",
    "\n",
    "We do this by minimizing the error function\n",
    "$$\\textbf{E}(\\Lambda,X,Y,Z) = \\frac{1}{2}\\|T - S\\|^2 = \\frac{1}{2} \\sum_{i,j,k=0}^n \\left( T_{ijk} - \\sum_{\\ell=1}^r \\Lambda_\\ell \\cdot X_{\\ell_i} Y_{\\ell_j} Z_{\\ell_j} \\right)^2 = \\frac{1}{2} \\sum_{i,j,k=0}^n r_{ijk}^2(\\Lambda, X,Y,Z) = \\frac{1}{2} \\|\\textbf{r}(\\Lambda,X,Y,Z)\\|^2,$$\n",
    "where\n",
    "$$\\Lambda = (\\Lambda_1, \\ldots, \\Lambda_r),$$\n",
    "$$X = (X_1, \\ldots, X_r),$$\n",
    "$$Y = (Y_1, \\ldots, Y_r),$$\n",
    "$$Z = (Z_1, \\ldots, Z_r),$$\n",
    "and $\\textbf{r} = (r_{000}, r_{001}, \\ldots, r_{nnn})$ is the function of the residuals.\n",
    "\n",
    "In the python function called *residuals* the program constructs $\\textbf{r}(\\Lambda,X,Y,Z)$ for a given $(\\Lambda,X,Y,Z)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@njit(nogil=True,parallel=True)\n",
    "def residuals(T,L,X,Y,Z,r,n):\n",
    "    res = np.zeros((n+1)**3)\n",
    "    augX = np.zeros(r*(n+1))\n",
    "    augY = np.zeros(r*(n+1))\n",
    "    augZ = np.zeros(r*(n+1))\n",
    "    \n",
    "    #The program constructs the augmented vectors by considering the 0-th entries, which are equal to 1.\n",
    "    for l in prange(0,r):\n",
    "        augX[l*(n+1)] = 1\n",
    "        augX[l*(n+1)+1 : l*(n+1) + n+1] = X[l*n : l*n + n]\n",
    "        augY[l*(n+1)] = 1\n",
    "        augY[l*(n+1)+1 : l*(n+1) + n+1] = Y[l*n : l*n + n]\n",
    "        augZ[l*(n+1)] = 1\n",
    "        augZ[l*(n+1)+1 : l*(n+1) + n+1] = Z[l*n : l*n + n]\n",
    "        \n",
    "    #Construction of the vector r = (r_{000}, r_{001}, ..., r_{nnn}).\n",
    "    for i in prange(0,n+1):\n",
    "        for j in range(0,n+1):\n",
    "            for k in range(0,n+1):\n",
    "                s = 0\n",
    "                for l in range(0,r):\n",
    "                    s += L[l]*augX[l*(n+1)+i]*augY[l*(n+1)+j]*augZ[l*(n+1)+k]\n",
    "                res[(n+1)**2*i + (n+1)*j + k] = T[i,j,k] - s\n",
    "                \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the python function *derivative_residuals* the program constructs the Jacobian matrix of $\\textbf{r}$ at $(\\Lambda,X,Y,Z)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@njit(nogil=True,parallel=True)\n",
    "def derivative_residuals(L,X,Y,Z,r,n):\n",
    "    data = np.zeros(4*(n+1)**3*r, dtype = np.float64)\n",
    "    augX = np.zeros(r*(n+1))\n",
    "    augY = np.zeros(r*(n+1))\n",
    "    augZ = np.zeros(r*(n+1))\n",
    "    s = 0\n",
    "        \n",
    "    #The program constructs the augmented vectors by considering the 0-th entries, which are equal to 1.\n",
    "    for l in prange(0,r):\n",
    "        augX[l*(n+1)] = 1\n",
    "        augX[l*(n+1)+1 : l*(n+1) + n+1] = X[l*n : l*n + n]\n",
    "        augY[l*(n+1)] = 1\n",
    "        augY[l*(n+1)+1 : l*(n+1) + n+1] = Y[l*n : l*n + n]\n",
    "        augZ[l*(n+1)] = 1\n",
    "        augZ[l*(n+1)+1 : l*(n+1) + n+1] = Z[l*n : l*n + n]\n",
    "    \n",
    "    #Computation of all entries of Dr.\n",
    "    for i in range(0,n+1):\n",
    "        for j in range(0,n+1):\n",
    "            for k in range(0,n+1):\n",
    "                for l in range(0,r):\n",
    "                    #Partial derivative with respect to Lambda.\n",
    "                    data[s] = -augX[l*(n+1) + i]*augY[l*(n+1) + j]*augZ[l*(n+1) + k]\n",
    "                    s = s+1\n",
    "                    #Partial derivative with respect to X.\n",
    "                    if i != 0:\n",
    "                        data[s] = -L[l]*augY[l*(n+1) + j]*augZ[l*(n+1) + k]\n",
    "                        s = s+1\n",
    "                    #Partial derivative with respect to Y.\n",
    "                    if j != 0:\n",
    "                        data[s] = -L[l]*augX[l*(n+1) + i]*augZ[l*(n+1) + k]\n",
    "                        s = s+1\n",
    "                    #Partial derivative with respect to Z.\n",
    "                    if k != 0:\n",
    "                        data[s] = -L[l]*augX[l*(n+1) + i]*augY[l*(n+1) + j]\n",
    "                        s = s+1\n",
    "    \n",
    "    data = data[0:s]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function *initialize* creates the arrays *data,row,col*, which are necessary for working with the sparse matrices **Dr**. Since the sparse structure of these matrices is always the same, the arrays *row,col* only need to be initialized one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@njit(nogil=True,cache=True)\n",
    "def initialize(r,n):\n",
    "    row = np.zeros(4*(n+1)**3*r, dtype = np.int64)\n",
    "    col = np.zeros(4*(n+1)**3*r, dtype = np.int64)\n",
    "    data = np.zeros(4*(n+1)**3*r, dtype = np.float64)\n",
    "    s = 0\n",
    "        \n",
    "    for i in range(0,n+1):\n",
    "        for j in range(0,n+1):\n",
    "            for k in range(0,n+1):\n",
    "                for l in range(0,r):\n",
    "                    #Partial derivative with respect to Lambda.\n",
    "                    row[s] = (n+1)**2*i + (n+1)*j + k\n",
    "                    col[s] = l\n",
    "                    data[s] = 1\n",
    "                    s = s+1\n",
    "                    #Partial derivative with respect to X.\n",
    "                    if i != 0:\n",
    "                        row[s] = (n+1)**2*i + (n+1)*j + k\n",
    "                        col[s] = r + l*n + i-1\n",
    "                        data[s] = 1\n",
    "                        s = s+1\n",
    "                    #Partial derivative with respect to Y.\n",
    "                    if j != 0:\n",
    "                        row[s] = (n+1)**2*i + (n+1)*j + k\n",
    "                        col[s] = r + r*n + l*n + j-1\n",
    "                        data[s] = 1\n",
    "                        s = s+1\n",
    "                    #Partial derivative with respect to Z.\n",
    "                    if k != 0:\n",
    "                        row[s] = (n+1)**2*i + (n+1)*j + k\n",
    "                        col[s] = r + 2*r*n + l*n + k-1\n",
    "                        data[s] = 1\n",
    "                        s = s+1\n",
    "    \n",
    "    row = row[0:s]\n",
    "    col = col[0:s]\n",
    "    data = data[0:s]\n",
    "    \n",
    "    return(data,row,col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python function *point2tens* constructs the tensor $S = \\sum_{\\ell=1}^r \\Lambda_\\ell \\cdot X_\\ell \\otimes Y_\\ell \\otimes Z_\\ell$ from a point $x = (\\Lambda,X,Y,Z)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@njit(nogil=True,parallel=True)\n",
    "def point2tens(x,r,n):\n",
    "    S = np.zeros((n+1, n+1, n+1))\n",
    "    #The first entries of X,Y,Z are set equal to one. \n",
    "    X = np.ones(r*(n+1))\n",
    "    Y = np.ones(r*(n+1))\n",
    "    Z = np.ones(r*(n+1))\n",
    "    L = x[0:r]\n",
    "    \n",
    "    for l in prange(0,r):\n",
    "        X[l*(n+1) + 1:(l+1)*(n+1)] = x[r + l*n:r + (l+1)*n]\n",
    "        Y[l*(n+1) + 1:(l+1)*(n+1)] = x[r + r*n + l*n:r + r*n + (l+1)*n]\n",
    "        Z[l*(n+1) + 1:(l+1)*(n+1)] = x[r + 2*r*n + l*n:r + 2*r*n + (l+1)*n]\n",
    "    \n",
    "    for i in prange(0,n+1):\n",
    "        for j in range(0,n+1):\n",
    "            for k in range(0,n+1):\n",
    "                s = 0\n",
    "                for l in range(0,r):\n",
    "                    s += L[l]*X[l*(n+1)+i]*Y[l*(n+1)+j]*Z[l*(n+1)+k]\n",
    "                S[i,j,k] = s\n",
    "         \n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given initial point $x^{(0)} = (\\Lambda^{(0)},X^{(0)},Y^{(0)},Z^{(0)})$, the python function *gauss_newton* tries to minimize the residual function using the damped Gauss-Newton method. The user may choose the maximum number of iterations and the tolerance value to stop the iteration process. The parameter *tol* makes the iteration stops when $\\|T-S\\|/\\|T\\| < tol$ or $\\|x^{(k+1)} - x^{(k)}\\| < tol$. \n",
    "\n",
    "This function returns the approximating tensor $S$, the obtained error $\\|T-S\\|$, and the following additional information:\n",
    "\n",
    "$\\bullet$ The final point $x = (\\Lambda,X,Y,Z) \\in \\mathbb{R}^{r+3rn}$ computed and used to construct the approximating tensor $S$.\n",
    "\n",
    "$\\bullet$ An array $[\\|x^{(1)} - x^{(0)}\\|, \\|x^{(2)} - x^{(1)}\\|, \\ldots ]$ with the distance between the points in each iterarion.\n",
    "\n",
    "$\\bullet$ An array $[\\kappa\\left( D\\textbf{r}(x^{(0)})^T D\\textbf{r}(x^{(0)}) \\right), \\kappa\\left( D\\textbf{r}(x^{(1)})^T D\\textbf{r}(x^{(1)}) \\right), \\ldots]$ with the condition numbers of $D\\textbf{r}(x^{(k)})^T D\\textbf{r}(x^{(k)})$ in each iteration.\n",
    "\n",
    "$\\bullet$ An array $[\\|T-S^{(0)}\\|, \\|T-S^{(1)}\\|, \\ldots ]$ with the absolute errors in each iteration.\n",
    "\n",
    "$\\bullet$ An array $[x^{(0)}, x^{(1)}, \\ldots]$ with the path of the points computed in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gauss_newton(T,L,X,Y,Z,r,n,maxit=500,tol=10**(-3)):\n",
    "    S = np.zeros((n+1,n+1,n+1))\n",
    "    x = np.concatenate((L,X,Y,Z))\n",
    "    step_sizes = np.zeros(maxit)\n",
    "    errors = np.zeros(maxit)\n",
    "    xpath = np.zeros((maxit,r+3*r*n))\n",
    "    error = np.inf\n",
    "    Tsize = np.linalg.norm(T)\n",
    "    #Initialize the row and column indexes for sparse matrix creation.\n",
    "    [data,row,col] = initialize(r,n)\n",
    "    Dr = sparse.csr_matrix((data, (row, col)), shape=((n+1)**3,r+3*r*n))\n",
    "    #u is the damping parameter.\n",
    "    u = 1\n",
    "    atol = 10**(-5)\n",
    "    btol = 10**(-3)\n",
    "    old_residualnorm = 0\n",
    "    \n",
    "    #Gauss-Newton iterations starting at x.\n",
    "    for it in range(0,maxit):  \n",
    "        #Computation of r(x) and Dr(x).\n",
    "        res = residuals(T,L,X,Y,Z,r,n)\n",
    "        data = derivative_residuals(L,X,Y,Z,r,n)\n",
    "        Dr[row,col] = data\n",
    "        \n",
    "        #Sets the old values to compare with the new ones.\n",
    "        old_x = x\n",
    "        old_error = error\n",
    "        \n",
    "        #Computation of the Gauss-Newton iteration formula to obtain the new point x.\n",
    "        #The vector a is the solution of min_y |Ay - b|, with A = Dr(x) and b = -res(x). \n",
    "        [y,istop,itn,residualnorm,auxnorm,Drnorm,estimateDrcond,ynorm] = ssl.lsmr(Dr,-res,u,atol,btol)\n",
    "        x = x + y\n",
    "        \n",
    "        #Computation of the respective tensor S associated to x and its error.\n",
    "        S = point2tens(x,r,n)\n",
    "        error = np.linalg.norm(T-S)\n",
    "                \n",
    "        #Update the damping parameter. \n",
    "        g = 2*(old_error - error)/(old_residualnorm - residualnorm)\n",
    "        if g < 0.25:\n",
    "            u = 2*u\n",
    "        elif g > 0.75:\n",
    "            u = u/3\n",
    "        old_residualnorm = residualnorm\n",
    "        \n",
    "        #Update the arrays with information about the iteration.\n",
    "        step_sizes[it] = np.linalg.norm(x - old_x)   \n",
    "        errors[it] = error\n",
    "        xpath[it,:] = x\n",
    "        #After 10 iterations, the program starts to verify if the size of the current step or the difference between the errors are smaller than tol.\n",
    "        if it >= 10:\n",
    "            errors_diff = 1/Tsize*np.abs(errors[it] - errors[it-1])\n",
    "            if step_sizes[it] < tol or errors_diff < tol:\n",
    "                break\n",
    "        #Update the vectors L,X,Y,Z for the next iteration.\n",
    "        L = x[0:r]\n",
    "        X = x[r:r+r*n]\n",
    "        Y = x[r+r*n:r+2*r*n]\n",
    "        Z = x[r+2*r*n:r+3*r*n]\n",
    "        \n",
    "    step_sizes = step_sizes[0:it+1]\n",
    "    errors = errors[0:it+1]\n",
    "    xpath = xpath[0:it+1,:]\n",
    "\n",
    "    return(x,S,step_sizes,errors,xpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function does the same thing as *gauss_newton*, but with the difference that it measures the times of several parts in the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gauss_newton_timing(T,L,X,Y,Z,r,n,maxit=500,tol=10**(-3)):\n",
    "    S = np.zeros((n+1,n+1,n+1))\n",
    "    x = np.concatenate((L,X,Y,Z))\n",
    "    step_sizes = np.zeros(maxit)\n",
    "    errors = np.zeros(maxit)\n",
    "    xpath = np.zeros((maxit,r+3*r*n))\n",
    "    error = np.inf\n",
    "    Tsize = np.linalg.norm(T)\n",
    "    #Initialize the row and column indexes for sparse matrix creation.\n",
    "    [data,row,col] = initialize(r,n)\n",
    "    Dr = sparse.csr_matrix((data, (row, col)), shape=((n+1)**3,r+3*r*n))\n",
    "    #u is the damping parameter.\n",
    "    u = 1\n",
    "    atol = 10**(-5)\n",
    "    btol = 10**(-3)\n",
    "    old_residualnorm = 0\n",
    "    \n",
    "    sparse_time = np.zeros(maxit)\n",
    "    gauss_newton_time = np.zeros(maxit)\n",
    "    rest_time = np.zeros(maxit)\n",
    "    \n",
    "    #Gauss-Newton iterations starting at x.\n",
    "    for it in range(0,maxit):  \n",
    "        #Computation of r(x) and Dr(x).\n",
    "        start = time.time()\n",
    "        res = residuals(T,L,X,Y,Z,r,n)\n",
    "        data = derivative_residuals(L,X,Y,Z,r,n)\n",
    "        rest_time[it] = time.time() - start\n",
    "        \n",
    "        #Convert the matrix to a scipy sparse matrix.\n",
    "        start = time.time()\n",
    "        Dr[row,col] = data\n",
    "        sparse_time[it] = time.time() - start\n",
    "        \n",
    "        #Sets the old values to compare with the new ones.\n",
    "        start = time.time()\n",
    "        old_x = x\n",
    "        old_error = error\n",
    "        rest_time[it] = rest_time[it] + time.time() - start\n",
    "        \n",
    "        start = time.time()\n",
    "        #Computation of the Gauss-Newton iteration formula to obtain the new point x.\n",
    "        #The vector a is the solution of min_y |Ay - b|, with A = Dr(x) and b = -res(x). \n",
    "        [y,istop,itn,residualnorm,auxnorm,Drnorm,estimateDrcond,ynorm] = ssl.lsmr(Dr,-res,u,atol,btol)\n",
    "        x = x + y\n",
    "        gauss_newton_time[it] = time.time() - start\n",
    "        \n",
    "        start = time.time()\n",
    "        #Computation of the respective tensor S associated to x and its error.\n",
    "        S = point2tens(x,r,n)\n",
    "        error = np.linalg.norm(T-S)\n",
    "                \n",
    "        #Update the damping parameter. \n",
    "        g = 2*(old_error - error)/(old_residualnorm - residualnorm)\n",
    "        if g < 0.25:\n",
    "            u = 2*u\n",
    "        elif g > 0.75:\n",
    "            u = u/3\n",
    "        old_residualnorm = residualnorm\n",
    "        \n",
    "        #Update the arrays with information about the iteration.\n",
    "        step_sizes[it] = np.linalg.norm(x - old_x)   \n",
    "                \n",
    "        errors[it] = error\n",
    "        xpath[it,:] = x\n",
    "        #After 10 iterations, the program starts to verify if the size of the current step or the difference between the errors are smaller than tol.\n",
    "        if it >= 10:\n",
    "            errors_diff = 1/Tsize*np.abs(errors[it] - errors[it-1])\n",
    "            if step_sizes[it] < tol or errors_diff < tol:\n",
    "                break\n",
    "        #Update the vectors L,X,Y,Z for the next iteration.\n",
    "        L = x[0:r]\n",
    "        X = x[r:r+r*n]\n",
    "        Y = x[r+r*n:r+2*r*n]\n",
    "        Z = x[r+2*r*n:r+3*r*n]\n",
    "        rest_time[it] = rest_time[it] + time.time() - start\n",
    "        \n",
    "    step_sizes = step_sizes[0:it+1]\n",
    "    errors = errors[0:it+1]\n",
    "    xpath = xpath[0:it+1,:]\n",
    "    sparse_time = sparse_time[0:it+1]\n",
    "    gauss_newton_time = gauss_newton_time[0:it+1]\n",
    "    rest_time = rest_time[0:it+1]\n",
    "\n",
    "    return(x,S,step_sizes,errors,xpath,sparse_time,gauss_newton_time,rest_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python function *low_rank* tries to minimize the residual function calling several times the Gauss-Newton method. The user may choose the number of trials the program makes with the parameter *maxtrials*. Also, the user may choose the maximum number of iterations at each trials. Finally, the user may define the tolerance value to stop the iteration process. The parameter *tol* is passed to each Gauss_Newton trial, we also use this parameter to stop the program when $\\|T-S\\|/\\|T\\| < tol$ or the improvement of $\\|T-S\\|/\\|T\\|$ in some trial is less than *tol*. \n",
    "\n",
    "The first outputs of this function are essentially the best output of all functions *gauss_newton* computed. With respect to the point $x$, the program also returns the vectors $(\\Lambda,X,Y,Z)$, including the 0th entries (equal to 1). This can be useful if the user wants to use the CPD form of $S$. The latter outputs are general information about all the trials. These informations are the following:\n",
    "\n",
    "$\\bullet$ The total time spent in each trial.\n",
    "\n",
    "$\\bullet$ The number of steps used in each trial.\n",
    "\n",
    "$\\bullet$ The relative error $\\|T-S\\|/\\|T\\|$ obtained in each trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def low_rank(T,r,n,maxtrials,maxit=500,tol=10**(-3)):\n",
    "    times = np.zeros(maxtrials)\n",
    "    steps = np.zeros(maxtrials)\n",
    "    rel_errors = np.zeros(maxtrials)\n",
    "    best_X = np.ones(r*(n+1))\n",
    "    best_Y = np.ones(r*(n+1))\n",
    "    best_Z = np.ones(r*(n+1))\n",
    "    best_error = np.inf\n",
    "    \n",
    "    #Computation of the Frobenius norm of T.\n",
    "    Tsize = np.linalg.norm(T.flatten())\n",
    "    \n",
    "    #At each trial the program generates a random starting point (L,X,Y,Z) to apply the Gauss-Newton method.\n",
    "    for trial in range(0,maxtrials):\n",
    "        L = np.random.randn(r)\n",
    "        X = np.random.randn(r*n)\n",
    "        Y = np.random.randn(r*n)\n",
    "        Z = np.random.randn(r*n)\n",
    "        \n",
    "        #Computation of one Gauss-Newton method starting at (L,X,Y,Z).\n",
    "        start = time.time()\n",
    "        [x,S,step_sizes,errors,xpath] = gauss_newton(T,L,X,Y,Z,r,n,maxit,tol) \n",
    "        \n",
    "        #Update the vectors with general information.\n",
    "        times[trial] = time.time() - start\n",
    "        steps[trial] = step_sizes.shape[0]\n",
    "        rel_errors[trial] = 1/Tsize*errors[-1]\n",
    "                \n",
    "        #We use the subroutine \"pocket\" to save the best tensor S computed  so far.    \n",
    "        error = errors[-1]\n",
    "        if error < best_error:\n",
    "            best_x = x\n",
    "            best_L = x[0:r]\n",
    "            for l in range(0,r):\n",
    "                best_X[l*(n+1) + 1:(l+1)*(n+1)] = x[r + l*n:r + (l+1)*n]\n",
    "                best_Y[l*(n+1) + 1:(l+1)*(n+1)] = x[r + r*n + l*n:r + r*n + (l+1)*n]\n",
    "                best_Z[l*(n+1) + 1:(l+1)*(n+1)] = x[r + 2*r*n + l*n:r + 2*r*n + (l+1)*n]\n",
    "            best_S = S\n",
    "            old_best_error = best_error\n",
    "            best_error = error\n",
    "            best_step_sizes = step_sizes\n",
    "            best_errors = errors\n",
    "            best_xpath = xpath\n",
    "            #The search for better solutions stops when the best relative error is small enough or the improvement is irrelevant.\n",
    "            if trial > 1:\n",
    "                if best_error/Tsize < tol or np.abs(best_error - old_best_error) < Tsize*tol :\n",
    "                    break\n",
    "                \n",
    "    #After everything, we rename all the information related to the best S, for convenience.\n",
    "    x = best_x\n",
    "    L = best_L\n",
    "    X = best_X\n",
    "    Y = best_Y\n",
    "    Z = best_Z\n",
    "    S = best_S\n",
    "    error = best_error\n",
    "    step_sizes = best_step_sizes\n",
    "    errors = best_errors\n",
    "    xpath = best_xpath\n",
    "    times = times[0:trial+1]\n",
    "    steps = steps[0:trial+1]\n",
    "    rel_errors = rel_errors[0:trial+1]\n",
    "\n",
    "    return(x,L,X,Y,Z,S,error,step_sizes,errors,xpath,times,steps,rel_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function computes several approximations of $T$ for $r = 1 \\ldots n^2$. We use these computations to determine the (most probable) rank of $T$. The function also returns an array *rank_errors* with the relative errors for the rank varying from 1 to $r+1$, where $r$ is the computed rank of $T$. It is relevant to say that the value $r$ computed can also be the *border rank* of $T$, not the actual rank. \n",
    "\n",
    "The idea is that the minimum of $\\|T-S\\|$, for each rank $r$, stabilizes when $S$ has the same rank as $T$. This function also plots the graph of the errors so the user are able to visualize the moment when the error stabilize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank(T):\n",
    "    #R is an upper bound for the rank.\n",
    "    n = T.shape[0]-1\n",
    "    R = n**2\n",
    "    rank_errors = np.zeros(R)\n",
    "    Tsize = np.linalg.norm(T)\n",
    "    for r in range(1,R):\n",
    "        maxit = 100\n",
    "        L = np.random.randn(r)\n",
    "        X = np.random.randn(r*n)\n",
    "        Y = np.random.randn(r*n)\n",
    "        Z = np.random.randn(r*n)\n",
    "        [x,S,step_sizes,errors,xpath] = gauss_newton(T,L,X,Y,Z,r,n,maxit)\n",
    "        rank_errors[r-1] = errors[-1]/Tsize\n",
    "        if r > 1:\n",
    "            #Verification of the stabilization condition.\n",
    "            if np.abs(rank_errors[r-1] - rank_errors[r-2]) < tol:\n",
    "                break\n",
    "           \n",
    "    rank_errors = rank_errors[0:r] \n",
    "    \n",
    "    print('R(T) =',r-1)\n",
    "    print('|T-S|/|T| =',rank_errors[-2])\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.plot(range(1,r+1),np.log10(rank_errors))\n",
    "    plt.plot(r-1,np.log10(rank_errors[-2]),marker = 'o',color = 'k')\n",
    "    plt.title('Rank trials')\n",
    "    plt.xlabel('r')\n",
    "    plt.ylabel('$log10 \\|T - S\\|/|T|$')\n",
    "    plt.show()\n",
    "            \n",
    "    return(r-1,rank_errors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
