"""
 Initialization Module
 ===================
 As we mentioned in the main module *Tensor Fox*, the module *Construction* is responsible for constructing the more 
complicated objects necessary to make computations. Between these objects we have the array of residuals, the derivative 
of the residuals, the starting points to begin the iterations and so on. 
""" 

# Python modules
import numpy as np
from numpy import dot, empty, zeros, ones, float64, int64, arange, sqrt, inf, argmin, array, linspace
from numpy.linalg import norm, qr, svd
from numpy.random import randn, uniform, randint
import sys
import matplotlib.pyplot as plt
from numba import jit, njit, prange

# Tensor Fox modules
import Auxiliar as aux
import Compression as cmpr
import Conversion as cnv
import GaussNewton as gn


def starting_point(T, Tsize, S, U1, U2, U3, r, R1, R2, R3, ordering, options):
    """
    This function generates a starting point to begin the iterations of the Gauss-Newton method. There are three options:
        - list: the user may give a list [X,Y,Z] with the three arrays to use as starting point.
        - 'random': each entry of X, Y, Z are generated by the normal distribution with mean 0 and variance 1.
        - 'smart_random': generates a random starting point with a method based on the MLSVD which always guarantee a 
           small relative error. Check the function 'smart_random' for more details about this method.
        - 'smart': works similar as the smart_random method, but this one is deterministic and generates the best rank-r
          approximation based on the MLSVD.
    
    Inputs
    ------
    T: float 3-D ndarray
    Tsize: float
    S: float 3-D ndarray with shape (R1, R2, R3)
    U1: float 2-D ndarrays with shape (R1, r)
    U2: float 2-D ndarrays with shape (R2, r)
    U3: float 2-D ndarrays with shape (R3, r)
    r, R1, R2, R3: int
    init_method: string or list
       Method of initialization. The five methods were described above.
    symm: bool
    display: int
    
    Outputs
    -------
    X: float 2-D ndarray of shape (R1, r)
    Y: float 2-D ndarray of shape (R2, r)
    Z: float 2-D ndarray of shape (R3, r)
    rel_error: float
        Relative error associate to the starting point. More precisely, it is the relative error between T and 
        (U1,U2,U3)*S_init, where S_init = (X,Y,Z)*I.
    """

    # Extract all variable from the class of options.
    trunc_dims = options.trunc_dims
    level = options.level
    init_method = options.init_method
    low, upp, factor = options.constraints
    c = options.constant_norm
    symm = options.symm
    display = options.display
    
    if type(init_method) == list: 
        X = init_method[ordering[0]]
        Y = init_method[ordering[1]]
        Z = init_method[ordering[2]]  
        dims = [R1, R2, R3]
        X = dot(U1.T, X)
        Y = dot(U2.T, Y)
        Z = dot(U3.T, Z) 
                
    elif init_method == 'random':
        X = randn(R1, r)
        Y = randn(R2, r)
        Z = randn(R3, r)

    elif init_method == 'smart_random':
        X, Y, Z = smart_random(S, r, R1, R2, R3)

    elif init_method == 'smart':
        X, Y, Z = smart(S, r, R1, R2, R3)
        
    else:
        sys.exit('Error with init parameter.') 

    # Depending on the tensor, the factors X, Y, Z may have null entries. We want to
    # avoid that. The solution is to introduce some little random noise. 
    X, Y, Z = clean_zeros(S, X, Y, Z)

    # Make all factors balanced.
    X, Y, Z = cnv.equalize(X, Y, Z, r)

    # Apply additional transformations if requested.
    X, Y, Z = cnv.transform(X, Y, Z, R1, R2, R3, r, low, upp, factor, symm, c)
    
    if display > 2:
        # Computation of relative error associated with the starting point given.
        S_init = empty((R1, R2, R3), dtype = float64)
        S_init = cnv.cpd2tens(S_init, [X, Y, Z], (R1, R2, R3))
        S1_init = cnv.unfold(S_init, 1, (R1, R2, R3))
        rel_error = aux.compute_error(T, Tsize, S_init, S1_init, [U1, U2, U3], (R1, R2, R3))
        return X, Y, Z, rel_error

    return X, Y, Z


def smart_random(S, r, R1, R2, R3):
    """
    1 + int(sqrt(R1*R2*R3)) samples of random possible initializations are generated and compared. We keep the closest 
    to S_trunc. This method draws r points in S_trunc and generates a tensor with rank <= r from them. The distribution is 
    such that it tries to maximize the energy of the sampled tensor, so the error is minimized. Althought we are using the 
    variables named as R1, R2, R3, remember they refer to R1_trunc, R2_trunc, R3_trunc at the main function. Since this 
    function depends on the energy, it only makes sense using it when the original tensor can be compressed. If this is 
    not the case, avoid using this function.
    
    Inputs
    ------
    S: 3-D float ndarray
    r: int
    R1, R2, R3: int
        The dimensions of the truncated tensor S.
    samples: int
        The number of tensors drawn randomly. Default is 100.
        
    Outputs
    -------
    X: float 2-D ndarray of shape (R1, r)
    Y: float 2-D ndarray of shape (R2, r)
    Z: float 2-D ndarray of shape (R3, r)
    """
    
    # Initialize auxiliary values and arrays.
    samples = 1 + int(sqrt(R1*R2*R3))
    best_error = inf
    Ssize = norm(S)
    
    # Start search for a good initial point.
    for sample in range(0,samples):
        X, Y, Z = smart_sample(S, r, R1, R2, R3)
        S_init = empty((R1, R2, R3), dtype = float64)
        S_init = cnv.cpd2tens(S_init, [X, Y, Z], (R1, R2, R3))
        rel_error = norm(S - S_init)/Ssize
        if rel_error < best_error:
            best_error = rel_error
            best_X, best_Y, best_Z = X, Y, Z

    return best_X, best_Y, best_Z


@jit(nogil=True)
def smart_sample(S, r, R1, R2, R3):
    """
    We consider a distribution that gives more probability to smaller coordinates. This is because these are associated 
    with more energy. We choose a random number c1 in the integer interval [0, R1 + (R1-1) + (R1-2) + ... + 1]. 
    If 0 <= c1 < R1, we choose i = 1, if R1 <= c1 < R1 + (R1-1), we choose i = 2, and so on. The same goes for the other
    coordinates.
    Let S_{i_l,j_l,k_l}, l = 1...r, be the points chosen by this method. With them we form the tensor 
    S_init = sum_{l=1}^r S_{i_l,j_l,k_l} e_{i_l} ⊗ e_{j_l} ⊗ e_{k_l}, which should be close to S_trunc.
    
    Inputs
    ------
    S: 3-D float ndarray
    r: int
    R1, R2, R3: int
    
    Ouputs
    ------
    X: float 2-D ndarray of shape (R1, r)
    Y: float 2-D ndarray of shape (R2, r)
    Z: float 2-D ndarray of shape (R3, r)
    """
    
    # Initialize arrays to construct initial approximate CPD.
    X = zeros((R1, r), dtype = float64)
    Y = zeros((R2, r), dtype = float64)
    Z = zeros((R3, r), dtype = float64)
    # Construct the upper bounds of the intervals.
    arr1 = R1*ones(R1, dtype = int64) - arange(R1)
    arr2 = R2*ones(R2, dtype = int64) - arange(R2)
    arr3 = R3*ones(R3, dtype = int64) - arange(R3)
    high1 = np.sum(arr1)
    high2 = np.sum(arr2)
    high3 = np.sum(arr3)

    # Arrays with all random choices.
    C1 = randint(high1, size=r)
    C2 = randint(high2, size=r)  
    C3 = randint(high3, size=r)

    # Update arrays based on the choices made.
    for l in range(0,r):
        X[:,l], Y[:,l], Z[:,l] = assign_values(S, X, Y, Z, r, R1, R2, R3, C1, C2, C3, arr1, arr2, arr3, l) 
          
    return X, Y, Z


@jit(nogil=True)
def assign_values(S, X, Y, Z, r, R1, R2, R3, C1, C2, C3, arr1, arr2, arr3, l):
    """
    For each l = 1...r, this function constructs l-th one rank term in the CPD of the initialization tensor, which is of 
    the form S[i,j,k]*e_i ⊗ e_j ⊗ e_k for some i,j,k choosed through the random distribution described earlier.
    """
    
    for i in range(0,R1):
        if (np.sum(arr1[0:i]) <= C1[l]) and (C1[l] < np.sum(arr1[0:i+1])):
            X[i,l] = 1
            break
    for j in range(0,R2):
        if (np.sum(arr2[0:j]) <= C2[l]) and (C2[l] < np.sum(arr2[0:j+1])):
            Y[j,l] = 1
            break
    for k in range(0,R3):
        if (np.sum(arr3[0:k]) <= C3[l]) and (C3[l] < np.sum(arr3[0:k+1])):
            Z[k,l] = 1
            break   

    X[i,l] = S[i,j,k] 
        
    return X[:,l], Y[:,l], Z[:,l]


@njit(nogil=True)
def smart(S, r, R1, R2, R3):
    """
    Construct a truncated version of S with the r entries with higher energy. Let S_{i_l,j_l,k_l}, l = 1...r, be the 
    points chosen by this method. With them we form the tensor 
    S_init = sum_{l=1}^r S_{i_l,j_l,k_l} e_{i_l} ⊗ e_{j_l} ⊗ e_{k_l}, which should be close to S_trunc.
    
    Inputs
    ------
    S: 3-D float ndarray
    r: int
    R1, R2, R3: int
        The dimensions of the truncated tensor S.
            
    Outputs
    -------
    X: float 2-D ndarray of shape (R1, r)
    Y: float 2-D ndarray of shape (R2, r)
    Z: float 2-D ndarray of shape (R3, r)
    """

    # Find the entries of S with higher energy.
    largest = zeros(r, dtype=float64)
    indexes = zeros((r,3), dtype=int64)
    for i in range(R1):
        for j in range(R2):
            for k in range(R3):
                if np.abs(S[i,j,k]) > np.min(np.abs(largest)):
                    idx = argmin(np.abs(largest))
                    largest[idx] = S[i,j,k]
                    indexes[idx,:] = array([i,j,k])

    # Initialize the factors X, Y, Z.
    X = zeros((R1, r), dtype=float64)
    Y = zeros((R2, r), dtype=float64)
    Z = zeros((R3, r), dtype=float64)
    
    # Use the entries computed previously to generates the factors X, Y, Z.
    for l in range(r):
        i, j, k = indexes[l,:]
        X[i,l] = largest[l]
        Y[j,l] = 1
        Z[k,l] = 1
                    
    return X, Y, Z


@jit(nogil=True)
def clean_zeros(T, X, Y, Z):
    """
    Any null entry is redefined to be a small random number.
    """

    m, n, p = X.shape[0], Y.shape[0], Z.shape[0]
    r = X.shape[1]

    # Initialize the factors X, Y, Z with small noises to avoid null entries.
    s = 1/norm(1 + T.flatten())**2

    for i in range(m):
        for l in range(r):
            if X[i,l] == 0.0:
                X[i,l] = s*randn() 
    for j in range(n):
        for l in range(r):
            if Y[j,l] == 0.0:
                Y[j,l] = s*randn() 
    for k in range(p):
        for l in range(r):
            if Z[k,l] == 0.0:
                Z[k,l] = s*randn() 

    return X, Y, Z


def find_factor(T, Tsize, r, options, plot=False):
    # Prepare options and dimensions.
    m, n, p = T.shape
    T, ordering = aux.sort_dims(T, m, n, p)
    m, n, p = T.shape
    dims = (m, n, p)
    options = aux.complete_options(options, dims)
    options.display = 3
    display = 3
    trunc_dims = options.trunc_dims
    level = options.level
    options = aux.make_options(options, dims)
    N = 101

    if type(options.init_method) == list:
        print('Type of initialization: user')
    else:
        print('Type of initialization:', options.init_method)
    print()
     
    # compute compressed version of T   
    S, best_energy, R1, R2, R3, U1, U2, U3, sigma1, sigma2, sigma3, mlsvd_stop, best_error = cmpr.trimlsvd(T, Tsize, r, trunc_dims, level, display)

    # first run
    factors = linspace(0, 100, 100)
    errors = []
    i = 1
    print('First run')
    for factor in factors:
        X, Y, Z, rel_error = starting_point(T, Tsize, S, U1, U2, U3, r, R1, R2, R3, ordering, options)
        errors.append(rel_error)
        # display progress bar
        s = "[" + i*"=" + (N-i-1)*" " + "]" + " " + str(i) + "%"
        sys.stdout.write('\r'+s)
        i += 1

    # second run    
    best_factor = factors[argmin(errors)]
    factors = linspace(best_factor-1, best_factor+1, 100)
    errors = []
    i = 1
    print('\nSecond run')
    for factor in factors:
        X, Y, Z, rel_error = starting_point(T, Tsize, S, U1, U2, U3, r, R1, R2, R3, ordering, options) 
        errors.append(rel_error)
        s = "[" + i*"=" + (N-i-1)*" " + "]" + " " + str(i) + "%"
        sys.stdout.write('\r'+s)
        i += 1
    
    # final result    
    best_factor = factors[argmin(errors)]
    best_error = errors[argmin(errors)]

    # plot factor x error curve if requested
    if plot:
        plt.plot(factors, errors, '+')
        plt.plot(best_factor, best_error, 'r*', label='Optimal factor')
        plt.xlabel('Factor')
        plt.ylabel('Relative error')
        plt.grid()
        plt.legend()
        plt.show()
    
    return best_factor, best_error
