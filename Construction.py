"""
Construction Module
 
 As we mentioned in the main module *Tensor Fox*, the module *Construction* is responsible for constructing the more complicated objects necessary to make computations. Between these objects we have the array of residuals, the derivative of the residuals, the starting points to begin the iterations and so on. Below we list all funtions presented in this module.
 
 - residual
 
 - residual_entries
 
 - start_point
 
 - smart_random
 
 - smart_sample
 
 - assign_values
 
 - truncation
 
 - truncate1
 
 - truncate2
""" 


import numpy as np
import sys
import scipy.io
import time
import matplotlib.pyplot as plt
from scipy import sparse
from numba import jit, njit, prange
import Conversion as cnv
import Auxiliar as aux


@njit(nogil=True, parallel=True)
def residual(res, T, X, Y, Z, r, m, n, p):
    """
    This function computes (updates) the residuals between a 3-D tensor T in R^m⊗R^n⊗R^p
    and an approximation T_approx of rank r. The tensor T_approx is of the form
    T_approx = Lambda_1*X_1⊗Y_1⊗Z_1 + ... + Lambda_r*X_r⊗Y_r⊗Z_r, where
    X = [X_1, ..., X_r],
    Y = [Y_1, ..., Y_r],
    Z = [Z_1, ..., Z_r].
    
    The `residual map` is a map res:R^{r+r(m+n+p)}->R^{m*n*p}. For each i,j,k=0...n, the residual 
    r_{i,j,k} is given by res_{i,j,k} = T_{i,j,k} - sum_{l=1}^r Lambda_l*X_{il}*Y_{jl}*Z_{kl}.
    
    Inputs
    ------
    res: float 1-D ndarray with m*n*p entries 
        Each entry is a residual.
    T: float 3-D ndarray
    Lambda: float 1-D ndarray with r entries
    X: float 2-D ndarray of shape (m,r)
    Y: float 2-D ndarray of shape (n,r)
    Z: float 2-D ndarray of shape (p,r)
    r: int
    m,n,p: 
    
    Outputs
    -------
    res: float 1-D ndarray with m*n*p entries 
        Each entry is a residual.
    """   
    
    s = 0
    
    #Construction of the vector res = (res_{111}, res_{112}, ..., res_{mnp}).
    for i in prange(0,m):
        for j in range(0,n):
            for k in range(0,p):
                s = n*p*i + p*j + k
                res[s] = residual_entries(T, X, Y, Z, r, m, n, p, i, j, k)
                            
    return res


@njit(nogil=True)
def residual_entries(T, X, Y, Z, r, m, n, p, i, j, k):
    """ Computation of each individual residual in the residual function. """
    
    acc = 0.0
    for l in range(0,r):
        acc += X[i,l]*Y[j,l]*Z[k,l]
        
    res_ijk = T[i,j,k] - acc
        
    return res_ijk


def start_point(T, Tsize, S_trunc, U1_trunc, U2_trunc, U3_trunc, r, R1_trunc, R2_trunc, R3_trunc, init='smart_random'):
    """
    This function generates a starting point to begin the iterations of the
    Gauss-Newton method. There are three options:
        'fixed': for each values of rank and dimensions this option always
    generates the same starting point, which looks random. This is good when
    one want to change the code and compare performances.
        'random': each entry of Lambda, X, Y, Z are generated by the normal
    distribution with mean 0 and variance 1.
        'smart_random': generates a random starting point with a method which
    always guarantee a small relative error. Check the function 'smart' for 
    more details about this method.
    
    Inputs
    ------
    T: float 3-D ndarray
    Tsize: float
    S_trunc: float 3-D ndarray with shape (R1_trunc, R2_trunc, R3_trunc)
    U1_trunc: float 2-D ndarrays with shape (R1_trunc, r)
    U2_trunc: float 2-D ndarrays with shape (R2_trunc, r)
    U3_trunc: float 2-D ndarrays with shape (R3_trunc, r)
    r, R1_trunc, R2_trunc, R3_trunc: int
    init: string
       Method of initialization. The three method were described above.
    
    Outputs
    -------
    Lambda: float 1-D ndarray with r entries
    X: float 2-D ndarray with shape (m, r)
    Y: float 2-D ndarray with shape (n, r)
    Z: float 2-D ndarray with shape (p, r)
    rel_err: float
        Relative error associate to the starting point. More precisely, it is the relative 
    error between T and (U1_trunc,U2_trunc,U3_trunc)*S_init, where S_init = (X,Y,Z)*Lambda.
    """
    
    if init == 'fixed': 
        X = np.sin(np.arange(R1_trunc*r))/10
        X = X.reshape(R1_trunc,r)
        Y = np.cos(np.arange(R2_trunc*r))/10
        Y = Y.reshape(R2_trunc,r)
        Z = np.tan(np.arange(R3_trunc*r))/10
        Z = Z.reshape(R3_trunc,r)    
        
    elif init == 'random':
        X = np.random.randn(R1_trunc,r)
        Y = np.random.randn(R2_trunc,r)
        Z = np.random.randn(R3_trunc,r)
        
    elif init == 'smart_random':
        X, Y, Z = smart_random(S_trunc, r, R1_trunc, R2_trunc, R3_trunc)
        
    else:
        sys.exit('Error with `init` parameter.') 
    
    # Computation of relative error associated with the starting point given.
    T_aux = np.zeros(S_trunc.shape, dtype = np.float64)
    S_init = cnv.CPD2tens(T_aux, X, Y, Z, R1_trunc, R2_trunc, R3_trunc, r)
    T_init = aux.multilin_mult(S_init, U1_trunc, U2_trunc, U3_trunc, R1_trunc, R2_trunc, R3_trunc) 
    rel_err = np.linalg.norm(T - T_init)/Tsize
        
    return X, Y, Z, rel_err


def smart_random(S_trunc, r, R1, R2, R3, samples=100):
    """
    100 samples of random possible initializations are generated and compared. We
    keep the closest to S_trunc. This method draws r points in S_trunc and generates
    a tensor with rank <= r from them. The distribution is such that it tries to
    maximize the energy of the sampled tensor, so the error is minimized.
    Althought we are using the variables named as R1, R2, R3, remember they refer to
    R1_trunc, R2_trunc, R3_trunc at the main function.
    
    Inputs
    ------
    S_trunc: 3-D float ndarray
    r: int
    R1, R2, R3: int
        The dimensions of the truncated tensor S_trunc.
    samples: int
        The number of tensors drawn randomly. Default is 100.
        
    Outputs
    -------
    Lambda: float 1-D ndarray with r entries
    X: float 2-D ndarray of shape (m, R1)
    Y: float 2-D ndarray of shape (n, R2)
    Z: float 2-D ndarray of shape (p, R3)
    """
    
    # Initialize auxiliary values and arrays.
    best_loss = np.inf
    S_truncsize = np.linalg.norm(S_trunc)
    T_aux = np.zeros(S_trunc.shape, dtype = np.float64)

    # Start search for a good initial point.
    for sample in range(0,samples):
        X, Y, Z = smart_sample(S_trunc, r, R1, R2, R3)
        S_initial = cnv.CPD2tens(T_aux, X, Y, Z, R1, R2, R3, r)
        loss = np.linalg.norm(S_trunc - S_initial)/S_truncsize
        if loss < best_loss:
            best_loss = loss
            best_X, best_Y, best_Z = X, Y, Z

    return best_X, best_Y, best_Z


@jit(nogil=True, parallel=True)
def smart_sample(S_trunc, r, R1, R2, R3):
    """
    We consider a distribution that gives more probability to smaller coordinates. This 
    is because these are associated with more energy. We choose a random number c1 in the 
    integer interval [0, R1 + (R1-1) + (R1-2) + ... + 1]. If 0 <= c1 < R1, we choose i = 1,
    if R1 <= c1 < R1 + (R1-1), we choose i = 2, and so on. The same goes for the other
    coordinates.
    Let S_{i_l,j_l,k_l}, l = 1...r, be the points chosen by this method. With them we form
    the tensor S_init = sum_{l=1}^r S_{i_l,j_l,k_l} e_{i_l} ⊗ e_{j_l} ⊗ e_{k_l}, which 
    should be close to S_trunc.
    
    Inputs
    ------
    S_trunc: 3-D float ndarray
    r: int
    R1, R2, R3: int
    samples: int
    
    Ouputs
    ------
    Lambda: float 1-D ndarray with r entries
    X: float 2-D ndarray of shape (m, R1)
    Y: float 2-D ndarray of shape (n, R2)
    Z: float 2-D ndarray of shape (p, R3)
    """
    
    # Initialize arrays to construct initial approximate CPD.
    X = np.zeros((R1, r), dtype = np.float64)
    Y = np.zeros((R2, r), dtype = np.float64)
    Z = np.zeros((R3, r), dtype = np.float64)
    # Construct the upper bounds of the intervals.
    arr1 = R1*np.ones(R1, dtype = np.int64) - np.arange(R1)
    arr2 = R2*np.ones(R2, dtype = np.int64) - np.arange(R2)
    arr3 = R3*np.ones(R3, dtype = np.int64) - np.arange(R3)
    high1 = np.sum(arr1)
    high2 = np.sum(arr2)
    high3 = np.sum(arr3)

    # Arrays with all random choices.
    C1 = np.random.randint(high1, size=r)
    C2 = np.random.randint(high2, size=r)  
    C3 = np.random.randint(high3, size=r)

    # Update arrays based on the choices made.
    for l in prange(0,r):
        X[:,l], Y[:,l], Z[:,l] = assign_values(S_trunc, X, Y, Z, r, R1, R2, R3, C1, C2, C3, arr1, arr2, arr3, l) 
          
    return X, Y, Z


@jit(nogil=True)
def assign_values(S_trunc, X, Y, Z, r, R1, R2, R3, C1, C2, C3, arr1, arr2, arr3, l):
    """
    For each l = 1...r, this function constructs l-th one rank term in the CPD of the
    initialization tensor, which is of the form S_trunc[i,j,k]*e_i ⊗ e_j ⊗ e_k for some
    i,j,k choosed through the random distribution described earlier.
    """
    
    for i in range(0,R1):
        if (np.sum(arr1[0:i]) <= C1[l]) and (C1[l] < np.sum(arr1[0:i+1])):
            X[i,l] = 1
            break
    for j in range(0,R2):
        if (np.sum(arr2[0:j]) <= C2[l]) and (C2[l] < np.sum(arr2[0:j+1])):
            Y[j,l] = 1
            break
    for k in range(0,R3):
        if (np.sum(arr3[0:k]) <= C3[l]) and (C3[l] < np.sum(arr3[0:k+1])):
            Z[k,l] = 1
            break   

    X[i,l] = S_trunc[i,j,k] 
    
    return X[:,l], Y[:,l], Z[:,l]


def truncation(T, Tsize, S, U1, U2, U3, r, sigma1, sigma2, sigma3, energy):
    """
    This function computes an adequate truncation for the central tensor S of the
    HOSVD of T.
    There are four possibilities: 
        1) The user passes a list [r1, r2, r3] with the truncation dimensions 
        2) No truncation (energy == 100)
        3) Truncation by energy (1 <= energy < 100)
        4) Truncation by relative error (0 < energy < 1)
    The energy parameter is a multiple purpose parameter, acting as different things
    depending on the value passed to it.
    When energy is a list [r1, r2, r3], these values are the dimensions of the truncation.
    In this case the user must know something about the specifics of the problem, just 
    guessing dimensions is not recommended.
    When 1 <= energy < 100, the program uses the function 'truncate1', which computes
    the truncation with lowest energy bigger than the value 'energy'.
    When 0 < energy < 1, the program consider the value 'energy' as a relative error.
    Then it computes the smallest truncation (U1_trunc, U2_trunc, U3_trunc) * S_trunc 
    such that the relative error between T and (U1_trunc,U2_trunc,U3_trunc)*S_trunc is 
    lower than 'energy'.
    
    Inputs
    ------
    T: float 3-D ndarray
    Tsize: float
        norm of T.
    S: float 3-D ndarray
    U1, U2, U3: float 2-D ndarrays
    r: int 
    sigma1, sigma2, sigma3: float 1-D arrays
    energy: list or float
    
    Outputs
    -------
    S_trunc: float 3-D ndarray with shape R1_trunc x R2_trunc x R3_trunc
        Truncated central tensor.
    U1_trunc: float 2-D ndarrays with shape R1_trunc x r
    U2_trunc: float 2-D ndarrays with shape R2_trunc x r
    U3_trunc: float 2-D ndarrays with shape R3_trunc x r
    best_energy: float
        The energy of the truncation. The biggest is the energy, closer to T is the truncation
    R1_trunc, R2_trunc, R3_trunc: int
        The reduced dimensions obtained after truncating.
    rel_err: float
        Relative error associate to the truncation. More precisely, it is the relative error
    between T and (U1_trunc,U2_trunc,U3_trunc)*S_trunc.
    """

    if type(energy) == list:
        R1_trunc, R2_trunc, R3_trunc = energy 
        s1 = np.sum(sigma1[0 : R1_trunc]**2)
        s2 = np.sum(sigma2[0 : R2_trunc]**2)
        s3 = np.sum(sigma3[0 : R3_trunc]**2)
        S_energy = np.sum(sigma1**2) + np.sum(sigma2**2) + np.sum(sigma3**2)
        best_energy = 100*(s1 + s2 + s3)/S_energy
    
    elif energy == 100:
        best_energy = 100
        
    elif (energy >= 1) and (energy < 100) :
        best_energy, R1_trunc, R2_trunc, R3_trunc = truncate1(r, sigma1, sigma2, sigma3, energy=energy) 
        
    elif (energy > 0) and (energy < 1):
        best_energy, R1_trunc, R2_trunc, R3_trunc = truncate2(T, S, U1, U2, U3, r, sigma1, sigma2, sigma3, rel_error=energy)

    else:
        sys.exit('Invalid energy value.')
      
    # Construct truncations of S, U1, U2, U3.    
    if best_energy == 100:
        R1_trunc, R2_trunc, R3_trunc = sigma1.size, sigma2.size, sigma3.size
        S_trunc = S
        U1_trunc = U1
        U2_trunc = U2
        U3_trunc = U3  
        
    else:
        S_trunc = S[:R1_trunc, :R2_trunc, :R3_trunc]
        U1_trunc = U1[:, :R1_trunc]
        U2_trunc = U2[:, :R2_trunc]
        U3_trunc = U3[:, :R3_trunc]  
    
    # Computation of relative error associated with truncation. 
    T_trunc = aux.multilin_mult(S_trunc, U1_trunc, U2_trunc, U3_trunc, R1_trunc, R2_trunc, R3_trunc) 
    rel_err = np.linalg.norm(T - T_trunc)/Tsize
        
    return S_trunc, U1_trunc, U2_trunc, U3_trunc, best_energy, R1_trunc, R2_trunc, R3_trunc, rel_err


@njit(nogil=True)
def truncate1(r, sigma1, sigma2, sigma3, energy=100):
    """
    sigma1, sigma2, sigma3 are the list of singular values of the unfoldings of T. Using
    this lists we start to truncating S with respect to some energy in the interval [1,100],
    where 100 means no truncation at all.
    Remember the energy associated to some truncation S_trunc is the value |S_trunc|/|S|*100.
    Given the value 'energy', this function searches for the truncation more energy than
    'energy' but as close as possible to 'energy'. It is the infimum of all truncations with
    more energy than 'energy'.
    
    Inputs
    ------
    r: int
    sigma1, sigma2, sigma3: 1-D float ndarrays
    energy: float    

    Outputs
    -------
    best_energy: float
        Energy of the best truncation obtained. 
    best_r1, best_r2, best_r3: int
        Dimensions of the best truncation obtained.
    """
    
    # Initialize values and arrays.
    best_energy = 100
    R1, R2, R3 = sigma1.size, sigma2.size, sigma3.size
    best_r1, best_r2, best_r3 = R1, R2, R3
    S_energy = np.sum(sigma1**2) + np.sum(sigma2**2) + np.sum(sigma3**2)
    
    # Start to search for the best truncation based on energy.
    for r1 in range(2, R1+1): 
        s1 = np.sum(sigma1[0:r1]**2)
        for r2 in range(2, R2+1):
            s2 = np.sum(sigma2[0:r2]**2)
            for r3 in range(2, R3+1):
                s3 = np.sum(sigma3[0:r3]**2)
                # Compute the total energy 100*|S_trunc|^2/|S|^2 of each truncation.
                total_energy = 100*(s1 + s2 + s3)/S_energy
                max_r = min(r1*r2, r1*r3, r2*r3)
                # Verify if the energy is good enough. 
                if (total_energy > energy) and (total_energy < best_energy) and (r <= max_r):
                    best_energy = total_energy
                    best_r1, best_r2, best_r3 = r1, r2, r3
                                          
    return best_energy, best_r1, best_r2, best_r3


@jit(nogil=True)
def truncate2(T, S, U1, U2, U3, r, sigma1, sigma2, sigma3, rel_error=0.05):
    """
    This function computes the smallest truncation with relative error < 'rel_error' 
    with respect to the original tensor, after transformations. More precisely, this 
    relative error is given by |T - (U1_trunc, U2_trunc, U3_trunc)*S_trunc|/|T|.
    Only truncations with more than 80% of the total energy are inspected.
    
    Inputs
    ------
    T: float 3-D ndarray
    S: float 3-D ndarray
    U1, U2, U3: float 2-D ndarrays
    r: int
    sigma1, sigma2, sigma3: float 1-D arrays
    rel_error: float
        We have 0 < rel_error < 1, with default of 0.05.
        
    Outputs
    -------
    best_energy: float
        Energy of the best truncation obtained.
    best_r1, best_r2, best_r3: int
        Dimensions of the best truncation obtained.    
    """
    
    # Initialize values and arrays.
    best_energy = 100
    R1, R2, R3 = sigma1.size, sigma2.size, sigma3.size
    best_r1, best_r2, best_r3 = R1, R2, R3
    best_dims = R1*R2*R3
    Tsize = np.sqrt(np.sum(T**2))
    S_energy = np.sum(sigma1**2) + np.sum(sigma2**2) + np.sum(sigma3**2)
    
    # Compute minimal dimensions of the truncations.
    R1_low = int(R1/2)
    R2_low = int(R2/2)
    R3_low = int(R3/2)
            
    # Start to search for the best truncation based on relative error.
    for r1 in range(R1_low, R1+1): 
        for r2 in range(R2_low, R2+1):
            for r3 in range(R3_low, R3+1):
                # Construct the truncation for each energy.
                S_trunc = S[:r1, :r2, :r3]
                U1_trunc = U1[:, :r1]
                U2_trunc = U2[:, :r2]
                U3_trunc = U3[:, :r3]  
                # Compute the relative error of this truncation.
                T_trunc = aux.multilin_mult(S_trunc, U1_trunc, U2_trunc, U3_trunc, r1, r2, r3) 
                rel_err = np.sqrt(np.sum((T - T_trunc)**2))/Tsize
                max_r = min(r1*r2, r1*r3, r2*r3)
                # Verify if this truncation is good enough.
                if (rel_err < rel_error) and (r1*r2*r3 < best_dims) and (r <= max_r):
                    s1 = np.sum(sigma1[0:r1]**2)
                    s2 = np.sum(sigma2[0:r2]**2)
                    s3 = np.sum(sigma3[0:r3]**2)
                    best_energy = 100*(s1 + s2 + s3)/S_energy
                    best_dims = r1*r2*r3
                    best_r1, best_r2, best_r3 = r1, r2, r3
                    return best_energy, best_r1, best_r2, best_r3
           
    return best_energy, best_r1, best_r2, best_r3
