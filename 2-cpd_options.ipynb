{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPD Options\n",
    "\n",
    "The cpd function has several options at disposal. Some of them may improve performance, precision or give insight about the tensor at hand. If you look at the source code, the cpd is defined this way:\n",
    "\n",
    ">def cpd(T, r, energy=0.05, maxiter=200, tol=1e-4, init='smart_random', display='none'):\n",
    "\n",
    "We will see all these parameters now. Let's start importing the necessary modules and creating the same tensor of the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import TensorFox as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [1. 2.]]\n",
      "\n",
      "[[1. 2.]\n",
      " [2. 3.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and print the tensor.\n",
    "T = np.zeros((2,2,2))\n",
    "for i in range(0,2):\n",
    "    for j in range(0,2):\n",
    "        for k in range(0,2):\n",
    "            T[i,j,k] = i+j+k\n",
    "            \n",
    "tf.showtens(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The *Display* Option\n",
    "\n",
    "There are three choices for the *display* option: 'none' (default), 'partial' and 'full'. These options controls what the user can see during the computations. Previously we let the default option and there were no output whatsoever. The 'partial' option shows useful information about the principal stages of the computation. The 'full' option shows everything the 'partial' option shows plus information about each iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "Starting computation of the HOSVD of T.\n",
      "------------------------------------------------------------------------------\n",
      "No compression detected.\n",
      "------------------------------------------------------------------------------\n",
      "Starting truncation.\n",
      "No truncation detected.\n",
      "------------------------------------------------------------------------------\n",
      "Initialization: smart random\n",
      "Relative error of initial guess = 0.11882705024255454\n",
      "------------------------------------------------------------------------------\n",
      "Starting damped Gauss-Newton method.\n",
      "------------------------------------------------------------------------------\n",
      "Starting refinement.\n",
      "------------------------------------------------------------------------------\n",
      "Number of steps = 46\n",
      "Final Relative error = 2.2288621390821184e-05\n"
     ]
    }
   ],
   "source": [
    "# Compute the CPD of T with partial display.\n",
    "r = 2\n",
    "Lambda, X, Y, Z, T_approx, rel_err, step_sizes_trunc, step_sizes_ref, errors_trunc, errors_ref = tf.cpd(T, r, display='partial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "Starting computation of the HOSVD of T.\n",
      "------------------------------------------------------------------------------\n",
      "No compression detected.\n",
      "------------------------------------------------------------------------------\n",
      "Starting truncation.\n",
      "No truncation detected.\n",
      "------------------------------------------------------------------------------\n",
      "Initialization: smart random\n",
      "Relative error of initial guess = 0.11882705024255454\n",
      "------------------------------------------------------------------------------\n",
      "Starting damped Gauss-Newton method.\n",
      "Iteration | Step Size | Rel Error | Line Search\n",
      "    1     |  3.35168  |  0.11176  |  Success\n",
      "    2     |  0.05219  |  0.10959  |  Success\n",
      "    3     |  0.07330  |  0.10702  |  Success\n",
      "    4     |  0.41478  |  0.08993  |  Success\n",
      "    5     |  0.45392  |  0.04825  |  Success\n",
      "    6     |  0.61860  |  0.05210  |  Fail\n",
      "    7     |  0.12039  |  0.01372  |  Success\n",
      "    8     |  0.38310  |  0.02438  |  Fail\n",
      "    9     |  0.06480  |  0.00733  |  Success\n",
      "    10     |  0.30550  |  0.01488  |  Fail\n",
      "    11     |  0.05163  |  0.00455  |  Success\n",
      "    12     |  0.27675  |  0.01116  |  Fail\n",
      "    13     |  0.04777  |  0.00301  |  Success\n",
      "    14     |  0.26978  |  0.00955  |  Fail\n",
      "    15     |  0.04718  |  0.00205  |  Success\n",
      "    16     |  0.27307  |  0.00880  |  Fail\n",
      "    17     |  0.04807  |  0.00142  |  Success\n",
      "    18     |  0.28188  |  0.00842  |  Fail\n",
      "    19     |  0.04977  |  0.00099  |  Success\n",
      "    20     |  0.29406  |  0.00822  |  Fail\n",
      "    21     |  0.05200  |  0.00069  |  Success\n",
      "    22     |  0.30860  |  0.00811  |  Fail\n",
      "    23     |  0.05461  |  0.00049  |  Success\n",
      "    24     |  0.32499  |  0.00806  |  Fail\n",
      "    25     |  0.05753  |  0.00034  |  Success\n",
      "    26     |  0.34298  |  0.00802  |  Fail\n",
      "    27     |  0.06072  |  0.00024  |  Success\n",
      "    28     |  0.36242  |  0.00800  |  Fail\n",
      "    29     |  0.06417  |  0.00017  |  Success\n",
      "    30     |  0.38336  |  0.00799  |  Fail\n",
      "    31     |  0.06784  |  0.00012  |  Success\n",
      "    32     |  0.40571  |  0.00798  |  Fail\n",
      "    33     |  0.07183  |  0.00009  |  Success\n",
      "    34     |  0.42803  |  0.00792  |  Fail\n",
      "    35     |  0.07621  |  0.00006  |  Success\n",
      "    36     |  0.45543  |  0.00799  |  Fail\n",
      "    37     |  0.08060  |  0.00004  |  Success\n",
      "    38     |  0.48053  |  0.00793  |  Fail\n",
      "    39     |  0.08550  |  0.00003  |  Success\n",
      "    40     |  0.51108  |  0.00799  |  Fail\n",
      "    41     |  0.00105  |  0.00002  |  Fail\n",
      "    42     |  0.00166  |  0.00002  |  Success\n",
      "------------------------------------------------------------------------------\n",
      "Starting refinement.\n",
      "Iteration | Step Size | Rel Error | Line Search\n",
      "    1     |  0.00000  |  0.00002  |  Fail\n",
      "    2     |  0.00000  |  0.00002  |  Success\n",
      "    3     |  0.00000  |  0.00002  |  Success\n",
      "    4     |  0.00000  |  0.00002  |  Success\n",
      "------------------------------------------------------------------------------\n",
      "Number of steps = 46\n",
      "Final Relative error = 2.2288621390821184e-05\n"
     ]
    }
   ],
   "source": [
    "# Compute the CPD of T with full display.\n",
    "Lambda, X, Y, Z, T_approx, rel_err, step_sizes_trunc, step_sizes_ref, errors_trunc, errors_ref = tf.cpd(T, r, display='full')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "The iteration process needs a starting point for iterating. This starting point depends on the 'init' option, and there are three possible choices in this case: 'smart_random' (default), 'random', 'fixed'. The 'smart_random' option generates a random CPD of rank $r$ with a original strategy, which makes the starting point to have small relative error, so it is already close to the objective tensor. The 'random' option generates a CPD of rank $r$ with entries drawn by the Normal Distribution. The relative error in this case usually is close to $1$. Finally, there is the 'fixed' option, which generates always the same CPD for the same $r$ and the same dimensions. This is good if the user want to change the code and compare performance.\n",
    "\n",
    "As we can see in the previous outputs, the initialization used was 'smart_random', and the relative error obtained was $0.1188$ approximtely. With this we could achieve a CPD in $46$ steps, and the respective relative error is of $2.2288 \\cdot 10^{-5}$ approximately. Let's see what we get from the other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "Starting computation of the HOSVD of T.\n",
      "------------------------------------------------------------------------------\n",
      "No compression detected.\n",
      "------------------------------------------------------------------------------\n",
      "Starting truncation.\n",
      "No truncation detected.\n",
      "------------------------------------------------------------------------------\n",
      "Initialization: random\n",
      "Relative error of initial guess = 1.098724120998715\n",
      "------------------------------------------------------------------------------\n",
      "Starting damped Gauss-Newton method.\n",
      "------------------------------------------------------------------------------\n",
      "Starting refinement.\n",
      "------------------------------------------------------------------------------\n",
      "Number of steps = 44\n",
      "Final Relative error = 2.0570294223419894e-05\n"
     ]
    }
   ],
   "source": [
    "# Compute the CPD of T with random initialization.\n",
    "Lambda, X, Y, Z, T_approx, rel_err, step_sizes_trunc, step_sizes_ref, errors_trunc, errors_ref = tf.cpd(T, r, init='random', display='partial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "Starting computation of the HOSVD of T.\n",
      "------------------------------------------------------------------------------\n",
      "No compression detected.\n",
      "------------------------------------------------------------------------------\n",
      "Starting truncation.\n",
      "No truncation detected.\n",
      "------------------------------------------------------------------------------\n",
      "Initialization: fixed\n",
      "Relative error of initial guess = 1.000170243379101\n",
      "------------------------------------------------------------------------------\n",
      "Starting damped Gauss-Newton method.\n",
      "------------------------------------------------------------------------------\n",
      "Starting refinement.\n",
      "------------------------------------------------------------------------------\n",
      "Number of steps = 44\n",
      "Final Relative error = 2.4940541716982312e-05\n"
     ]
    }
   ],
   "source": [
    "# Compute the CPD of T with fixed initialization.\n",
    "Lambda, X, Y, Z, T_approx, rel_err, step_sizes_trunc, step_sizes_ref, errors_trunc, errors_ref = tf.cpd(T, r, init='fixed', display='partial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Maxiter* and *Tol*\n",
    "\n",
    "As the names suggest, 'maxiter' is the maximum number of iterations permitted, while 'tol' is the tolerance parameter, gives a stopping criterion to stop iterating. Both values are related in the sense we should increase 'maxiter' when we decrease 'tol'. Of course in this little example this might not matter, but for larger tensors we may want to increase precision by decreasing 'tol'. In this case the algorithm can reach the maximum number permitted of iterations, so we should increase 'maxiter' to keep iterating.\n",
    "\n",
    "Let's decrease 'tol' and see if we get better approximations for the CPD. We will use 'tol' = 1e-10 and default initialization and display partial output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "Starting computation of the HOSVD of T.\n",
      "------------------------------------------------------------------------------\n",
      "No compression detected.\n",
      "------------------------------------------------------------------------------\n",
      "Starting truncation.\n",
      "No truncation detected.\n",
      "------------------------------------------------------------------------------\n",
      "Initialization: smart random\n",
      "Relative error of initial guess = 0.11882705024255454\n",
      "------------------------------------------------------------------------------\n",
      "Starting damped Gauss-Newton method.\n",
      "------------------------------------------------------------------------------\n",
      "Starting refinement.\n",
      "------------------------------------------------------------------------------\n",
      "Number of steps = 52\n",
      "Final Relative error = 1.3761033160356665e-05\n"
     ]
    }
   ],
   "source": [
    "# Compute the CPD of T with tol = 1e-10.\n",
    "Lambda, X, Y, Z, T_approx, rel_err, step_sizes_trunc, step_sizes_ref, errors_trunc, errors_ref = tf.cpd(T, r, tol=1e-10, display='partial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just more six iterations we could decrease the relative error nearly by a half. Remember that the previous error was of $2.2288 \\cdot 10^{-5}$. \n",
    "\n",
    "Sometimes the tolerance parameter may not behave as expected. Decreasing this value makes the algorithm perform more iterations, but also make it follows a different path in the space of tensors. This path can be worse sometimes, and in this case the user could achieve worse results. This is just bad luck and in this case we can increase 'maxiter' or just repeat the computation (which will generate another initialization, maybe better)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy\n",
    "\n",
    "Consider a matrix $A \\in \\mathbb{R}^{m \\times n}$ and its reduced SVD \n",
    "\n",
    "$$A = U \\Sigma V^T = [U_1 \\ldots U_n] \\cdot \\text{diag}(\\sigma_1, \\ldots, \\sigma_n) \\cdot [V_1 \\ldots V_m]^T.$$ \n",
    "\n",
    "It is commom to truncate $\\Sigma$ in order to obtain the *truncate SVD* of $A$ given by \n",
    "\n",
    "$$\\tilde{A} = [U_1 \\ldots U_p] \\cdot \\text{diag}(\\sigma_1, \\ldots, \\sigma_p) \\cdot [V_1 \\ldots V_p]^T,$$\n",
    "where $p < n$.\n",
    "\n",
    "There are several application in this procedure we won't discuss here. We just want to mention that the sum $\\sigma_1^2 + \\ldots + \\sigma_p^2$ is called the *energy* of $\\tilde{A}$. The more energy the truncation has, more close to $A$ it is. On the other hand, less energy means more truncation, which means fewer dimensions to take in account, and this translate to less computational time. As you can see, there is a trade off between proximity and dimensionality. We want to truncate as much as possible, but keeping the truncation close enough to $A$. \n",
    "\n",
    "With the 'energy' parameter the user can impose the least energy permitted at the truncation stage. For example, with we set 'energy' = $95$, the program will search for the truncation with lowest energy bigger than $95$. In this context it means $95 \\%$, i.e., the truncation retains $95 \\%$ of the energy of the original tensor. \n",
    "\n",
    "This is valid whenever $1 \\leq $ 'energy' $ < 100$. If the user chooses a value betwenn $0$ and $1$ (exclusive for both), then the program uses another strategy. It will search for a truncation with more than half of the original dimensions and with relative error smaller than 'energy'. This procedure starts with the smaller tensors, i.e., the program tries to truncate the most it can, provided that the relative error is smaller than 'energy'. This second approach performed better in practice, so the default is 'energy' = $0.05$.\n",
    "\n",
    "We also have an advanced use for 'energy'. If the user knows what truncation to use, it can ba passed as a list with three numbers, the dimensions of the truncation.\n",
    "\n",
    "Since the example showed here is too much simple, truncating it is just not possible. In the next sections we will see a problem which needs to be truncated in order to be computed in a reasonable time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
