{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate options\n",
    "\n",
    "In the previous lesson we talked about the following options:\n",
    "\n",
    "    display\n",
    "    maxiter  \n",
    "    tol \n",
    "    \n",
    "These options tend to be the standard in many iterative algorithms and the user may be satisfied with this. In fact, Tensor Fox was constructed in order to be as robust as possible. The several extra options should be used only when the options above are not enough (and when this happens, be sure you are handling a difficult tensor).\n",
    "\n",
    "The following options will be introduced now:\n",
    "\n",
    "    trunc_dims\n",
    "    energy\n",
    "    init_method\n",
    "    refine    \n",
    "    init_damp\n",
    "    symm    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import TensorFox as tfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [2. 3.]]\n",
      "\n",
      "[[4. 5.]\n",
      " [6. 7.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and print the tensor.\n",
    "m = 2\n",
    "T = np.zeros((m, m, m))\n",
    "s = 0\n",
    "\n",
    "for k in range(m):\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            T[i,j,k] = s\n",
    "            s += 1\n",
    "                    \n",
    "tfx.disp.showtens(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncation\n",
    "\n",
    "In applications of linear algebra, often one have to compute the truncated SVD of some matrix. By truncating we can reduce the dimensionality of the problem, which leads to lots of speed up for the upcoming algorithms. On the other hand, we lost information after the truncation. Ideally, we want to truncate as much as possible without losing the relevant information.\n",
    "\n",
    "The same can be made for tensors, but in this context we use the [multilinear singular value decomposition](https://epubs.siam.org/doi/abs/10.1137/s0895479896305696) (MLSVD). If $T$ is a $L$-order tensor, then its MLSVD is written as $T = (U_1, \\ldots, U_L) \\cdot S$, where each $U_l$ is a orthogonal matrix and $S$ is a tensor with the same shape as $T$ (we consider $S$ as the *compressed* version of $T$). For the interested people, the notation used stands for the [multilinear multiplication](https://en.wikipedia.org/wiki/Multilinear_multiplication). This tensor $S$ is called the *central tensor* and it is the analogous of $\\Sigma$ in the classical SVD of the form $A = U \\Sigma V^T$. Note that we've said that $S$ is of the same shape as $T$, and just as in the 2D linear algebra, this is the *full* MLSVD, in contrast to the *reduced* MLSVD. In the same way we can consider the reduced SVD $A = U \\Sigma V^T$ where $\\Sigma$ is $R \\times R$ (and $R$ is the rank of $A$), we can have $S$ of shape $R_1 \\times R_2 \\times \\ldots \\times R_L$. The tuple $(R_1, R_2, \\ldots, R_L)$ is the *multilinear rank* of $T$. \n",
    "\n",
    "At first all we have is the full MLSVD, by Tensor Fox works the truncation for you at your taste. More precisely, you can tell the program if you want to truncate \"a lot\", \"just a few\", not truncate at all, etc. You can even tell the program to not compute the MLSVD and work with the original tensor (I do not advise to do that!). You communicate the program your level of truncation through the parameter *energy*, where low energy translates to truncate a large amount and large energy translate to truncate small ammounts. After the full MLSVD is performed, the program tries to find the truncation with smallest dimensions with energy closest to *energy*. The default is *energy* $= 0.99999$, which translate to truncations with at least $99.999 \\%$ of energy. Setting *energy* $= 1$ means to use the full tensor $S$ obtained my the MLSVD, and *energy* $= -1$ means to use the original tensor. In this case the MLSVD is not performed. Sometimes this can speed up the program, but generally avoid compressing is not recommended.\n",
    " \n",
    "If you are working with tensor with order higher than $3$, then you can pass *energy* as a list with two values. The first one works the truncation of the original high order tensor. The second one works the truncations of the third order tensors of the CPD tensor train associated to the original tensor (see next lesson). If you set *energy* to a single value, the program assumes you want to use this value for both high order and third order tensors. Finally, we want to mention that you can also use the parameter *trunc_dims* to tell the program the exactly truncation you want to use. Just set this parameter to be a list of the dimensions you want (default is *trunc_dims = 0*, which lets the program to decide the best truncation). If you are work with tensors with order higher than $3$, this parameter refers to the original high order tensor only. \n",
    "\n",
    "In the example we are working it is possible to note that the program is unable to truncate. Still, we can force some truncation, say $2 \\times 1 \\times 1$, and see if we get good precision of this. The precision can already be antecipated just by seeing the relative error of the compression (remember that this requires setting *display* to $3$, which can be costly), that is, the line\n",
    "\n",
    "    Compression relative error = 0.130828\n",
    "\n",
    "That line says this is the best precision we can get. This is because this is the error of the truncated $S$ and $T$, and all the iterations to be made will try to obtain a CPD for $S$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------\n",
      "Computing MLSVD\n",
      "    Compression detected\n",
      "    Compressing from (2, 2, 2) to (2, 1, 1)\n",
      "    99.31 % of the energy was retained\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Type of initialization: random\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing CPD\n",
      "===============================================================================================\n",
      "Final results\n",
      "    Number of steps = 23\n",
      "    Relative error = 0.1308280869848383\n",
      "    Accuracy =  86.91719 %\n"
     ]
    }
   ],
   "source": [
    "class options:\n",
    "    display = 1\n",
    "    trunc_dims = [2,1,1]\n",
    "\n",
    "r = 3\n",
    "factors, T_approx, output = tfx.cpd(T, r, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "The iteration process needs a starting point for iterating. This starting point depends on the *init_method* option, and there are three possible choices in this case: *smart*, *smart_random*, *random* (default), and *user*. Both *smart* and *smart_random* options generates a CPD of rank $r$ with a strategy relying on the MLSVD. The strategy *smart* maximizes the energy of the initialization wheareas *smart random* makes almost the same, but with a chance to take some different entries. These strategies generates starting points with small relative error, so it is already close to the objective tensor. Although this seems to be a good thing, there is also a risk to be close to a local minimum or saddle point, and in this cases these methods will always fail. The *random* is more robust, this option generates a CPD of rank $r$ with entries drawn from the normal distribution. The relative error in this case usually is close to $1$. Finally, there is the 'user' option where the user provides a list $[X, Y, Z]$ as starting point. This is a good idea when we already have a close CPD and want to increase its precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------\n",
      "Computing MLSVD\n",
      "    No compression detected\n",
      "    Working with dimensions (2, 2, 2)\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Type of initialization: smart_random\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing CPD\n",
      "===============================================================================================\n",
      "Final results\n",
      "    Number of steps = 43\n",
      "    Relative error = 6.619903317471826e-05\n",
      "    Accuracy =  99.99338 %\n"
     ]
    }
   ],
   "source": [
    "# Compute the CPD of T with random initialization. \n",
    "# Notice we need to set trunc_dims to zero so the program can decide which truncation to use.\n",
    "options.trunc_dims = 0\n",
    "options.init_method = 'smart_random'\n",
    "factors, T_approx, output = tfx.cpd(T, r, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------\n",
      "Computing MLSVD\n",
      "    No compression detected\n",
      "    Working with dimensions (2, 2, 2)\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Type of initialization: user\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing CPD\n",
      "===============================================================================================\n",
      "Final results\n",
      "    Number of steps = 8\n",
      "    Relative error = 0.13079807315002634\n",
      "    Accuracy =  86.92019 %\n"
     ]
    }
   ],
   "source": [
    "# Compute the CPD of T with user initialization.\n",
    "X = np.ones((m, r))\n",
    "Y = np.ones((m, r))\n",
    "Z = np.ones((m, r))\n",
    "options.init_method = [X,Y,Z]\n",
    "factors, T_approx, info = tfx.cpd(T, r, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refinement\n",
    "\n",
    "As we mentioned before, the user may give an initial CPD as starting point for our iterative algorithm, which may be a good idea when it is desired to increase the precision of the CPD. This process can be done automatically by setting *refine* to True. This option makes the program runs the algorithm two times, where the second run uses the approximated CPD computed in the first run as starting point. However, this second run is made in the original space (the space of the tensor $T$). Ideally, we want to compress and limit ourselves to the compressed version of $T$, but if this is not enough, the *refine* option can squeeze more precision at a cost of working with uncompressed tensors. This options obly work for third order tensors. If you are working with a high order tensor, the program will use this options only for the intermediate third order tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------\n",
      "Computing MLSVD\n",
      "    No compression detected\n",
      "    Working with dimensions (2, 2, 2)\n",
      "    Compression relative error = 0.000000e+00\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Type of initialization: random\n",
      "    Initial guess relative error = 1.024220e+00\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing CPD\n",
      "    Iteration | Rel error | Improvement | norm(grad) | Predicted error | # Inner iterations\n",
      "        1     | 6.43e-01  |  6.43e-01   |  1.22e+01  |    7.49e-02     |        2        \n",
      "        2     | 4.01e-01  |  2.42e-01   |  1.63e+01  |    3.50e-04     |        3        \n",
      "        3     | 2.88e-01  |  1.14e-01   |  1.29e+01  |    1.14e-04     |        3        \n",
      "        4     | 1.94e-01  |  9.34e-02   |  9.05e+00  |    1.00e-04     |        4        \n",
      "        5     | 1.33e-01  |  6.10e-02   |  4.81e+00  |    7.32e-05     |        5        \n",
      "        6     | 8.39e-02  |  4.95e-02   |  2.33e+00  |    2.11e-04     |        5        \n",
      "        7     | 3.02e-02  |  5.38e-02   |  7.63e-01  |    3.92e-04     |        5        \n",
      "        8     | 1.90e-02  |  1.12e-02   |  2.92e-01  |    7.52e-05     |        4        \n",
      "        9     | 1.41e-02  |  4.88e-03   |  1.29e-01  |    1.60e-05     |        6        \n",
      "       10     | 1.13e-02  |  2.78e-03   |  5.39e-02  |    3.39e-04     |        6        \n",
      "       11     | 8.45e-03  |  2.88e-03   |  3.14e-02  |    7.86e-05     |        8        \n",
      "       12     | 5.86e-03  |  2.58e-03   |  4.02e-02  |    1.08e-04     |        7        \n",
      "       13     | 2.60e-03  |  3.26e-03   |  3.04e-02  |    5.59e-06     |        11       \n",
      "       14     | 1.15e-03  |  1.45e-03   |  2.38e-02  |    2.53e-06     |        6        \n",
      "       15     | 6.00e-04  |  5.54e-04   |  5.56e-03  |    3.88e-07     |        5        \n",
      "       16     | 3.80e-04  |  2.20e-04   |  2.50e-03  |    4.64e-07     |        5        \n",
      "       17     | 2.02e-04  |  1.78e-04   |  1.58e-03  |    6.26e-07     |        5        \n",
      "       18     | 1.78e-04  |  2.40e-05   |  1.05e-03  |    8.64e-07     |        1        \n",
      "       19     | 1.71e-04  |  7.02e-06   |  2.01e-03  |    5.59e-07     |        1        \n",
      "       20     | 1.64e-04  |  6.47e-06   |  9.87e-04  |    7.97e-07     |        1        \n",
      "       21     | 1.58e-04  |  6.28e-06   |  1.92e-03  |    5.00e-07     |        1        \n",
      "       22     | 1.52e-04  |  5.92e-06   |  8.92e-04  |    6.97e-07     |        1        \n",
      "       23     | 1.46e-04  |  5.82e-06   |  1.77e-03  |    4.34e-07     |        1        \n",
      "       24     | 1.41e-04  |  5.48e-06   |  8.31e-04  |    6.01e-07     |        1        \n",
      "       25     | 1.36e-04  |  5.40e-06   |  1.64e-03  |    3.74e-07     |        1        \n",
      "       26     | 1.30e-04  |  5.08e-06   |  7.70e-04  |    5.16e-07     |        1        \n",
      "       27     | 1.25e-04  |  5.00e-06   |  1.52e-03  |    3.21e-07     |        1        \n",
      "       28     | 1.21e-04  |  4.71e-06   |  7.13e-04  |    4.42e-07     |        1        \n",
      "       29     | 1.16e-04  |  4.63e-06   |  1.40e-03  |    2.75e-07     |        1        \n",
      "       30     | 1.12e-04  |  4.36e-06   |  6.61e-04  |    3.79e-07     |        1        \n",
      "       31     | 1.07e-04  |  4.29e-06   |  1.30e-03  |    2.36e-07     |        1        \n",
      "       32     | 1.03e-04  |  4.04e-06   |  6.12e-04  |    3.25e-07     |        1        \n",
      "       33     | 9.94e-05  |  3.98e-06   |  1.20e-03  |    2.03e-07     |        1        \n",
      "       34     | 9.57e-05  |  3.74e-06   |  5.66e-04  |    2.78e-07     |        1        \n",
      "       35     | 9.20e-05  |  3.68e-06   |  1.11e-03  |    1.74e-07     |        1        \n",
      "       36     | 8.86e-05  |  3.46e-06   |  5.24e-04  |    2.38e-07     |        1        \n",
      "       37     | 8.52e-05  |  3.41e-06   |  1.03e-03  |    1.49e-07     |        1        \n",
      "       38     | 8.19e-05  |  3.21e-06   |  4.85e-04  |    2.04e-07     |        1        \n",
      "       39     | 7.88e-05  |  3.16e-06   |  9.53e-04  |    1.28e-07     |        1        \n",
      "       40     | 7.58e-05  |  2.97e-06   |  4.49e-04  |    1.75e-07     |        1        \n",
      "       41     | 7.29e-05  |  2.92e-06   |  8.82e-04  |    1.09e-07     |        1        \n",
      "       42     | 7.02e-05  |  2.75e-06   |  4.16e-04  |    1.50e-07     |        1        \n",
      "       43     | 6.74e-05  |  2.70e-06   |  8.16e-04  |    9.36e-08     |        1        \n",
      "\n",
      "===============================================================================================\n",
      "Computing refinement of solution\n",
      "    Initial guess relative error = 6.744881e-05\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing CPD\n",
      "    Iteration | Rel error | Improvement | norm(grad) | Predicted error | # Inner iterations\n",
      "        1     | 6.67e-05  |  6.67e-05   |  3.85e-04  |    5.77e-10     |        1        \n",
      "        2     | 6.63e-05  |  4.38e-07   |  2.81e-04  |    6.91e-11     |        1        \n",
      "        3     | 6.55e-05  |  7.80e-07   |  2.47e-04  |    2.06e-10     |        1        \n",
      "===============================================================================================\n",
      "Final results\n",
      "    Number of steps = 46\n",
      "    Relative error = 6.548682198482073e-05\n",
      "    Accuracy =  99.99345 %\n"
     ]
    }
   ],
   "source": [
    "# Compute the CPD of T with refinement.\n",
    "options.display = 3\n",
    "options.init_method = 'random'\n",
    "options.refine = True\n",
    "factors, T_approx, output = tfx.cpd(T, r, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Damping parameter\n",
    "\n",
    "In the previous section we mentioned that, at each iteration, the program solves a minimization problem. This minimization is of the form\n",
    "\n",
    "$$\\min_x \\| Jx - b\\|,$$\n",
    "where $J$ is a tall matrix (the Jacobian matrix of the function of the residuals) and $x, b$ are vectors. There are two problems here: $J$ has too many rows and is not of full rank. In fact, the number of rows of the matrix brings the curse of dimensionality to the problem. One way to avoid this is to consider solving the normal equations\n",
    "\n",
    "$$J^T J x = J^Tb.$$\n",
    "\n",
    "Now the matrix has a reasonable size. To solve the problem of lack of full rank we introduce regularization, thus obtaining the new set of equations\n",
    "\n",
    "$$(J^T J + \\mu D) x = J^Tb$$\n",
    "where $\\mu > 0$ is the damping parameter and $D$ is a diagonal matrix. At each iteration the damping parameter is updated following a certain rule, and the user doesn't have influence over this. On the other hand, the user can choose the initial damping parameter factor. More precisely, the first damping parameter is $\\mu = \\tau \\cdot E[T]$, where $\\tau$ is the damping parameter factor and $E[T]$ is the mean of the values of $T$ (if there is compression, use $S$ instead of $T$). The default value we use is $\\tau = 1$, but the user can change it with the parameter *init_damp*. Experience shows that this value has little influence on the overall process, but sometimes it has a noticeable influence, so be aware of that.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symmetric Tensors\n",
    "\n",
    "If one want to work with symmetric tensors, just set *symm* to True. With this option activated the initialization and all iterations of the dGN function will be done with symmetric tensors. At each iteration the approximated CPD is given by a triplet $X, Y, Z$. The next step is to make the assignements\n",
    "\n",
    "$$X \\leftarrow \\frac{X+Y+Z}{3}, \\quad Y \\leftarrow X, \\quad Z \\leftarrow X.$$\n",
    "\n",
    "If the objective tensor is really symmetric, then this procedure converges. Otherwise it can diverge or become unstable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
