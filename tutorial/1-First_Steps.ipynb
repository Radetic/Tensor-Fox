{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Things First\n",
    "\n",
    "Welcome to our very first introduction to *Tensor Fox*, a specialized library made to deal with tensors of any order (the number of indexes), with focus on the CPD (Canonical Polyadic Decomposition). In order to have everything working properly, all files of Tensor Fox must be in the same folder of your program. Another option is to add all modules to your Python path, if more convenient. To be able to use Tensor Fox you will need the following modules:\n",
    "\n",
    "- numpy\n",
    "\n",
    "- scipy\n",
    "\n",
    "- time\n",
    "\n",
    "- matplotlib\n",
    "\n",
    "- numba\n",
    "\n",
    "- decimal\n",
    "\n",
    "Also make sure Numpy is using a nice version of BLAS. That is all! Tensor Fox is read to go! Let's start importing Tensor Fox and other necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import TensorFox as tfx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Tensors and Getting Information \n",
    "\n",
    "Let's create a little tensor $T$ just to see how Tensor Fox works at its basics. For third order tensors (3D arrays) I use the convention that $T_{ijk}$ refers to the $i$-th row, $j$-column and $k$-section (frontal slice) of $T$. For instance, consider the tensor defined above (the frontal slices of $T$ are showed)\n",
    "\n",
    "$$T = \\left\\{ \\left[\n",
    "\\begin{array}{cc}\n",
    "    0 & 1\\\\\n",
    "    2 & 3\n",
    "\\end{array}\n",
    "\\right], \\quad\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "    4 & 5\\\\\n",
    "    6 & 7\n",
    "\\end{array}\n",
    "\\right] \\right\\}.$$\n",
    "\n",
    "Since Numpy's convention is different from ours with regard to third order tensors. This convention may be irrelevant when using the routines of Tensor Fox, but since I build all the modules thinking this way, it is fair that this point is made explicitly. The function **showtens** prints a third order tensor with this particular convention and print tensors of higher order just as Numpy would print. Below we show both conventions with an example of third order tensor. This particular tensor will be our toy model through all lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our view of T:\n",
      "--------------\n",
      "[[0. 1.]\n",
      " [2. 3.]]\n",
      "\n",
      "[[4. 5.]\n",
      " [6. 7.]]\n",
      "\n",
      "\n",
      "Numpy view of T:\n",
      "----------------\n",
      "[[[0. 4.]\n",
      "  [1. 5.]]\n",
      "\n",
      " [[2. 6.]\n",
      "  [3. 7.]]]\n"
     ]
    }
   ],
   "source": [
    "# Create and print the tensor, which is 2 x 2 x 2.\n",
    "m = 2\n",
    "T = np.zeros((m, m, m))\n",
    "s = 0\n",
    "\n",
    "for k in range(m):\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            T[i,j,k] = s\n",
    "            s += 1\n",
    "\n",
    "print('Our view of T:')\n",
    "print('--------------')          \n",
    "tfx.disp.showtens(T)\n",
    "print()\n",
    "print('Numpy view of T:')\n",
    "print('----------------')\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of questions we can make about a given tensor $T$. What are its maximum and minimum entries, what are the dimensions of $T$, the rank and multirank, etc. Even in the case of the simple tensor above we can't know all these answers in advance. The function **infotens** shows lots of information about $T$ for your convenience. This function is useful for small tensors, but remember that finding the rank is a NP-hard task, so don't abuse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T is a tensor of dimensions (2, 2, 2)\n",
      "\n",
      "|T| = 11.832159566199232\n",
      "\n",
      "max(T) = 7.0\n",
      "\n",
      "min(T) = 0.0\n",
      "\n",
      "E[T] = 3.5\n",
      "\n",
      "E[|T|] = 3.5\n",
      "\n",
      "1 <= rank(T) <= 4\n",
      "\n",
      "Generic rank of the tensor space of T = 2\n",
      "\n",
      "Computing multilinear rank...\n",
      "------------------------------------\n",
      "Estimated multirank(T) = 2 , 2 , 2\n",
      "|T - (U1, U2, U3)*S|/|T| = 0.0\n",
      "\n",
      "Computing rank...\n",
      "Start searching for rank\n",
      "Stops at r = 4  or less\n",
      "-----------------------------\n",
      "Testing r = 3\n",
      "Estimated rank(T) = 3\n",
      "|T - T_approx|/|T| = 1.439523236170629e-07\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's see some information about T.\n",
    "tfx.disp.infotens(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the CPD\n",
    "\n",
    "Now let's turn to the most important tool of Tensor Fox, the computation of the CPD. As the previous function hinted, $T$ should have rank 3. We can compute the corresponding CPD with the function **cpd**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =\n",
      " [[-0.76402529  1.45265512  0.39536087]\n",
      " [-1.14945451  1.11280149  1.77842897]]\n",
      "\n",
      "Y =\n",
      " [[ 0.52225638  1.34541518  1.47254883]\n",
      " [-1.27758702  1.24031932  1.07271607]]\n",
      "\n",
      "Z =\n",
      " [[ 0.83063184 -0.13815892  1.03309272]\n",
      " [ 1.10228448  1.824677    1.50061296]]\n",
      "\n",
      "T_approx =\n",
      "[[-3.04483394e-06  1.00000328e+00]\n",
      " [ 2.00000219e+00  2.99999724e+00]]\n",
      "\n",
      "[[4.00000179 4.99999761]\n",
      " [5.9999981  7.00000166]]\n",
      "\n",
      "|T - T_approx|/|T| = 5.83244866700309e-07\n"
     ]
    }
   ],
   "source": [
    "# Compute the CPD of T, assuming T has rank 2.\n",
    "r = 3\n",
    "factors, T_approx, output = tfx.cpd(T, r)\n",
    "\n",
    "# 'factors' is the list of the factor matrices associated with the CPD.\n",
    "X = factors[0]\n",
    "Y = factors[1]\n",
    "Z = factors[2]\n",
    "\n",
    "# Show the CPD computed. \n",
    "print('X =\\n', X)\n",
    "print()\n",
    "print('Y =\\n', Y)\n",
    "print()\n",
    "print('Z =\\n', Z)\n",
    "print()\n",
    "\n",
    "# Show the coordinate representation of this CPD.\n",
    "print('T_approx =')\n",
    "tfx.disp.showtens(T_approx)\n",
    "\n",
    "# Show relative error of this approximation.\n",
    "print('|T - T_approx|/|T| =', output.rel_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it is convenient to use normalized factors, we can convert everything to the format $\\Lambda, X, Y, Z$, where the columns of $X, Y, Z$ are unit norm and their magnitudes are passed to the central tensor $\\Lambda \\in \\mathbb{R}^{r \\times r \\times r}$. Since $\\Lambda$ is a diagonal tensor, we can store its values as a unidimensional array of size $r$. This is made such that\n",
    "$$T \\approx \\sum_{\\ell=1}^r \\Lambda_{\\ell} \\ X_{:, \\ell} \\otimes Y_{:, \\ell} \\otimes Z_{:, \\ell}.$$\n",
    "\n",
    "We denote this relation as $T \\approx (X, Y, Z) \\cdot \\Lambda$, the multilinear multiplication. With a simple command we can obtain this factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda =\n",
      " [2.62927337 6.12748247 6.04692245]\n",
      "\n",
      "X =\n",
      " [[-0.55355718  0.79384399  0.21701124]\n",
      " [-0.83281117  0.60812147  0.97616911]]\n",
      "\n",
      "Y =\n",
      " [[ 0.378389    0.73523973  0.80827331]\n",
      " [-0.92564667  0.67780716  0.58880748]]\n",
      "\n",
      "Z =\n",
      " [[ 0.60181544 -0.0755008   0.56705846]\n",
      " [ 0.7986352   0.99714574  0.82367755]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Lambda, factors = tfx.aux.normalize(factors, r)\n",
    "X = factors[0]\n",
    "Y = factors[1]\n",
    "Z = factors[2]\n",
    "\n",
    "print('Lambda =\\n', Lambda)\n",
    "print()\n",
    "print('X =\\n', X)\n",
    "print()\n",
    "print('Y =\\n', Y)\n",
    "print()\n",
    "print('Z =\\n', Z)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you already noted that there are some submodules to TensorFox. In particular, *aux* refers to the *Auxiliar* module, which is responsible for minor subroutines or just some pieces of an important algorithm. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
