{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic options\n",
    "\n",
    "The *cpd* function has several options at disposal. Some of them may improve performance, precision or give insights about the tensor at hand. If you look at the source code, the first line of *cpd* is the following:\n",
    "\n",
    "    def cpd(T, r, options=False):\n",
    "\n",
    "The first action of the **cpd** function is to read *options*. When set to False, this function set the parameters to their default values. In order to change some of them the user needs to create the class *options* and add the parameters of interest with their corresponding values. The default class with all the default parameters is showed below:\n",
    "\n",
    "    class options:\n",
    "        maxiter = 200  \n",
    "        tol = 1e-12\n",
    "        method = 'cg'\n",
    "        method_maxiter = 1\n",
    "        method_tol = 1e-6\n",
    "        init_method = 'random'\n",
    "        trunc_dims = 0\n",
    "        level = 1\n",
    "        init_damp = 1\n",
    "        refine = False\n",
    "        symm = False\n",
    "        constraints = [0, 0, 0]\n",
    "        trials = 10\n",
    "        display = 0\n",
    "\n",
    "There are a lot of options, but don't worry, I will explain them one by one now. If you don't want to bother learning the details, be assured that all default values were obtained by an long and exhausting marathon of tests, with lots of different tensors. Of course we can't say these values will apply to all possible tensor, but you can just wait to verify a failure before digging deeper in the details. \n",
    "\n",
    "For the others with more patience, let's see what each options does, not in the order above but in a more didatic order. Let's start importing the necessary modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import TensorFox as tfx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trials\n",
    "\n",
    "Tensor Fox has distinct approaches when it comes to computing the CPD of third order tensors and higher order tensors. For third order tensors the program relies on the *Damped Gauss-Newton* (dGN) method. For higher order tensors the program relies on the *Tensor Train format* (TT format), also called *Tensor Train decomposition*. Without going in too much details, we use a specific configuration of the TT format which can be obtained by computing several third order CPD's. More precisely, if $T$ is a tensor of order $L$, then we can compute a CPD for it by computing $L-2$ third order CPD's. Once we have the TT format of $T$, the CPD can also be computed.\n",
    "\n",
    "In the case $T$ has order higher than $3$, the parameter *trials* defines how much times we compute each one of these third order CPD's. The idea is to compute several times and keep the best result (smaller error). This may be helpful because all $L-2$ CPD's needs to be of good quality in order to get a good CPD for $T$. If anyone of the third order CPD's has bad precision, than everything falls apart. Currently the default is *trials*$= 10$, but this may change depending on the problem. This parameter doesn't makes difference if $T$ is a third order tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display\n",
    "\n",
    "There are four choices for the *display* option: $-1,0,1,2,3,4$. These options controls what the user can see during the computations (works as the *verbose* parameter, but I prefer the name *display*). In the previous lesson we let the defaults and there were no output whatsoever (because the display default is $0$).\n",
    "\n",
    " - *display* $=0$ (default): show nothing on the screen.\n",
    "\n",
    " - *display* $=1$: shows useful information about the principal stages of the computation \n",
    "    \n",
    " - *display* $=2$: shows everything the option *display* $=1$ shows plus information about each iteration\n",
    "    \n",
    " - *display* $=3$ is special, it shows eveything the option *display* $=2$ shows and also shows the relative error of the compressed tensor (the computation of this error is costly so avoid that for big tensors)\n",
    "    \n",
    " - *display* $=4$ is almost equal to *display* $=3$ but now there are more digits displayed on the screen (*display* $=3$ is a \"clean\" version of *display* $=4$, but with less informtation) \n",
    "    \n",
    " - *display* $=-1$ is a special option for it is reserved for tensors of order higher than $3$. \n",
    "    \n",
    "To see how this parameter affects the visualization, let's start creating our toy model tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [2. 3.]]\n",
      "\n",
      "[[4. 5.]\n",
      " [6. 7.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and print the tensor.\n",
    "m = 2\n",
    "T = np.zeros((m, m, m))\n",
    "s = 0\n",
    "\n",
    "for k in range(m):\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            T[i,j,k] = s\n",
    "            s += 1\n",
    "                    \n",
    "tfx.disp.showtens(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------\n",
      "Computing MLSVD\n",
      "    No compression detected\n",
      "    Working with dimensions (2, 2, 2)\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Type of initialization: random\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing CPD\n",
      "===============================================================================================\n",
      "Final results\n",
      "    Number of steps = 32\n",
      "    Relative error = 1.5322304953907926e-07\n",
      "    Accuracy =  99.99998 %\n"
     ]
    }
   ],
   "source": [
    "# Create class of options with display=1.\n",
    "class options:\n",
    "    display = 1\n",
    "\n",
    "# Compute the CPD of T with partial display.\n",
    "r = 3\n",
    "factors, T_approx, output = tfx.cpd(T, r, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------\n",
      "Computing MLSVD\n",
      "    No compression detected\n",
      "    Working with dimensions (2, 2, 2)\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Type of initialization: random\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing CPD\n",
      "    Iteration | Rel error | Improvement | norm(grad) | Predicted error | # Inner iterations\n",
      "        1     | 9.44e-01  |  9.44e-01   |  1.01e+01  |    2.10e-02     |        2        \n",
      "        2     | 4.01e-01  |  5.43e-01   |  6.75e+00  |    7.92e-03     |        3        \n",
      "        3     | 1.42e-01  |  2.58e-01   |  1.60e+01  |    3.39e-03     |        3        \n",
      "        4     | 8.03e-02  |  6.19e-02   |  4.40e+00  |    2.88e-06     |        4        \n",
      "        5     | 5.96e-02  |  2.08e-02   |  1.97e+00  |    3.02e-07     |        5        \n",
      "        6     | 4.45e-02  |  1.51e-02   |  9.71e-01  |    9.72e-06     |        6        \n",
      "        7     | 3.24e-02  |  1.20e-02   |  3.79e-01  |    3.55e-08     |        7        \n",
      "        8     | 2.51e-02  |  7.39e-03   |  3.81e-01  |    2.69e-04     |        7        \n",
      "        9     | 2.15e-02  |  3.58e-03   |  2.50e-01  |    3.36e-03     |        4        \n",
      "       10     | 1.50e-02  |  6.50e-03   |  2.70e-01  |    2.52e-04     |        9        \n",
      "       11     | 1.08e-02  |  4.13e-03   |  2.47e-01  |    6.70e-03     |        4        \n",
      "       12     | 7.78e-03  |  3.06e-03   |  8.50e-02  |    3.44e-04     |        6        \n",
      "       13     | 4.76e-03  |  3.02e-03   |  1.24e-01  |    2.69e-05     |        7        \n",
      "       14     | 2.76e-03  |  2.00e-03   |  1.11e-01  |    1.29e-06     |        10       \n",
      "       15     | 1.55e-03  |  1.21e-03   |  6.13e-02  |    3.55e-05     |        8        \n",
      "       16     | 9.17e-04  |  6.29e-04   |  3.67e-02  |    1.84e-05     |        7        \n",
      "       17     | 5.63e-04  |  3.54e-04   |  1.93e-02  |    4.78e-06     |        6        \n",
      "       18     | 3.37e-04  |  2.26e-04   |  1.19e-02  |    3.85e-08     |        13       \n",
      "       19     | 1.93e-04  |  1.44e-04   |  7.36e-03  |    1.05e-08     |        12       \n",
      "       20     | 1.13e-04  |  8.08e-05   |  4.08e-03  |    2.85e-07     |        8        \n",
      "       21     | 6.91e-05  |  4.35e-05   |  2.47e-03  |    7.71e-08     |        6        \n",
      "       22     | 4.49e-05  |  2.43e-05   |  1.22e-03  |    1.39e-09     |        13       \n",
      "       23     | 2.63e-05  |  1.86e-05   |  9.54e-04  |    3.52e-11     |        18       \n",
      "       24     | 1.18e-05  |  1.45e-05   |  6.85e-04  |    2.80e-10     |        10       \n",
      "       25     | 5.44e-06  |  6.38e-06   |  2.23e-04  |    4.23e-10     |        7        \n",
      "       26     | 3.97e-06  |  1.48e-06   |  5.34e-05  |    2.17e-10     |        9        \n",
      "       27     | 2.62e-06  |  1.35e-06   |  7.58e-05  |    3.28e-10     |        7        \n",
      "       28     | 1.68e-06  |  9.43e-07   |  6.39e-05  |    4.14e-12     |        9        \n",
      "       29     | 5.73e-07  |  1.11e-06   |  4.74e-05  |    3.13e-12     |        4        \n",
      "       30     | 4.71e-07  |  1.02e-07   |  6.17e-06  |    3.75e-12     |        2        \n",
      "       31     | 4.31e-07  |  4.02e-08   |  3.01e-06  |    4.07e-12     |        1        \n",
      "       32     | 4.22e-07  |  9.01e-09   |  3.30e-06  |    2.95e-12     |        1        \n",
      "       33     | 4.14e-07  |  7.89e-09   |  3.13e-06  |    2.09e-12     |        1        \n",
      "       34     | 4.08e-07  |  5.94e-09   |  2.24e-06  |    2.74e-12     |        1        \n",
      "===============================================================================================\n",
      "Final results\n",
      "    Number of steps = 34\n",
      "    Relative error = 4.0830277503516243e-07\n",
      "    Accuracy =  99.99996 %\n"
     ]
    }
   ],
   "source": [
    "# Compute the CPD of T with full display.\n",
    "options.display = 2\n",
    "factors, T_approx, output = tfx.cpd(T, r, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------\n",
      "Computing MLSVD\n",
      "    No compression detected\n",
      "    Working with dimensions (2, 2, 2)\n",
      "    Compression relative error = 0.000000e+00\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Type of initialization: random\n",
      "    Initial guess relative error = 1.207675e+00\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing CPD\n",
      "    Iteration | Rel error | Improvement | norm(grad) | Predicted error | # Inner iterations\n",
      "        1     | 9.83e-01  |  9.83e-01   |  1.99e+01  |    1.95e-01     |        2        \n",
      "        2     | 3.41e-01  |  6.41e-01   |  5.27e+00  |    6.32e-02     |        3        \n",
      "        3     | 1.83e-01  |  1.58e-01   |  1.07e+01  |    1.36e-03     |        3        \n",
      "        4     | 1.33e-01  |  5.01e-02   |  5.88e+00  |    4.27e-05     |        3        \n",
      "        5     | 9.50e-02  |  3.80e-02   |  3.76e+00  |    7.65e-05     |        4        \n",
      "        6     | 7.34e-02  |  2.15e-02   |  2.16e+00  |    7.74e-04     |        4        \n",
      "        7     | 6.05e-02  |  1.29e-02   |  1.12e+00  |    5.16e-03     |        4        \n",
      "        8     | 5.31e-02  |  7.41e-03   |  6.42e-01  |    1.05e-02     |        4        \n",
      "        9     | 4.93e-02  |  3.83e-03   |  4.59e-01  |    3.31e-02     |        6        \n",
      "       10     | 4.36e-02  |  5.69e-03   |  4.83e-01  |    4.50e-03     |        6        \n",
      "       11     | 3.92e-02  |  4.40e-03   |  4.36e-01  |    1.41e-03     |        9        \n",
      "       12     | 3.40e-02  |  5.16e-03   |  4.41e-01  |    2.87e-03     |        8        \n",
      "       13     | 3.12e-02  |  2.83e-03   |  5.21e-01  |    1.09e-02     |        7        \n",
      "       14     | 3.00e-02  |  1.21e-03   |  2.62e-01  |    6.25e-03     |        5        \n",
      "       15     | 2.86e-02  |  1.34e-03   |  1.38e-01  |    6.46e-03     |        9        \n",
      "       16     | 2.94e-02  |  7.97e-04   |  1.11e+00  |    5.92e-03     |        11       \n",
      "       17     | 1.08e-02  |  1.87e-02   |  1.52e+00  |    4.54e-04     |        11       \n",
      "       18     | 8.11e-03  |  2.65e-03   |  1.71e-01  |    1.07e-03     |        13       \n",
      "       19     | 5.58e-03  |  2.53e-03   |  2.05e-01  |    8.08e-04     |        9        \n",
      "       20     | 4.40e-03  |  1.17e-03   |  9.51e-02  |    4.32e-04     |        11       \n",
      "       21     | 3.83e-03  |  5.71e-04   |  8.78e-02  |    6.14e-04     |        16       \n",
      "       22     | 2.07e-03  |  1.76e-03   |  4.83e-02  |    3.71e-05     |        11       \n",
      "       23     | 1.48e-03  |  5.86e-04   |  2.01e-02  |    1.02e-04     |        14       \n",
      "       24     | 8.93e-04  |  5.91e-04   |  1.47e-02  |    3.08e-06     |        12       \n",
      "       25     | 7.82e-04  |  1.11e-04   |  7.52e-03  |    5.39e-06     |        11       \n",
      "       26     | 5.89e-04  |  1.93e-04   |  1.36e-02  |    4.27e-06     |        5        \n",
      "       27     | 4.61e-04  |  1.29e-04   |  6.55e-03  |    2.13e-06     |        9        \n",
      "       28     | 3.84e-04  |  7.69e-05   |  5.87e-03  |    1.19e-06     |        13       \n",
      "       29     | 2.76e-04  |  1.07e-04   |  6.35e-03  |    5.06e-07     |        12       \n",
      "       30     | 2.59e-04  |  1.72e-05   |  3.16e-03  |    3.57e-06     |        15       \n",
      "       31     | 1.24e-04  |  1.35e-04   |  8.67e-03  |    1.56e-07     |        10       \n",
      "       32     | 1.02e-04  |  2.16e-05   |  2.18e-03  |    4.52e-07     |        8        \n",
      "       33     | 7.14e-05  |  3.07e-05   |  2.82e-03  |    8.35e-08     |        15       \n",
      "       34     | 1.96e-05  |  5.18e-05   |  1.81e-03  |    5.36e-09     |        14       \n",
      "       35     | 1.67e-05  |  2.89e-06   |  3.57e-04  |    3.03e-10     |        16       \n",
      "       36     | 4.21e-06  |  1.25e-05   |  3.90e-04  |    6.88e-12     |        14       \n",
      "       37     | 1.13e-06  |  3.08e-06   |  5.84e-05  |    5.31e-12     |        6        \n",
      "       38     | 5.79e-07  |  5.54e-07   |  1.32e-05  |    4.37e-12     |        3        \n",
      "       39     | 4.78e-07  |  1.01e-07   |  6.25e-06  |    2.61e-12     |        2        \n",
      "       40     | 4.52e-07  |  2.58e-08   |  2.15e-06  |    5.35e-12     |        1        \n",
      "===============================================================================================\n",
      "Final results\n",
      "    Number of steps = 40\n",
      "    Relative error = 4.522279791589869e-07\n",
      "    Accuracy =  99.99995 %\n"
     ]
    }
   ],
   "source": [
    "# Compute the CPD of T with full display plus relative error of compression.\n",
    "options.display = 3\n",
    "factors, T_approx, output = tfx.cpd(T, r, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between display $2$ and $3$ is only the initial guess relative error, which is given when *display* $=3$. This is the error $\\|T - T_{approx}^{(0)}\\| / \\|T\\|$, where $T_{approx}^{(0)}$ is the starting point of the iterations. Sometimes it can be useful to know if the starting point is too far away or not from the objective tensor. Since the computation of this error is very costly, I've made this as an extra option. Below there are the description of each column.\n",
    "\n",
    " - Iteration: it just the numbering of the iterations.\n",
    " \n",
    " - Rel error: the relative error between the current approximation and the objective tensor.\n",
    " \n",
    " - Improvement: the difference (in absolute value) between two consecutive errors, i.e., it is the value\n",
    " \n",
    " $$\\left| \\frac{\\| T - T_{approx}^{(k)} \\|}{\\| T \\|} - \\frac{\\| T - T_{approx}^{(k-1)} \\|}{\\| T \\|} \\right|,$$\n",
    "where $k$ is the numbering of the current iteration. \n",
    "\n",
    " - norm(grad): the original problem can be regarded as a nonlinear least squares problem, and a minimizer is also a critical point, so it is of interest to keep track of the (infinite) norm of the gradient to check if it is approaching zero.\n",
    " \n",
    " - Predicted error: each iteration tries to minimize a linear model of the original problem. After we compute such a minimizer we have a error of this model which is expected to be close to the original one. This is important for updating the damping parameter.\n",
    " \n",
    " - Inner iterations: the linear model mentioned above can be solved by the [conjugate gradient](https://en.wikipedia.org/wiki/Conjugate_gradient_method) method (default) os the [LSMR](http://web.stanford.edu/group/SOL/software/lsmr/) (least squares with minimal residuals) method. Both method are iterative and it can be useful to keep track of the number of iterations they are using. \n",
    " \n",
    " It should be noted that the first informations are about the MLSVD and initialization method, we will talk about it soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've said, the options *trials* says about the repetition of third order CPD computations. If *display* is set to $1, 2, 3$ or $4$, then all the information of each one of these CPD's are printed on the screen. This means we wil have $(L-2) \\cdot trials$ CPD's informations printed on the screen when $T$ has order $L$ . Sometimes this amount of information is just too much. We can make everything more succint in these situations just by setting *display* $=-1$. Consider the following fourth order tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A = \n",
      "[[[[-7.02988856e-01  2.59239766e-03 -2.27199661e-01]\n",
      "   [-4.22709058e-02  1.47002543e-04  4.28931269e-03]\n",
      "   [ 1.79021392e-01 -1.93671383e-02  5.72776702e-02]]\n",
      "\n",
      "  [[ 1.83639213e-01 -4.92174624e-02  5.14500084e-02]\n",
      "   [ 7.77842070e-03  2.38025671e-02  9.43094904e-03]\n",
      "   [-3.96342390e-02 -1.80695503e-01 -4.12918384e-02]]\n",
      "\n",
      "  [[ 5.36203868e-01  4.83261697e-03  1.73760249e-01]\n",
      "   [ 2.20373741e-02 -8.67890506e-03  2.81922656e-02]\n",
      "   [-1.38967252e-01  3.97444229e-02 -3.54555464e-02]]]\n",
      "\n",
      "\n",
      " [[[ 1.42958825e-01 -2.37462432e-02  4.15311521e-02]\n",
      "   [-7.73526514e-03  4.14066780e-03  4.98103257e-02]\n",
      "   [-3.49594408e-02 -8.64242036e-02 -1.79548716e-02]]\n",
      "\n",
      "  [[-2.77746053e-02 -2.35737940e-01 -4.68497284e-02]\n",
      "   [-9.04446306e-03  1.19995663e-01  2.52235531e-02]\n",
      "   [ 4.28480247e-02 -9.02417271e-01 -1.29273898e-01]]\n",
      "\n",
      "  [[-1.09797997e-01  5.18541121e-02 -2.83256874e-02]\n",
      "   [-2.32292025e-02 -3.50841017e-02  5.15473163e-02]\n",
      "   [ 1.77305028e-02  1.91811615e-01  4.54040796e-02]]]\n",
      "\n",
      "\n",
      " [[[-1.12713125e-02 -3.41908760e-03  7.07587314e-03]\n",
      "   [ 1.85762610e-01  9.31250907e-02 -5.75942426e-01]\n",
      "   [ 2.82124783e-02  5.49986976e-03 -8.04248621e-02]]\n",
      "\n",
      "  [[ 2.29310752e-03 -2.03400799e-02 -5.60019332e-04]\n",
      "   [ 3.19971082e-02  2.62581436e-02 -9.82537339e-02]\n",
      "   [ 6.82309771e-03 -7.43508328e-02 -2.53595291e-02]]\n",
      "\n",
      "  [[ 1.99611004e-03  2.23650299e-03  1.51113510e-02]\n",
      "   [ 2.33589130e-01  1.13164937e-01 -7.21382923e-01]\n",
      "   [ 3.02011697e-02  3.24565891e-02 -9.82682806e-02]]]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize dimensions of the tensor.\n",
    "k = 3\n",
    "dims = (k, k, k, k)\n",
    "L = len(dims)\n",
    "\n",
    "# Create four random factors matrices so thar \n",
    "# A = (orig_factors[0], orig_factors[1], orig_factors[2], orig_factors[3])*I.\n",
    "orig_factors = []\n",
    "for l in range(L):\n",
    "    M = np.random.randn(k, r)\n",
    "    Q, R = np.linalg.qr(M)\n",
    "    orig_factors.append(Q)\n",
    "    \n",
    "# From the factor matrices generate the respective tensor in coordinates.\n",
    "A = np.zeros(dims)\n",
    "A = tfx.cnv.cpd2tens(A, orig_factors, dims)\n",
    "\n",
    "print('A = ')\n",
    "tfx.disp.showtens(A) # now this is the same as print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 2 third order CPDs to be computed:\n",
      "===============================================================================================\n",
      "1 ) CPD error = 8.216388230917005e-08\n",
      "2 ) CPD error = 1.8083612346754555e-07\n",
      "\n",
      "===============================================================================================\n",
      "===============================================================================================\n",
      "Final results\n",
      "    Number of steps = 45\n",
      "    Relative error = 1.8504773394049537e-07\n",
      "    Accuracy =  99.99998 %\n"
     ]
    }
   ],
   "source": [
    "# Compute the CPD of A with succint display for higher order tensors.\n",
    "options.display = -1\n",
    "factors, A_approx, output = tfx.cpd(A, r, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 2 third order CPDs to be computed:\n",
      "===============================================================================================\n",
      "\n",
      "CPD 1\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing MLSVD\n",
      "    No compression detected\n",
      "    Working with dimensions (3, 3, 3)\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Type of initialization: random\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing CPD\n",
      "===============================================================================================\n",
      "Final results\n",
      "    Number of steps = 16\n",
      "    Relative error = 2.863647294855388e-07\n",
      "    Accuracy =  99.99997 %\n",
      "\n",
      "CPD 2\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing MLSVD of T\n",
      "    No compression detected\n",
      "    Working with dimensions (3, 3, 3)\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Type of initialization: fixed + random\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing CPD of T\n",
      "===============================================================================================\n",
      "Final results of bicpd\n",
      "    Number of steps = 32\n",
      "    Relative error = 3.069915851416899e-07\n",
      "    Accuracy =  99.99997 %\n",
      "\n",
      "===============================================================================================\n",
      "===============================================================================================\n",
      "Final results\n",
      "    Number of steps = 48\n",
      "    Relative error = 2.8547512797727495e-07\n",
      "    Accuracy =  99.99997 %\n"
     ]
    }
   ],
   "source": [
    "# The user may be interested in see how each third order CPD is computed.\n",
    "options.display = 1\n",
    "factors, A_approx, output = tfx.cpd(A, r, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncation\n",
    "\n",
    "In applications of linear algebra, often one have to compute the truncated SVD of some matrix. By truncating we can reduce the dimensionality of the problem, which leads to lots of speed up for the upcoming algorithms. On the other hand, we lost information after the truncation. Ideally, we want to truncate as much as possible without losing the relevant information.\n",
    "\n",
    "The same can be made for tensors, but in this context we use the [multilinear singular value decomposition](https://epubs.siam.org/doi/abs/10.1137/s0895479896305696) (MLSVD). If $T$ is a $L$-order tensor, then its MLSVD is written as $T = (U_1, \\ldots, U_L) \\cdot S$, where each $U_l$ is a orthogonal matrix and $S$ is a tensor with the same shape as $T$ (we consider $S$ as the *compressed* version of $T$). For the interested people, the notation used stands for the [multilinear multiplication](https://en.wikipedia.org/wiki/Multilinear_multiplication). This tensor $S$ is called the *central tensor* and it is the analogous of $\\Sigma$ in the classical SVD of the form $A = U \\Sigma V^T$. Note that we've said that $S$ is of the same shape as $T$, and just as in the 2D linear algebra, this is the *full* MLSVD, in contrast to the *reduced* MLSVD. In the same way we can consider the reduced SVD $A = U \\Sigma V^T$ where $\\Sigma$ is $R \\times R$ (and $R$ is the rank of $A$), we can have $S$ of shape $R_1 \\times R_2 \\times \\ldots \\times R_L$. The tuple $(R_1, R_2, \\ldots, R_L)$ is the *multilinear rank* of $T$. \n",
    "\n",
    "At first all we have is the full MLSVD, by Tensor Fox works the truncation for you at your taste. More precisely, you can tell the program if you want to truncate \"a lot\", \"just a few\", not truncate at all, etc. You can even tell the program to not compute the MLSVD and work with the original tensor (I do not advise to do that!). You communicate the program your type of truncation in the means of *levels*, where low levels translates to truncate a large amount and large levels translate to truncate small ammounts. Below we summarize the possible options you have (at the moment these only applies to third order tensors).\n",
    "\n",
    " - *level* $=0$: the conditions to truncate are very loose, so it is \"easy\" to truncate. This level causes very large truncations, which speeds uo the program at a cost of losing too much information of the original tensor.\n",
    " \n",
    " - *level* $=1$ (default): the conditions to truncation are median and reasonable. \n",
    " \n",
    " - *level* $=2$: the conditions to truncate are more tight but still manageable. Some tensors require this kind of truncation where you just cut off a little piece of the central tensor.\n",
    " \n",
    " - *level* $=3$: the conditions to truncate are extremely tight. This is almost as saying you don't want to truncate anything.\n",
    " \n",
    " - *level* $=4$: you want to work with the tensor $S$ of the full MLSVD, which has the same shape of $T$.\n",
    " \n",
    " - *level* $=5$: you want to work with the original tensor $T$. In this case the MLSVD is not computed.\n",
    " \n",
    "Finally,  we want to mention that you can also use the parameter *trunc_dims* to tell the program the exactly truncation you want to use. Just set this parameter to be a list of the dimensions you want (default is *trunca_dims = 0*, which lets the program to decide the best truncation). In the example we are working it is possible to note that the program is unable to truncate. Still, we can force some truncation, say $2 \\times 1 \\times 1$, and see if we get good precision of this. The precision can already be antecipated just by seeing the relative error of the compression (remember that this requires setting *display* to $3$, which can be costly), that is, the line\n",
    "\n",
    "    Compression relative error = 0.130828\n",
    "\n",
    "That line says this is the best precision we can get. This is because this is the error of the truncated $S$ and $T$, and all the iterations to be made will try to obtain a CPD for $S$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------\n",
      "Computing MLSVD\n",
      "    Compression detected\n",
      "    Compressing from (2, 2, 2) to (2, 1, 1)\n",
      "    99.31 % of the energy was retained\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Type of initialization: random\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing CPD\n",
      "===============================================================================================\n",
      "Final results\n",
      "    Number of steps = 31\n",
      "    Relative error = 0.13082808697999668\n",
      "    Accuracy =  86.91719 %\n"
     ]
    }
   ],
   "source": [
    "options.display = 1\n",
    "options.trunc_dims = [2,1,1]\n",
    "factors, T_approx, output = tfx.cpd(T, r, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "The iteration process needs a starting point for iterating. This starting point depends on the *init_method* option, and there are three possible choices in this case: *smart*, *smart_random*, *random* (default), and *user*. Both *smart* and *smart_random* options generates a CPD of rank $r$ with a strategy relying on the MLSVD. The strategy *smart* maximizes the energy of the initialization wheareas *smart random* makes almost the same, but with a chance to take some different entries. These strategies generates starting points with small relative error, so it is already close to the objective tensor. Although this seems to be a good thing, there is also a risk to be close to a local minimum or saddle point, and in this cases these methods will always fail. The *random* is more robust, this option generates a CPD of rank $r$ with entries drawn from the normal distribution. The relative error in this case usually is close to $1$. Finally, there is the 'user' option where the user provides a list $[X, Y, Z]$ as starting point. This is a good idea when we already have a close CPD and want to increase its precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------\n",
      "Computing MLSVD\n",
      "    No compression detected\n",
      "    Working with dimensions (2, 2, 2)\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Type of initialization: smart_random\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing CPD\n",
      "===============================================================================================\n",
      "Final results\n",
      "    Number of steps = 33\n",
      "    Relative error = 1.7925153865634859e-07\n",
      "    Accuracy =  99.99998 %\n"
     ]
    }
   ],
   "source": [
    "# Compute the CPD of T with random initialization.\n",
    "options.trunc_dims = 0\n",
    "options.init_method = 'smart_random'\n",
    "factors, T_approx, output = tfx.cpd(T, r, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------\n",
      "Computing MLSVD\n",
      "    No compression detected\n",
      "    Working with dimensions (2, 2, 2)\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Type of initialization: user\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing CPD\n",
      "===============================================================================================\n",
      "Final results\n",
      "    Number of steps = 11\n",
      "    Relative error = 0.13079807124885923\n",
      "    Accuracy =  86.92019 %\n"
     ]
    }
   ],
   "source": [
    "# Compute the CPD of T with user initialization.\n",
    "X = np.ones((m, r))\n",
    "Y = np.ones((m, r))\n",
    "Z = np.ones((m, r))\n",
    "options.init_method = [X,Y,Z]\n",
    "factors, T_approx, info = tfx.cpd(T, r, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refinement\n",
    "\n",
    "As we mentioned before, the user may give an initial CPD as starting point for our iterative algorithm, which may be a good idea when it is desired to increase the precision of the CPD. This process can be done automatically by setting *refine* to True. This option makes the program runs the algorithm two times, where the second run uses the approximated CPD computed in the first run as starting point. However, this second run is made in the original space (the space of the tensor $T$). Ideally, we want to compress and limit ourselves to the compressed version of $T$, but if this is not enough, the *refine* option can squeeze more precision at a cost of working with uncompressed tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------\n",
      "Computing MLSVD\n",
      "    No compression detected\n",
      "    Working with dimensions (2, 2, 2)\n",
      "    Compression relative error = 0.000000e+00\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Type of initialization: random\n",
      "    Initial guess relative error = 1.149328e+00\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing CPD\n",
      "    Iteration | Rel error | Improvement | norm(grad) | Predicted error | # Inner iterations\n",
      "        1     | 9.90e-01  |  9.90e-01   |  1.53e+01  |    2.27e-01     |        2        \n",
      "        2     | 7.68e-01  |  2.22e-01   |  3.10e+00  |    4.59e-03     |        3        \n",
      "        3     | 4.03e-01  |  3.65e-01   |  1.30e+01  |    5.00e-02     |        3        \n",
      "        4     | 1.33e-01  |  2.70e-01   |  2.56e+01  |    3.49e-03     |        3        \n",
      "        5     | 6.96e-02  |  6.36e-02   |  6.70e+00  |    1.33e-04     |        3        \n",
      "        6     | 5.59e-02  |  1.37e-02   |  2.37e+00  |    1.68e-07     |        4        \n",
      "        7     | 4.86e-02  |  7.28e-03   |  1.10e+00  |    9.88e-09     |        7        \n",
      "        8     | 4.20e-02  |  6.63e-03   |  4.07e-01  |    4.39e-05     |        5        \n",
      "        9     | 3.26e-02  |  9.38e-03   |  3.16e-01  |    1.05e-04     |        5        \n",
      "       10     | 2.20e-02  |  1.06e-02   |  2.70e-01  |    2.78e-03     |        4        \n",
      "       11     | 1.21e-02  |  9.92e-03   |  2.00e-01  |    4.67e-03     |        4        \n",
      "       12     | 6.06e-03  |  6.06e-03   |  1.17e-01  |    2.20e-06     |        10       \n",
      "       13     | 3.69e-03  |  2.37e-03   |  6.23e-02  |    6.54e-07     |        10       \n",
      "       14     | 2.37e-03  |  1.32e-03   |  3.59e-02  |    5.21e-06     |        5        \n",
      "       15     | 1.27e-03  |  1.10e-03   |  2.19e-02  |    6.88e-06     |        5        \n",
      "       16     | 7.30e-04  |  5.39e-04   |  1.44e-02  |    3.25e-07     |        5        \n",
      "       17     | 4.40e-04  |  2.89e-04   |  6.05e-03  |    6.48e-10     |        14       \n",
      "       18     | 2.62e-04  |  1.79e-04   |  3.84e-03  |    1.04e-09     |        12       \n",
      "       19     | 1.59e-04  |  1.02e-04   |  2.31e-03  |    1.29e-08     |        5        \n",
      "       20     | 9.62e-05  |  6.29e-05   |  1.41e-03  |    1.64e-09     |        7        \n",
      "       21     | 5.82e-05  |  3.80e-05   |  8.48e-04  |    5.60e-10     |        7        \n",
      "       22     | 3.51e-05  |  2.31e-05   |  4.94e-04  |    2.63e-12     |        14       \n",
      "       23     | 2.10e-05  |  1.41e-05   |  3.22e-04  |    5.28e-12     |        12       \n",
      "       24     | 1.26e-05  |  8.45e-06   |  1.96e-04  |    5.08e-12     |        10       \n",
      "       25     | 7.54e-06  |  5.05e-06   |  1.18e-04  |    6.63e-12     |        8        \n",
      "       26     | 4.57e-06  |  2.97e-06   |  6.88e-05  |    2.95e-12     |        6        \n",
      "       27     | 2.82e-06  |  1.75e-06   |  3.90e-05  |    4.69e-12     |        5        \n",
      "       28     | 1.73e-06  |  1.09e-06   |  2.55e-05  |    1.24e-12     |        5        \n",
      "       29     | 1.10e-06  |  6.30e-07   |  1.55e-05  |    3.81e-12     |        4        \n",
      "       30     | 5.70e-07  |  5.28e-07   |  1.25e-05  |    3.66e-12     |        4        \n",
      "       31     | 4.48e-07  |  1.22e-07   |  8.95e-06  |    6.47e-12     |        1        \n",
      "       32     | 3.46e-07  |  1.02e-07   |  5.55e-06  |    5.47e-12     |        2        \n",
      "       33     | 3.05e-07  |  4.11e-08   |  3.46e-06  |    6.78e-12     |        1        \n",
      "       34     | 2.72e-07  |  3.22e-08   |  5.01e-06  |    5.33e-12     |        1        \n",
      "       35     | 2.44e-07  |  2.80e-08   |  2.18e-06  |    4.45e-12     |        1        \n",
      "\n",
      "===============================================================================================\n",
      "Computing refinement of solution\n",
      "    Initial guess relative error = 2.443611e-07\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing CPD\n",
      "    Iteration | Rel error | Improvement | norm(grad) | Predicted error | # Inner iterations\n",
      "        1     | 2.32e-07  |  2.32e-07   |  3.38e-06  |    5.30e-14     |        1        \n",
      "        2     | 2.26e-07  |  6.35e-09   |  2.92e-06  |    7.40e-15     |        1        \n",
      "        3     | 2.15e-07  |  1.03e-08   |  2.55e-06  |    1.16e-14     |        1        \n",
      "===============================================================================================\n",
      "Final results\n",
      "    Number of steps = 38\n",
      "    Relative error = 2.1530739078389696e-07\n",
      "    Accuracy =  99.99998 %\n"
     ]
    }
   ],
   "source": [
    "# Compute the CPD of T with refinement.\n",
    "options.display = 3\n",
    "options.init_method = 'random'\n",
    "options.refine = True\n",
    "factors, T_approx, output = tfx.cpd(T, r, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum number of iterations and tolerance\n",
    "\n",
    "As the names suggest, *maxiter* is the maximum number of iterations permitted, whereas *tol* is the tolerance parameter, which is used to make stopping criteria. Both values are related in the sense we should increase *maxiter* when we decrease *tol*. The default value for the tolerance is *tol* = $10^{-12}$, but this not the actual value used. Once the program starts, it reassign the tolerance to $\\min \\{ 10^{-2}, 10^{-12}mnp \\}$, where $m, n, p$ are the dimensions of the original tensor $T$. For example, one of the most relevant stopping criteria is the improvement of the relative error criterium. Let $T_{approx}^{(k)}$ be the approximation computed at the $k$-th iteration. Then the program stops if \n",
    "$$ \\left| \\frac{\\| T - T_{approx}^{(k)} \\|}{\\| T \\|} - \\frac{\\| T - T_{approx}^{(k-1)} \\|}{\\| T \\|} \\right| <  \\verb|tol|,$$\n",
    "where this tolerance is the new one, not the input one. Keep this in mind when giving tolerances for the program. Now let's decrease *tol* and see if we get better approximations for the CPD of this example. We use *tol* = 1e-16 and keep the rest with default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------\n",
      "Computing MLSVD\n",
      "    No compression detected\n",
      "    Working with dimensions (2, 2, 2)\n",
      "    Compression relative error = 0.000000e+00\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Type of initialization: random\n",
      "    Initial guess relative error = 1.028796e+00\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing CPD\n",
      "    Iteration | Rel error | Improvement | norm(grad) | Predicted error | # Inner iterations\n",
      "        1     | 9.34e-01  |  9.34e-01   |  7.21e+00  |    2.95e-01     |        2        \n",
      "        2     | 5.60e-01  |  3.74e-01   |  8.58e+00  |    1.83e-02     |        3        \n",
      "        3     | 2.28e-01  |  3.32e-01   |  3.82e+01  |    5.70e-03     |        3        \n",
      "        4     | 1.45e-01  |  8.26e-02   |  1.24e+01  |    4.35e-04     |        3        \n",
      "        5     | 1.06e-01  |  3.91e-02   |  6.01e+00  |    5.70e-04     |        4        \n",
      "        6     | 8.73e-02  |  1.91e-02   |  2.43e+00  |    9.03e-04     |        4        \n",
      "        7     | 7.06e-02  |  1.67e-02   |  1.19e+00  |    1.05e-02     |        5        \n",
      "        8     | 5.26e-02  |  1.80e-02   |  7.20e-01  |    2.53e-02     |        6        \n",
      "        9     | 3.65e-02  |  1.61e-02   |  5.04e-01  |    1.48e-03     |        5        \n",
      "       10     | 2.46e-02  |  1.19e-02   |  4.70e-01  |    9.53e-03     |        4        \n",
      "       11     | 1.16e-02  |  1.30e-02   |  7.11e-01  |    1.58e-04     |        8        \n",
      "       12     | 7.07e-03  |  4.51e-03   |  2.79e-01  |    1.35e-05     |        7        \n",
      "       13     | 5.23e-03  |  1.83e-03   |  1.46e-01  |    4.02e-06     |        8        \n",
      "       14     | 4.06e-03  |  1.17e-03   |  1.06e-01  |    9.35e-06     |        12       \n",
      "       15     | 2.97e-03  |  1.09e-03   |  1.22e-01  |    3.20e-05     |        9        \n",
      "       16     | 1.69e-03  |  1.28e-03   |  1.02e-01  |    9.85e-05     |        6        \n",
      "       17     | 1.92e-03  |  2.26e-04   |  3.37e-02  |    1.73e-05     |        6        \n",
      "       18     | 9.91e-04  |  9.25e-04   |  8.59e-02  |    1.29e-05     |        7        \n",
      "       19     | 1.09e-03  |  1.01e-04   |  3.17e-02  |    2.90e-06     |        6        \n",
      "       20     | 5.39e-04  |  5.53e-04   |  4.66e-02  |    5.67e-08     |        12       \n",
      "       21     | 3.63e-04  |  1.76e-04   |  2.14e-02  |    7.47e-08     |        6        \n",
      "       22     | 2.21e-04  |  1.42e-04   |  1.30e-02  |    6.54e-08     |        9        \n",
      "       23     | 1.68e-04  |  5.31e-05   |  8.47e-03  |    1.09e-09     |        14       \n",
      "       24     | 1.24e-04  |  4.34e-05   |  6.38e-03  |    1.29e-10     |        8        \n",
      "       25     | 9.94e-05  |  2.49e-05   |  5.19e-03  |    8.14e-10     |        10       \n",
      "       26     | 7.48e-05  |  2.46e-05   |  4.06e-03  |    1.63e-11     |        18       \n",
      "       27     | 6.04e-05  |  1.43e-05   |  3.10e-03  |    7.04e-14     |        16       \n",
      "       28     | 4.89e-05  |  1.15e-05   |  2.48e-03  |    7.69e-14     |        15       \n",
      "       29     | 3.99e-05  |  9.00e-06   |  2.05e-03  |    2.44e-15     |        17       \n",
      "       30     | 3.11e-05  |  8.86e-06   |  1.67e-03  |    8.70e-11     |        7        \n",
      "       31     | 2.37e-05  |  7.36e-06   |  1.31e-03  |    3.73e-13     |        13       \n",
      "       32     | 1.94e-05  |  4.35e-06   |  9.78e-04  |    1.02e-16     |        17       \n",
      "       33     | 1.58e-05  |  3.59e-06   |  8.01e-04  |    1.85e-16     |        17       \n",
      "       34     | 1.29e-05  |  2.87e-06   |  6.59e-04  |    7.39e-16     |        16       \n",
      "       35     | 1.06e-05  |  2.35e-06   |  5.40e-04  |    4.82e-16     |        16       \n",
      "       36     | 8.47e-06  |  2.09e-06   |  4.43e-04  |    5.00e-12     |        12       \n",
      "       37     | 6.67e-06  |  1.80e-06   |  3.54e-04  |    1.76e-12     |        12       \n",
      "       38     | 5.35e-06  |  1.33e-06   |  2.77e-04  |    1.83e-16     |        16       \n",
      "       39     | 4.16e-06  |  1.18e-06   |  2.22e-04  |    1.00e-12     |        8        \n",
      "       40     | 3.23e-06  |  9.34e-07   |  1.74e-04  |    3.53e-16     |        15       \n",
      "       41     | 2.64e-06  |  5.87e-07   |  1.34e-04  |    7.68e-16     |        14       \n",
      "       42     | 2.05e-06  |  5.89e-07   |  1.09e-04  |    2.56e-13     |        8        \n",
      "       43     | 1.59e-06  |  4.58e-07   |  8.61e-05  |    3.37e-16     |        14       \n",
      "       44     | 1.30e-06  |  2.90e-07   |  6.60e-05  |    1.88e-16     |        14       \n",
      "       45     | 1.06e-06  |  2.40e-07   |  5.41e-05  |    1.55e-16     |        14       \n",
      "       46     | 8.71e-07  |  1.93e-07   |  4.45e-05  |    1.09e-16     |        14       \n",
      "       47     | 6.79e-07  |  1.92e-07   |  3.65e-05  |    3.41e-14     |        9        \n",
      "       48     | 5.21e-07  |  1.58e-07   |  2.83e-05  |    5.18e-15     |        8        \n",
      "       49     | 4.20e-07  |  1.01e-07   |  2.16e-05  |    6.29e-16     |        13       \n",
      "       50     | 3.36e-07  |  8.40e-08   |  1.73e-05  |    5.34e-15     |        11       \n",
      "       51     | 2.62e-07  |  7.33e-08   |  1.41e-05  |    1.04e-15     |        11       \n",
      "       52     | 2.05e-07  |  5.73e-08   |  1.09e-05  |    7.42e-16     |        10       \n",
      "       53     | 1.58e-07  |  4.68e-08   |  8.50e-06  |    5.41e-16     |        8        \n",
      "       54     | 1.23e-07  |  3.56e-08   |  6.55e-06  |    7.59e-16     |        7        \n",
      "       55     | 9.47e-08  |  2.80e-08   |  5.08e-06  |    4.11e-16     |        7        \n",
      "       56     | 7.32e-08  |  2.15e-08   |  3.91e-06  |    2.59e-16     |        7        \n",
      "       57     | 5.46e-08  |  1.86e-08   |  3.03e-06  |    7.94e-16     |        5        \n",
      "       58     | 4.04e-08  |  1.42e-08   |  2.27e-06  |    2.91e-16     |        5        \n",
      "       59     | 3.06e-08  |  9.78e-09   |  1.69e-06  |    7.17e-16     |        4        \n",
      "       60     | 2.28e-08  |  7.78e-09   |  1.31e-06  |    3.49e-16     |        4        \n",
      "       61     | 1.68e-08  |  6.07e-09   |  9.79e-07  |    6.27e-16     |        3        \n",
      "       62     | 1.21e-08  |  4.71e-09   |  7.00e-07  |    2.05e-16     |        3        \n",
      "       63     | 7.10e-09  |  4.97e-09   |  4.96e-07  |    6.10e-16     |        2        \n",
      "       64     | 4.07e-09  |  3.03e-09   |  3.15e-07  |    8.32e-17     |        2        \n",
      "       65     | 2.43e-09  |  1.64e-09   |  1.69e-07  |    3.70e-16     |        1        \n",
      "       66     | 1.14e-09  |  1.29e-09   |  1.27e-07  |    2.57e-16     |        1        \n",
      "       67     | 3.94e-10  |  7.50e-10   |  5.67e-08  |    3.30e-17     |        1        \n",
      "       68     | 1.49e-10  |  2.45e-10   |  1.85e-08  |    1.24e-18     |        1        \n",
      "===============================================================================================\n",
      "Final results\n",
      "    Number of steps = 68\n",
      "    Relative error = 1.4855004253403815e-10\n",
      "    Accuracy =  100.0 %\n"
     ]
    }
   ],
   "source": [
    "# Compute the CPD of T with tol = 1e-16.\n",
    "options.refine = False\n",
    "options.tol = 1e-16\n",
    "factors, T_approx, output = tfx.cpd(T, r, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understad better how the tolerance influence the precision we can make a plot varying the tolerances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "errors = []\n",
    "options.display = 0\n",
    "tolerances = [1e-10, 1e-11, 1e-12, 1e-13, 1e-14, 1e-15, 1e-16, 1e-17, 1e-18, 1e-19]\n",
    "tolerances_str = ['1e-10', '1e-11', '1e-12', '1e-13', '1e-14', '1e-15', '1e-16', '1e-17', '1e-18', '1e-19']\n",
    "\n",
    "for tolerance in tolerances:\n",
    "    options.tol = tolerance\n",
    "    factors, T_approx, output = tfx.cpd(T, r, options)\n",
    "    errors.append(output.rel_error)\n",
    "    \n",
    "plt.figure(figsize=[10,4])\n",
    "plt.plot(tolerances_str, errors, 'ro')\n",
    "plt.title('Tolerance x Precision in Log10 scale')\n",
    "plt.xlabel('Tolerance')\n",
    "plt.ylabel('Relative error')\n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
