{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic options\n",
    "\n",
    "The *cpd* function has several options at disposal. Some of them may improve performance, precision or give insights about the tensor at hand. If you look at the source code, the first line of **cpd** is the following:\n",
    "\n",
    "    def cpd(T, r, options=False):\n",
    "\n",
    "The first action of the function **cpd** is to read the parameter $\\verb|options|$. When set to False, this function set the parameters to their default values. In order to change some of them the user needs to create the class $\\verb|options|$ and add the parameters of interest with their corresponding values. The default class with all the default parameters is showed below:\n",
    "\n",
    "    class options:\n",
    "        maxiter = 200  \n",
    "        tol = 1e-16\n",
    "        tol_step = 1e-6\n",
    "        tol_improv = 1e-6\n",
    "        tol_grad = 1e-6\n",
    "        tol_jump = 10\n",
    "        method = 'dGN'\n",
    "        inner_method = 'cg'\n",
    "        cg_maxiter = 100\n",
    "        cg_factor = 1\n",
    "        cg_tol = 1e-16\n",
    "        bi_method_parameters = ['als', 500, 1e-6] \n",
    "        initialization = 'random'\n",
    "        trunc_dims = 0\n",
    "        mlsvd_method = 'seq'\n",
    "        tol_mlsvd = 1e-16\n",
    "        init_damp = 1\n",
    "        refine = False\n",
    "        symm = False\n",
    "        factors_norm = 0\n",
    "        trials = 10\n",
    "        display = 0\n",
    "        epochs = 1\n",
    "        gpu = False\n",
    "        mkl_dot = True\n",
    "\n",
    "There are a lot of options, but don't worry, I will explain them one by one now. If you don't want to bother learning the details, be assured that all default values were obtained after a long and exhausting marathon of tests, with lots of different tensors. Of course we can't say these values will apply to any possible tensor, but you can learn more about these options as the necessity arises. The thing is, tensors are hard, very hard. Just there isn't a single algorithm which works for all of them, and very often you will need to make experimentation with the parameters in order to find the right combination for one specific tensor. In my experience, having several options to combine and tune works better than just having a few options like tolerance and number of iterations. \n",
    "\n",
    "The option *gpu* won't be covered yet because this one is not read. The option *mkl_dot* is specific for sparse tensors and will be covered in lecture 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import TensorFox as tfx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display\n",
    "\n",
    "There are four choices for the $\\verb|display|$ option: $-2,-1,0,1,2,3,4$. These options controls what the user can see during the computations (works as the *verbose* parameter, but I prefer the name $\\verb|display|$). In the previous lesson we let the defaults and there were no output whatsoever (because the display default is $0$).\n",
    "\n",
    " - $\\verb|display|$ $=0$ (default): show nothing on the screen.\n",
    "\n",
    " - $\\verb|display|$ $=1$: shows useful information about the principal stages of the computation. \n",
    "    \n",
    " - $\\verb|display|$ $=2$: shows everything the option $\\verb|display|$ $=1$ shows plus information about each iteration.\n",
    "    \n",
    " - $\\verb|display|$ $=3$ is special, it shows eveything the option $\\verb|display|$ $=2$ shows and also shows the relative error of the compressed tensor (the computation of this error is costly so avoid that for big tensors).\n",
    "    \n",
    " - $\\verb|display|$ $=4$ is almost equal to $\\verb|display|$ $=3$ but now there are more digits displayed on the screen ($\\verb|display|$ $=3$ is a \"clean\" version of $\\verb|displa|y$ $=4$, with less information). \n",
    "    \n",
    " - $\\verb|display|$ $=-1$ is a special option for it is reserved for tensors of order higher than $3$. \n",
    " \n",
    " - $\\verb|display|$ $=-2$ shows everything the option $\\verb|display|$ $=-1$ shows plus the error of the compressed tensor (very costly) and the errors of the tensor train approximation before and after the CPD.\n",
    "    \n",
    "The options $-1$ and $-2$ will be discussed in the lesson of advanced options. Now let's start creating our toy model tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensor.\n",
    "m = 2\n",
    "T = np.zeros((m, m, m))\n",
    "s = 0\n",
    "\n",
    "for k in range(m):\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            T[i,j,k] = s\n",
    "            s += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------\n",
      "Computing MLSVD\n",
      "    No compression detected\n",
      "    Working with dimensions (2, 2, 2)\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Type of initialization: random\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing CPD\n",
      "===============================================================================================\n",
      "Final results\n",
      "    Number of steps = 64\n",
      "    Relative error = 4.57548911182144e-15\n",
      "    Accuracy =  100.0 %\n"
     ]
    }
   ],
   "source": [
    "# Create class of options with display=1.\n",
    "class options:\n",
    "    display = 1\n",
    "\n",
    "# Compute the CPD of T with partial display.\n",
    "R = 3\n",
    "factors, output = tfx.cpd(T, R, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------\n",
      "Computing MLSVD\n",
      "    No compression detected\n",
      "    Working with dimensions (2, 2, 2)\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Type of initialization: random\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing CPD\n",
      "    Iteration | Rel error |  Step size  | Improvement | norm(grad) | Predicted error | # Inner iterations\n",
      "        1     | 1.09e+00  |  5.82e-01   |  1.09e+00   |  2.11e+01  |    2.82e+00     |        2        \n",
      "        2     | 1.00e+00  |  1.14e-01   |  9.42e-02   |  1.58e+01  |    3.13e-04     |        3        \n",
      "        3     | 6.75e-01  |  3.14e-01   |  3.25e-01   |  1.17e+01  |    4.48e-04     |        3        \n",
      "        4     | 1.98e-01  |  2.65e-01   |  4.77e-01   |  1.80e+01  |    2.64e-04     |        4        \n",
      "        5     | 1.24e-01  |  5.89e-02   |  7.42e-02   |  5.89e+00  |    1.29e-05     |        4        \n",
      "        6     | 6.43e-02  |  6.01e-02   |  5.98e-02   |  3.96e+00  |    3.16e-05     |        4        \n",
      "        7     | 3.04e-02  |  4.43e-02   |  3.39e-02   |  1.65e+00  |    9.80e-06     |        7        \n",
      "        8     | 2.60e-02  |  1.26e-02   |  4.41e-03   |  2.48e-01  |    3.59e-05     |        4        \n",
      "        9     | 1.31e-02  |  4.21e-02   |  1.29e-02   |  1.94e-01  |    4.42e-05     |        4        \n",
      "       10     | 7.42e-03  |  2.74e-02   |  5.67e-03   |  3.14e-01  |    2.38e-06     |        7        \n",
      "       11     | 1.41e-03  |  4.27e-03   |  6.01e-03   |  3.46e-01  |    1.81e-08     |        6        \n",
      "       12     | 6.36e-04  |  5.99e-04   |  7.73e-04   |  5.29e-02  |    1.67e-08     |        4        \n",
      "       13     | 3.09e-04  |  1.23e-03   |  3.27e-04   |  1.50e-02  |    1.74e-08     |        7        \n",
      "       14     | 7.31e-05  |  2.89e-04   |  2.36e-04   |  1.01e-02  |    8.81e-12     |        10       \n",
      "       15     | 2.08e-05  |  3.80e-05   |  5.24e-05   |  3.56e-03  |    1.66e-12     |        8        \n",
      "       16     | 6.73e-06  |  9.90e-06   |  1.40e-05   |  1.06e-03  |    4.57e-12     |        4        \n",
      "       17     | 1.93e-06  |  4.56e-06   |  4.80e-06   |  3.30e-04  |    5.27e-14     |        14       \n",
      "       18     | 5.04e-07  |  1.02e-06   |  1.43e-06   |  9.94e-05  |    2.02e-16     |        9        \n",
      "       19     | 1.39e-07  |  2.57e-07   |  3.65e-07   |  2.72e-05  |    2.60e-16     |        7        \n",
      "       20     | 4.15e-08  |  8.57e-08   |  9.79e-08   |  7.34e-06  |    2.12e-17     |        12       \n",
      "       21     | 1.05e-08  |  2.43e-08   |  3.10e-08   |  2.17e-06  |    8.79e-21     |        13       \n",
      "       22     | 3.26e-09  |  5.23e-09   |  7.23e-09   |  5.67e-07  |    2.38e-20     |        17       \n",
      "       23     | 9.22e-10  |  1.66e-09   |  2.34e-09   |  1.75e-07  |    8.00e-21     |        7        \n",
      "       24     | 3.16e-10  |  4.83e-10   |  6.06e-10   |  4.90e-08  |    1.07e-21     |        10       \n",
      "       25     | 1.25e-10  |  3.19e-10   |  1.91e-10   |  1.60e-08  |    4.36e-23     |        14       \n",
      "       26     | 3.43e-11  |  6.44e-11   |  9.03e-11   |  5.85e-09  |    2.91e-24     |        14       \n",
      "       27     | 9.45e-12  |  1.68e-11   |  2.48e-11   |  1.66e-09  |    2.16e-24     |        5        \n",
      "       28     | 3.72e-12  |  5.55e-12   |  5.73e-12   |  4.62e-10  |    3.62e-25     |        6        \n",
      "       29     | 2.15e-12  |  6.76e-12   |  1.57e-12   |  1.50e-10  |    5.49e-29     |        16       \n",
      "       30     | 5.55e-13  |  1.12e-12   |  1.60e-12   |  8.90e-11  |    7.00e-28     |        13       \n",
      "       31     | 1.66e-13  |  2.53e-13   |  3.88e-13   |  2.24e-11  |    1.18e-31     |        21       \n",
      "       32     | 4.43e-14  |  8.46e-14   |  1.22e-13   |  6.65e-12  |    2.00e-31     |        23       \n",
      "       33     | 1.21e-14  |  2.02e-14   |  3.22e-14   |  1.86e-12  |    2.29e-30     |        9        \n",
      "       34     | 3.92e-13  |  0.00e+00   |  3.80e-13   |  5.58e-13  |    9.49e-29     |        22       \n",
      "       35     | 4.06e-13  |  1.24e-13   |  1.40e-14   |  2.03e-11  |    8.55e-30     |        6        \n",
      "       36     | 2.19e-09  |  6.62e-09   |  2.19e-09   |  2.11e-11  |    1.81e-24     |        22       \n",
      "       37     | 3.17e-15  |  7.02e-10   |  2.19e-09   |  1.52e-07  |    1.03e-30     |        16       \n",
      "       38     | 1.82e-15  |  6.70e-15   |  1.35e-15   |  4.75e-14  |    1.29e-32     |        12       \n",
      "       39     | 1.90e-13  |  2.51e-12   |  1.88e-13   |  8.49e-14  |    4.98e-28     |        16       \n",
      "       40     | 6.17e-15  |  1.14e-13   |  1.84e-13   |  5.23e-12  |    8.29e-30     |        11       \n",
      "       41     | 1.83e-15  |  5.71e-15   |  4.34e-15   |  1.21e-13  |    1.95e-31     |        6        \n",
      "       42     | 9.12e-12  |  5.04e-11   |  9.11e-12   |  2.49e-14  |    8.84e-28     |        29       \n",
      "       43     | 1.43e-09  |  0.00e+00   |  1.42e-09   |  6.07e-10  |    1.32e-21     |        28       \n",
      "       44     | 1.41e-09  |  7.06e-10   |  1.86e-11   |  3.51e-08  |    6.99e-30     |        13       \n",
      "       45     | 3.94e-11  |  7.07e-10   |  1.37e-09   |  3.46e-08  |    6.27e-28     |        23       \n",
      "       46     | 1.23e-10  |  1.57e-10   |  8.34e-11   |  1.78e-09  |    1.33e-26     |        30       \n",
      "       47     | 4.36e-11  |  3.46e-10   |  7.93e-11   |  6.70e-09  |    2.87e-27     |        27       \n",
      "       48     | 8.16e-12  |  1.90e-11   |  3.54e-11   |  1.49e-09  |    5.44e-31     |        24       \n",
      "       49     | 2.29e-12  |  4.25e-12   |  5.87e-12   |  3.98e-10  |    4.16e-32     |        27       \n",
      "       50     | 7.42e-13  |  1.15e-12   |  1.55e-12   |  1.16e-10  |    5.07e-29     |        33       \n",
      "       51     | 2.08e-13  |  3.60e-13   |  5.34e-13   |  3.71e-11  |    1.75e-29     |        10       \n",
      "       52     | 9.60e-14  |  2.46e-13   |  1.12e-13   |  1.09e-11  |    2.67e-29     |        24       \n",
      "       53     | 2.38e-14  |  4.19e-14   |  7.22e-14   |  4.48e-12  |    5.63e-32     |        11       \n",
      "       54     | 7.05e-15  |  1.17e-14   |  1.68e-14   |  1.29e-12  |    1.27e-31     |        15       \n",
      "       55     | 7.73e-08  |  2.65e-07   |  7.73e-08   |  3.85e-13  |    1.12e-23     |        36       \n",
      "       56     | 3.80e-11  |  2.38e-06   |  7.73e-08   |  5.00e-06  |    1.11e-21     |        36       \n",
      "       57     | 6.46e-10  |  0.00e+00   |  6.08e-10   |  1.57e-09  |    4.27e-26     |        30       \n",
      "       58     | 6.51e-10  |  2.02e-10   |  4.52e-12   |  3.40e-08  |    1.37e-31     |        18       \n",
      "       59     | 6.06e-05  |  0.00e+00   |  6.06e-05   |  3.39e-08  |    8.40e-21     |        36       \n",
      "       60     | 6.20e-05  |  1.99e-05   |  1.33e-06   |  3.19e-03  |    2.09e-29     |        27       \n",
      "       61     | 4.43e-06  |  1.99e-05   |  5.75e-05   |  3.29e-03  |    4.92e-13     |        8        \n",
      "       62     | 9.08e-07  |  2.16e-06   |  3.52e-06   |  1.79e-04  |    1.34e-29     |        42       \n",
      "       63     | 2.73e-07  |  4.15e-07   |  6.35e-07   |  3.56e-05  |    8.10e-30     |        43       \n",
      "       64     | 7.25e-08  |  1.42e-07   |  2.00e-07   |  1.12e-05  |    3.67e-26     |        34       \n",
      "===============================================================================================\n",
      "Final results\n",
      "    Number of steps = 64\n",
      "    Relative error = 1.816007431746494e-15\n",
      "    Accuracy =  100.0 %\n"
     ]
    }
   ],
   "source": [
    "# Compute the CPD of T with full display.\n",
    "options.display = 2\n",
    "factors, output = tfx.cpd(T, R, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------\n",
      "Computing MLSVD\n",
      "    No compression detected\n",
      "    Working with dimensions (2, 2, 2)\n",
      "    Compression relative error = 5.105819e-16\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Type of initialization: random\n",
      "    Initial guess relative error = 1.010904e+00\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing CPD\n",
      "    Iteration | Rel error |  Step size  | Improvement | norm(grad) | Predicted error | # Inner iterations\n",
      "        1     | 8.56e-01  |  8.86e-02   |  8.56e-01   |  3.13e+01  |    3.84e-03     |        2        \n",
      "        2     | 5.64e-01  |  2.08e-01   |  2.92e-01   |  1.97e+01  |    9.41e-04     |        3        \n",
      "        3     | 1.73e-01  |  2.49e-01   |  3.91e-01   |  1.26e+01  |    1.30e-02     |        3        \n",
      "        4     | 6.63e-02  |  6.28e-02   |  1.07e-01   |  5.07e+00  |    3.94e-04     |        3        \n",
      "        5     | 5.76e-02  |  1.55e-02   |  8.67e-03   |  1.25e+00  |    6.56e-05     |        3        \n",
      "        6     | 5.46e-02  |  2.59e-02   |  3.01e-03   |  3.04e-01  |    6.52e-05     |        4        \n",
      "        7     | 5.17e-02  |  2.28e-02   |  2.89e-03   |  1.87e-01  |    1.27e-04     |        5        \n",
      "        8     | 3.90e-02  |  7.74e-02   |  1.28e-02   |  2.58e-01  |    1.74e-04     |        4        \n",
      "        9     | 2.97e-02  |  3.49e-02   |  9.31e-03   |  6.15e-01  |    2.00e-04     |        5        \n",
      "       10     | 1.87e-02  |  6.95e-02   |  1.10e-02   |  3.19e-01  |    1.24e-04     |        5        \n",
      "       11     | 9.38e-03  |  9.55e-03   |  9.29e-03   |  4.64e-01  |    6.53e-06     |        4        \n",
      "       12     | 4.98e-03  |  2.07e-02   |  4.40e-03   |  4.63e-02  |    1.30e-05     |        6        \n",
      "       13     | 9.12e-04  |  1.08e-02   |  4.07e-03   |  6.10e-02  |    6.43e-07     |        10       \n",
      "       14     | 1.66e-04  |  7.45e-04   |  7.46e-04   |  1.82e-02  |    1.31e-09     |        8        \n",
      "       15     | 1.26e-04  |  1.77e-04   |  4.02e-05   |  9.65e-04  |    4.69e-09     |        8        \n",
      "       16     | 8.49e-05  |  1.37e-04   |  4.11e-05   |  1.81e-03  |    3.11e-09     |        4        \n",
      "       17     | 1.51e-05  |  2.36e-04   |  6.98e-05   |  2.06e-03  |    2.18e-11     |        11       \n",
      "       18     | 4.62e-07  |  8.69e-06   |  1.46e-05   |  6.60e-04  |    4.20e-15     |        10       \n",
      "       19     | 2.04e-07  |  2.63e-07   |  2.58e-07   |  1.16e-05  |    1.57e-15     |        5        \n",
      "       20     | 1.91e-07  |  4.43e-08   |  1.25e-08   |  1.24e-06  |    2.41e-15     |        6        \n",
      "       21     | 3.14e-08  |  5.66e-07   |  1.60e-07   |  1.48e-06  |    3.45e-17     |        14       \n",
      "       22     | 2.24e-09  |  4.97e-08   |  2.91e-08   |  1.21e-06  |    9.48e-24     |        15       \n",
      "       23     | 7.41e-11  |  1.24e-09   |  2.17e-09   |  1.00e-07  |    3.77e-22     |        10       \n",
      "       24     | 7.00e-12  |  3.39e-11   |  6.71e-11   |  3.19e-09  |    1.17e-23     |        8        \n",
      "       25     | 1.17e-12  |  2.03e-11   |  5.83e-12   |  9.75e-11  |    1.89e-26     |        15       \n",
      "       26     | 2.22e-14  |  5.75e-13   |  1.15e-12   |  5.01e-11  |    1.37e-30     |        17       \n",
      "       27     | 1.91e-15  |  2.25e-14   |  2.03e-14   |  4.90e-13  |    5.86e-32     |        14       \n",
      "       28     | 1.26e-15  |  9.05e-15   |  6.53e-16   |  5.71e-14  |    5.47e-32     |        11       \n",
      "       29     | 1.42e-15  |  1.16e-14   |  1.61e-16   |  1.60e-14  |    4.11e-31     |        7        \n",
      "       30     | 1.47e-15  |  4.24e-15   |  4.53e-17   |  3.20e-14  |    2.92e-30     |        7        \n",
      "       31     | 5.41e-15  |  7.00e-14   |  3.94e-15   |  3.29e-14  |    9.18e-30     |        14       \n",
      "       32     | 5.06e-14  |  7.25e-13   |  4.52e-14   |  1.63e-13  |    1.98e-28     |        17       \n",
      "       33     | 1.20e-08  |  0.00e+00   |  1.20e-08   |  2.05e-12  |    3.53e-23     |        22       \n",
      "       34     | 1.20e-08  |  3.83e-09   |  1.17e-11   |  5.67e-07  |    1.94e-31     |        21       \n",
      "       35     | 7.04e-11  |  3.82e-09   |  1.20e-08   |  5.67e-07  |    2.45e-21     |        11       \n",
      "       36     | 3.28e-12  |  6.11e-11   |  6.71e-11   |  2.09e-09  |    4.00e-30     |        23       \n",
      "       37     | 1.50e-13  |  2.19e-12   |  3.13e-12   |  1.44e-10  |    1.08e-30     |        25       \n",
      "       38     | 6.78e-15  |  1.04e-13   |  1.43e-13   |  6.57e-12  |    1.03e-31     |        14       \n",
      "       39     | 2.01e-08  |  0.00e+00   |  2.01e-08   |  2.93e-13  |    9.95e-23     |        27       \n",
      "       40     | 2.00e-08  |  7.08e-09   |  1.32e-10   |  8.92e-07  |    3.70e-27     |        17       \n",
      "       41     | 4.23e-10  |  6.84e-09   |  1.95e-08   |  8.90e-07  |    4.71e-20     |        6        \n",
      "       42     | 4.52e-11  |  9.01e-10   |  3.78e-10   |  1.01e-08  |    2.83e-27     |        27       \n",
      "       43     | 1.96e-12  |  2.62e-11   |  4.33e-11   |  2.02e-09  |    3.36e-30     |        17       \n",
      "       44     | 7.84e-14  |  1.17e-12   |  1.88e-12   |  8.12e-11  |    2.76e-30     |        19       \n",
      "       45     | 4.90e-11  |  1.22e-09   |  4.89e-11   |  2.89e-12  |    8.41e-26     |        26       \n",
      "       46     | 6.55e-14  |  2.61e-11   |  4.89e-11   |  2.16e-09  |    7.62e-27     |        22       \n",
      "       47     | 7.63e-12  |  1.43e-10   |  7.57e-12   |  2.09e-12  |    1.91e-26     |        21       \n",
      "       48     | 1.30e-13  |  2.81e-12   |  7.50e-12   |  3.48e-10  |    2.33e-27     |        8        \n",
      "       49     | 9.15e-14  |  1.41e-13   |  3.82e-14   |  1.57e-12  |    1.34e-27     |        7        \n",
      "       50     | 1.39e-14  |  2.75e-13   |  7.76e-14   |  8.59e-13  |    6.51e-31     |        27       \n",
      "       51     | 1.23e-07  |  0.00e+00   |  1.23e-07   |  6.47e-13  |    3.53e-22     |        24       \n",
      "       52     | 1.23e-07  |  4.28e-08   |  1.32e-10   |  5.27e-06  |    1.66e-20     |        10       \n",
      "       53     | 1.06e-09  |  4.29e-08   |  1.22e-07   |  5.27e-06  |    7.17e-27     |        35       \n",
      "       54     | 4.91e-11  |  6.60e-10   |  1.01e-09   |  3.54e-08  |    8.86e-26     |        18       \n",
      "       55     | 2.70e-12  |  2.66e-11   |  4.64e-11   |  2.26e-09  |    6.93e-26     |        15       \n",
      "       56     | 1.83e-13  |  1.67e-12   |  2.52e-12   |  1.23e-10  |    2.61e-29     |        34       \n",
      "       57     | 3.05e-12  |  0.00e+00   |  2.87e-12   |  8.06e-12  |    3.37e-26     |        36       \n",
      "       58     | 1.94e-06  |  9.84e-05   |  1.94e-06   |  1.14e-10  |    2.30e-21     |        31       \n",
      "       59     | 9.48e-11  |  1.07e-06   |  1.94e-06   |  6.99e-05  |    6.80e-21     |        25       \n",
      "       60     | 2.31e-12  |  7.49e-11   |  9.25e-11   |  2.97e-09  |    2.66e-32     |        31       \n",
      "       61     | 5.78e-14  |  1.53e-12   |  2.25e-12   |  5.55e-11  |    8.23e-28     |        16       \n",
      "       62     | 2.64e-15  |  4.97e-14   |  5.51e-14   |  1.93e-12  |    1.43e-31     |        23       \n",
      "       63     | 8.07e-13  |  0.00e+00   |  8.05e-13   |  8.44e-14  |    4.33e-27     |        20       \n",
      "       64     | 8.03e-13  |  6.53e-13   |  4.51e-15   |  3.12e-11  |    4.65e-29     |        28       \n",
      "===============================================================================================\n",
      "Final results\n",
      "    Number of steps = 64\n",
      "    Relative error = 1.5334403815092284e-15\n",
      "    Accuracy =  100.0 %\n"
     ]
    }
   ],
   "source": [
    "# Compute the CPD of T with full display plus relative error of compression and initial guess.\n",
    "options.display = 3\n",
    "factors, output = tfx.cpd(T, R, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between display $2$ and $3$ is only the initial guess relative error, which is given when $\\verb|display|$ $=3$. This is the error $\\|T - T_{approx}^{(0)}\\| / \\|T\\|$, where $T_{approx}^{(0)}$ is the starting point of the iterations. Sometimes it can be useful to know if the starting point is too far away or not from the objective tensor. Since the computation of this error is very costly, I've made this as an extra option. \n",
    "\n",
    "Let $(X^{(k)}, Y^{(k)}, Z^{(k)})$ be the approximated factor matrices at the $k$-th iteration and define the error function \n",
    "$$F(w^{(k)}) = \\frac{1}{2} \\| T - T^{(k)}_{approx} \\|^2,$$ \n",
    "where $w^{(k)} = vec( vec(X^{(k)}), vec(Y^{(k)}), vec(Z^{(k)}) )$ is the vectorization of $(X^{(k)}, Y^{(k)}, Z^{(k)})$ and $T_{approx}^{(k)} = T^{(k)}_{approx}(X, Y, Z)$ is its corresponding coordinate tensor. Below there are the description of each output column.\n",
    "\n",
    " - **Iteration**: it just the numbering of the iterations.\n",
    " \n",
    " - **Rel error**: the relative error between the current approximation and the objective tensor, i.e., the value\n",
    " \n",
    " $$ \\frac{\\| T - T_{approx}^{(k)} \\|}{\\| T \\|},$$\n",
    " where $k$ is the numbering of the current iteration.\n",
    " \n",
    " - **Step size**: the distance between two consecutives CPDs, i.e., it is the value \n",
    " \n",
    " $$ \\frac{\\| w^{(k)} - w^{(k-1)} \\|}{\\| w^{(k)} \\|}.$$\n",
    "\n",
    " \n",
    " - **Improvement**: the difference (in absolute value) between two consecutive errors, i.e., the value\n",
    " \n",
    " $$\\left| \\frac{\\| T - T_{approx}^{(k-1)} \\|}{\\| T \\|} - \\frac{\\| T - T_{approx}^{(k)} \\|}{\\| T \\|} \\right|.$$ \n",
    "\n",
    " - **norm(grad)**: the original problem can be regarded as a nonlinear least squares problem, and a minimizer is also a critical point, so it is of interest to keep track of the infinite norm of the gradient (the value $\\| \\nabla F(w^{(k)}) \\|_\\infty$) to check if it is approaching zero. \n",
    " \n",
    " - **Predicted error**: each iteration tries to minimize a linear model of the original problem. After we compute such a minimizer we have a error of this model which is expected to be close to the original one. In this case we are keeping track of the absolute error. This error is important for updating the damping parameter. For more about this linear model check the section about the damping parameter in the next lesson.\n",
    " \n",
    " - **# Inner iterations**: the linear model mentioned above is solved by the [conjugate gradient](https://en.wikipedia.org/wiki/Conjugate_gradient_method) method. There are parameters to control the error tolerance and number of iterations of this method. We talk more about this at the *advanced options* lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum number of iterations and tolerance\n",
    "\n",
    "As the names suggest, $\\verb|maxiter|$ is the maximum number of iterations permitted, whereas $\\verb|tol|$ is the tolerance parameter, which is used to make stopping criteria. Both values are related in the sense we should increase *maxiter* when we decrease $\\verb|tol|$. One can note that this is not the only tolerance parameter. All parameters $\\verb|tol|, \\ \\verb|tol|\\_\\verb|step|, \\ \\verb|tol|\\_\\verb|improv|, \\ \\verb|tol|\\_\\verb|grad|$ are used at each iteration as stopping conditions. The program stops if \n",
    "\n",
    "$1) \\hspace{1cm} \\displaystyle \\frac{\\| T - T_{approx}^{(k)} \\|}{\\| T \\|} <  \\verb|tol| \\hspace{6.6cm} (\\text{relative error})$\n",
    "\n",
    "$2) \\hspace{1cm} \\displaystyle \\frac{\\| w^{(k)} - w^{(k-1)}\\|}{\\| w^{(k)} \\|} < \\verb|tol|\\_\\verb|step| \\hspace{5.6cm} (\\text{step size})$\n",
    "\n",
    "$3) \\hspace{1cm} \\displaystyle \\left| \\frac{\\| T - T_{approx}^{(k)} \\|}{\\| T \\|} - \\frac{\\| T - T_{approx}^{(k-1)} \\|}{\\| T \\|} \\right| <  \\verb|tol|\\_\\verb|improv| \\hspace{1.5cm} (\\text{relative improvement})$\n",
    "\n",
    "$4) \\hspace{1cm} \\displaystyle \\| \\nabla F(w^{(k)}) \\|_\\infty < \\verb|tol|\\_\\verb|grad| \\hspace{5.6cm}(\\text{gradient norm})$ \n",
    "\n",
    "There are three extra special stopping conditions not mentioned above. The first one considers the evolution of the average error. If this average starts to increase, the program stops. This prevents the program to oscillate without decreasing substantially. The second one compares the average improvement with the average error. If the average improvement is too much smaller than the error, then the program stops. This prevents the program to continue itearting when negligible improvements are being made. Finally, the third one verifies if the error is too big, in the case of divergence.\n",
    "\n",
    "Let's decrease all tolerances to $10^{-12}$ and see if we get better approximations for the CPD of this example. We keep the rest with default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------\n",
      "Computing MLSVD\n",
      "    No compression detected\n",
      "    Working with dimensions (2, 2, 2)\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Type of initialization: random\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing CPD\n",
      "===============================================================================================\n",
      "Final results\n",
      "    Number of steps = 64\n",
      "    Relative error = 6.910445598485606e-16\n",
      "    Accuracy =  100.0 %\n"
     ]
    }
   ],
   "source": [
    "# Compute the CPD of T with tol = 1e-12.\n",
    "options.display = 1\n",
    "options.tol = 1e-12\n",
    "options.tol_step = 1e-12\n",
    "options.tol_improv = 1e-12\n",
    "options.tol_grad = 1e-12\n",
    "factors, output = tfx.cpd(T, R, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understad better how the tolerance influence the precision we can make a plot varying the tolerances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm4AAAEWCAYAAADfMRsiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAhh0lEQVR4nO3deZhkdX3v8ffHQcRhYFxQjBGmZZMoihHUiN6wiJFcQfMkqGiHSETnYjSbMYleDCHGuSZeY4yBLCPB0TgCgsaA8bri4IYKKCDghjADKAZBGJZBXPjeP85pqOnppXq6qruq5v16nnqmz6/O8v3W6eU7v/M7v5OqQpIkSYPvAYsdgCRJkrpj4SZJkjQkLNwkSZKGhIWbJEnSkLBwkyRJGhIWbpIkSUPCwk0ackkOSXLDYscxSpLsnuTOJEtmWW88ySf6cPz/keRbvd7vqEqyJsmbFzsOaSFYuEkDpC0WJl73Jrm7Y3l8seMbBO0f6Z+0n8mPknwyyb69PEZVXVdVy6rq57Ost7aqfq2Xx273+7mqetzWbJvkuCSf73VMSVYn+Vb7fXncFO//cZIfJNmY5PQkD+p1DJIs3KSB0hYLy6pqGXAdcFRH29peHy/Jdr3e5wJ5a/sZPQa4CVgzeYU0/B3XO5cBvwd8dfIbSZ4LvB54NjAG7AH81UIGJ20r/KUmDYEkD0ryjiTfb1/vmK5HI8mjk3wwyQ+TXJvkDzreOznJOUnel+R24LgkT0tyYZLbktyY5JQk23dsU0lOSPKdJLcmOTVJOt5/ZZJvJLkjyVVJnjJbHJPi3T7JpUl+v11ekuQLSU6a7XOpqk3A+4H92m3XJVmV5AvAJmCPJPu2vXI/anuMXtRx7Acn+bskG9qeos+3bWNt3tu16x2X5Jo2x2snej8n924lOSjJRe2+LkpyUMd765L8dZvbHUk+kWSXaT6TzS5/J1mf5HVJLm/3fVaSHWb7fKbY70zxPTbJZ9vYPtWe5/d1fNanVtWngR9PseuXAf9WVVdW1a3AXwPHTRNDkvx9kpvaOC5PMnH+pjwf7Xtn5/4evc8mecIMeR7Zfk/dluSLSZ40189KGlQWbtJwOBH4FeDJwP7A04A3Tl4pTQ/TeTS9I79I0wPyR2l6RCa8ADgHeAiwFvg58MfALsAz2m1+b9KujwSe2h77RcBz2+O9EDgZ+B1gZ+D5wC1dxgFAVf0E+G3gTUl+iabnZgmwarYPJckyYBz4WkfzscBKYCfgh8AnaYq7RwIvAf6p44/+24ADgIOAhwF/Btw76Rg7Au8Efr2qdmrXvXSKWB4G/Fe77sOBtwP/leThHau9FPjdNpbtgdfNlmOHFwFHAI8FnsQ0hdF0uojv/cBX2vdOpvkcu/UEmnM94TJg10m5T/g14FeBfWi+B18M3NK+N9P5+H/A3jSf3VdpvnenyvMpwOnA/2pz+Vfg3HjpViPCwk0aDuPAm6rqpqr6Ic1lqKn+sD4VeERVvamqflJV1wDvAo7pWOfCqvpwVd1bVXdX1SVV9aWq+llVraf5Q3fwpP3+TVXdVlXXAZ+hKSABXkFz2fKialxdVRu6jOM+VXUF8GbgP2iKmWNnGV/2uiS3AVcDy9i8iFnT9vz8jKbQWV9V727z+yrwQeDotrh8OfCHVfW9qvp5VX2xqu6Z4nj3AvsleXBV3VhVV06xzvOA71TVv7fHOgP4JnBUxzrvrqpvV9XdwAc6PsduvLOqvl9VP6Ipiuey7YzxJdmd5pyd1J6vzwPnzmHfy4CNHcsTX+80xbo/bdv3BVJV36iqG2c7H1V1elXd0S6fDOyfZPkU+38l8K9V9eV2H+8B7qH5j4809CzcpOHwaGBDx/KGtm2yFcCj20tEt7XFzf8Gdu1Y5/rODZLsk+Qj7WWo24H/Q9P71ukHHV9vovlDDbAb8N2tjGOy99CMj/poVX1nhvUA3lZVD6mqR1XV86uqM4bO/FYAT58UxzjwKJocd5gm/vtU1V00vUInADcm+a9MfTPE5HNEu/yLHcvTfY7dmM+2MHN8jwZ+1F56nnA93buTpsd1wsTXd0xesarOB04BTgX+O81NDzszw/lIc/n8b5J8t/0eXd++NdWl5hXAn0w657sx9c+LNHQs3KTh8H2aP0gTdm/bJrseuLYtaiZeO1XV/+xYpyZt8880PS97V9XONAVW6M71wJ5bGcdk/wR8BHhukmd1efypdOZ3PXDBpDiWVdWrgJtpxmtNFf/mO6z6eFU9B/gFms/qXVOsNvkcQXOevrc1SfTBTPHdCDwsydKO93abw76vpLmMPmF/4L+r6papVq6qd1bVATSXWPcB/pSZz8dLaS7xHw4spynwYerv0+uBVZPO+dK2h1EaehZu0nA4A3hjkke0A9pPAt43xXpfAW5P8uftQO8lSfZL8tQZ9r0TcDtwZ9uT9Ko5xHUazWXLA9pB53slWTHXOJIcSzO26TjgD4D3tOPX5usjwD5Jjk3ywPb11CS/VFX30oyFenuaGymWJHnG5LFQSXZN8vx2rNs9NL1LU13G/Wh7rJcm2S7Ji4HHtzEstCTZofM1U3zt5e2LgZPT3CzyDDa/xDtxE8kONMXSA9v9TvwNeS9wfJLHJ3kozfjLNdME9tQkT0/yQOAummLt57Ocj51oPvtbgKU0vcLTeRdwQnuMJNkxyfOSTHXZVho6Fm7ScHgzzR/Wy4Gv0wzO3mLC0XZc2FE045+upenFOI2ml2I6r6Pp0biD5o/eWd0GVVVn09xE8P52+w8DD5tLHO34qncAv1NVd1bV+9tc/77bOGaI7w6awfDH0PQ4/QD4W2CiOHsdzed5EfCj9r3JvxcfAPxJu/2PaMb/Tb55g7Z36ch23VtoBtYfWVU3zzePrXAQcPek18ZZ4hunuTnlFprvrbNoiqUJn2j3cxCwuv36VwGq6mPAW2nGP25oX385TWw703yf3dqudwvNTQkw/fl4b7vu94CrgC9Nl3hVXUwzzu2U9hhXM8cbOaRBlqrJV00kSdu6JGcB36yq6QowSYvAHjdJ0sQlzD2TPCDJETRjyj68yGFJmmRYZ02XJPXWo4AP0cx9dgPwqqr62sybSFpoXiqVJEkaEl4qlSRJGhLbxKXSXXbZpcbGxvp+nLvuuosdd9yx78fpt1HJA8xlUI1KLqOSB5jLoBqVXEYlD1iYXC655JKbq+oRU723TRRuY2NjXHzxxX0/zrp16zjkkEP6fpx+G5U8wFwG1ajkMip5gLkMqlHJZVTygIXJJcnkp5zcx0ulkiRJQ8LCTZIkaUhYuEmSJA0JCzdJkqQhYeEmSZI0JCzcJC2ctWthbIyDDzsMxsaaZUlS17aJ6UAkDYC1a2HlSti0iQBs2NAsA4yPL2ZkkjQ07HGTtDBOPBE2bdq8bdOmpl2S1BULN0kL47rr5tYuSdqChZukhbH77nNrlyRtwcJN0sJYtQqWLt28benSpl2S1BULN0kLY3wcVq+GFSuoBFasaJa9MUGSumbhJmnhjI/D+vVccP75sH69RZskzZGFmyRJ0pCwcJMkSRoSFm6SJElDwsJNkiRpSFi4SZIkDQkLN0mSpCFh4SZJkjQkLNwkSZKGhIWbJEnSkLBwkyRJGhIWbpIkSUNi4Au3JHsk+bck50xq3zHJJUmOXKzYJEmSFlJfC7ckpye5KckVk9qPSPKtJFcnef1M+6iqa6rq+Cne+nPgA72MV5IkaZBt1+f9rwFOAd470ZBkCXAq8BzgBuCiJOcCS4C3TNr+5VV10+SdJjkcuArYoT9hS5IkDZ6+Fm5V9dkkY5OanwZcXVXXACQ5E3hBVb0F6Pay56HAjsDjgbuTfLSq7u1R2JIkSQMpVdXfAzSF20eqar92+WjgiKp6Rbt8LPD0qnrNNNs/HFhF00N3WlvgTbx3HHBzVX1kiu1WAisBdt111wPOPPPMXqY1pTvvvJNly5b1/Tj9Nip5gLkMqlHJZVTyAHMZVKOSy6jkAQuTy6GHHnpJVR041Xv9vlQ6lUzRNm31WFW3ACdM896aGbZbDawGOPDAA+uQQw6ZU5BbY926dSzEcfptVPIAcxlUo5LLqOQB5jKoRiWXUckDFj+Xxbir9AZgt47lxwDfX4Q4JEmShspiFG4XAXsneWyS7YFjgHMXIQ5JkqSh0u/pQM4ALgQel+SGJMdX1c+A1wAfB74BfKCqruxnHJIkSaOg33eVvmSa9o8CH+3nsSVJkkbNwD85YT6SHJVk9caNGxc7FEmSpHkb6cKtqs6rqpXLly9f7FAkSZLmbaQLN0mSpFFi4SZJkjQkLNwkSZKGhIWbJEnSkLBwkyRJGhIWbpIkSUNipAs353GTJEmjZKQLN+dxkyQNpbVrYWyMgw87DMbGmmWJPj/ySpIkzdHatbByJWzaRAA2bGiWAcbHFzMyDYCR7nGTJGnonHgibNq0edumTU27tnkWbpIkDZLrrptbu7YpFm6SJA2S3XefW7u2KRZukiQNklWrYOnSzduWLm3atc2zcJMkaZCMj8Pq1bBiBZXAihXNsjcmiBEv3JzHTZI0lMbHYf16Ljj/fFi/3qJN9xnpws153CRJ0igZ6cJNkiRplFi4SZIkDQkLN0mSpCFh4SZJkjQkLNwkSZKGhIWbJEnSkLBwkyRJGhIWbpIkSUNipAs3n5wgSZJGyUgXbj45QZIkjZKRLtwkSZJGiYWbJEnSkLBwkyRJGhIWbpIkSUPCwk2SJGlIWLhJkiQNCQs3SZKkIWHhJkmSNCQs3CRJkobESBduPvJKkiSNkpEu3HzklSRJGiUjXbhJkiSNEgs3SZKkITFj4ZZkSZI/XqhgJEmSNL0ZC7eq+jnwggWKRZIkSTPYrot1vpDkFOAs4K6Jxqr6at+ikiRJ0ha6KdwOav99U0dbAYf1PhxJkiRNZ9bCraoOXYhAJEmSNLNZ7ypNsjzJ25Nc3L7+LokTo0mSJC2wbqYDOR24A3hR+7odeHc/g5IkSdKWuhnjtmdV/VbH8l8lubRP8UiSJGka3fS43Z3kWRMLSZ4J3N2/kCRJkjSVbnrcTgDe2zGu7VbgZf0LSZIkSVOZsXBLsgT47araP8nOAFV1+4JE1gNJjgKO2muvvRY7FEmSpHnr5skJB7Rf3z5MRRtAVZ1XVSuXL/cmWEmSNPy6uVT6tSTnAmez+ZMTPtS3qCRJkrSFbgq3hwG3sPmTEgqwcJMkSVpA3Yxxu7mq/nSB4pEkSdI0uhnj9pQFikWSJEkz6OZS6aWOcZMkSVp8jnGTJEkaErMWblX1uwsRiCRJkmY26yOvkuyT5NNJrmiXn5Tkjf0PTZIkSZ26eVbpu4A3AD8FqKrLgWP6GZQkSZK21E3htrSqvjKp7Wf9CEaSJEnT66ZwuznJnjQ3JJDkaODGvkYlSZKkLXRzV+mrgdXAvkm+B1wLjPc1KkmSJG2hm7tKrwEOT7Ij8ICquqP/YUmSJGmybnrcAKiqu2ZfS5IkSf3SzRg3SZIkDQALN0mSpCHRzQS8S5P8RZJ3tct7Jzmy/6HNX5KjkqzeuHHjYociSZI0b930uL0buAd4Rrt8A/DmvkXUQ1V1XlWtXL58+WKHIkmSNG/dFG57VtVbuf/JCXcD6WtUkiRJ2kI3hdtPkjyY+yfg3ZOmB06SJEkLqJvpQE4GPgbslmQt8EzguD7GJEmSpCl0MwHvJ5JcAvwKzSXSP6yqm/semSRJkjYza+GW5FzgDOBcJ+GVJElaPN2Mcfs74H8AVyU5O8nRSXboc1ySJEmapJtLpRcAFyRZAhwGvBI4Hdi5z7FJkiSpQ1fPKm3vKj0KeDHwFOA9/QxKkiRJW+pmjNtZwNNp7iw9FVhXVff2OzBJkiRtrtsnJ+xZVSdU1fkWbZI0QtauhbExDj7sMBgba5YlDaxpe9ySHFZV5wNLgRckmz8soao+1OfYJEn9tHYtrFwJmzY1j8PZsKFZBhgfX8zIJE1jpkulBwPn04xtm6wACzdJGmYnngibNm3etmlT027hJg2kaQu3qvrL9ss3VdW1ne8leWxfo5Ik9d91182tXdKi62aM2wenaDun14FIkhbY7rvPrV3SoptpjNu+wBOA5Ul+s+OtnQEn4JWkYbdq1X1j3O6zdGnTLmkgzTTG7XHAkcBD2Hyc2x00k/BKkobZxDi2E0+krruO7L57U7Q5vk0aWDONcftP4D+TPKOqLlzAmCRJC2V8HMbHuWDdOg455JDFjkbSLLp5csLXkrya5rLpfZdIq+rlfYtKkiRJW+jm5oR/Bx4FPBe4AHgMzeVSSZIkLaBuCre9quovgLuq6j3A84An9jcsSZIkTdZN4fbT9t/bkuwHLAfG+haRJEmSptTNGLfVSR4K/AVwLrAMOKmvUUmSJGkLsxZuVXVa++UFwB79DUeSJEnTmWkC3tfOtGFVvb334fRWkqOAo/baa6/FDkWSJGneZhrjttMsr4FXVedV1crly5cvdiiSJEnzNtMEvH+1kIFIkiRpZrPeVZpknySfTnJFu/ykJG/sf2iSJEnq1M10IO8C3kA7LUhVXQ4c08+gJEmStKVuCrelVfWVSW0/60cwkiRJml43hdvNSfYECiDJ0cCNfY1KkiRJW+hmAt5XA6uBfZN8D7gWGO9rVJIkSdpCNxPwXgMcnmRHmh66u4EXAxv6HJskSZI6THupNMnOSd6Q5JQkzwE2AS8DrgZetFABSpIkqTFTj9u/A7cCFwKvBP4M2B74jaq6tP+hSZIkqdNMhdseVfVEgCSnATcDu1fVHQsSmSRJkjYz012lP534oqp+Dlxr0SZJkrR4Zupx2z/J7e3XAR7cLgeoqtq579FJkiTpPjM9q3TJQgYiSZKkmXUzAa8kSZIGgIWbJEnSkLBwkyRJGhIWbpIkSUPCwk2SJGlIWLj1wtq1MDbGwYcdBmNjzbIkSVKPzfqQec1i7VpYuRI2bSIAGzY0ywDj44sZmSRJGjH2uM3XiSfCpk2bt23a1LRLkiT1kIXbfF133dzaJUmStpKF23ztvvvc2iVJkraShdt8rVoFS5du3rZ0adMuSZLUQxZu8zU+DqtXw4oVVAIrVjTL3pggSZJ6zMKtF8bHYf16Ljj/fFi/3qJNveV0M5KkltOBSIPM6WYkSR3scZMGmdPNSJI6WLhJg8zpZiRJHSzcpEHmdDOSpA4Wbrqfg+AHj9PNSJI6WLipMTEIfsMGUnX/IHiLt8XldDOSpA4Wbmo4CH5wOd2MJKll4aaGg+AlSRp4Fm5qOAhekqSBZ+GmhoPgJUkaeBZuajgIXpKkgWfhpvs5CF6SpIE28IVbkj2S/FuSczraDknyuST/kuSQxYtOkiRp4fS1cEtyepKbklwxqf2IJN9KcnWS18+0j6q6pqqOn9wM3AnsANzQ26glSZIG03Z93v8a4BTgvRMNSZYApwLPoSm6LkpyLrAEeMuk7V9eVTdNsd/PVdUFSXYF3g54TU+SJI28vhZuVfXZJGOTmp8GXF1V1wAkORN4QVW9BTiyy/3e2355K/CgHoUrSZI00FJV/T1AU7h9pKr2a5ePBo6oqle0y8cCT6+q10yz/cOBVTQ9dKdV1VuS/CbwXOAhwD9X1boptlsJrATYddddDzjzzDN7nNmW7rzzTpYtW9b34/TbqOQB5jKoRiWXUckDzGVQjUouo5DHIz/1KfY47TQedNNN3PPIR3LNK17BTYcf3pdjHXrooZdU1YFTvllVfX0BY8AVHcsvpCnAJpaPBf6xnzEccMABtRA+85nPLMhx+m1U8qgyl0E1KrmMSh5V5jKoRiWXoc/jfe+rWrq0Cu5/LV3atPcBcHFNU9Msxl2lNwC7dSw/Bvj+IsQhSZI0uwF6nvdiFG4XAXsneWyS7YFjgHMXIQ5JkqTZDdDzvPs9HcgZwIXA45LckOT4qvoZ8Brg48A3gA9U1ZX9jEOSJGmrDdDzvPt9V+lLpmn/KPDRfh5bkiSpJ1atgpUrN79cukjP8x74JyfMR5KjkqzeuHHjYociSZKG1QA9z3ukC7eqOq+qVi5fvnyxQ5EkScNsQJ7nPdKFmyRJ0iixcJMkSRoSFm6SJElDwsJNkiRpSFi4SZIkDQkLN0maq7VrYWyMgw87DMbGmmUtPs+LtgF9nYB3sSU5Cjhqr732WuxQJI2KtWvvm4gzABs2NMuwaNMDCM+Lthkj3ePmPG6Sem6AHjatDp4XbSNGunCTpJ4boIdNq4PnRdsICzdJmosBeti0OnhetI2wcJOkuVi1qnm4dKdFeti0OnhetI2wcJOkuRigh02rg+dF2wgLN0maqwF52LQm8bxoG2DhJkmS+sO59XrOedwkSVLvObdeX4x0j5vzuEmStEicW68vRrpwkyRJi8S59frCwk2SJPWec+v1hYWbJEnqPefW6wsLN0mS1HvOrdcXFm6SJKk/nFuv5yzcJEmShoSFmyRJ0pCwcNNocrZuSdII8skJGj3O1i1JGlEj3ePmkxO2Uc7WLUkaUSNduGkb5WzdkqQRZeGm0eNs3ZKkEWXhptHjbN2SpBFl4abR42zdkqQRZeGm0eRs3ZKkEWThJkmSNCQs3CRJkoaEhZskSdKQsHCTJEkaEiNduCU5KsnqjRs3LnYokiRJ85aqWuwY+i7JD4ENC3CoXYCbF+A4/TYqeYC5DKpRyWVU8gBzGVSjksuo5AELk8uKqnrEVG9sE4XbQklycVUduNhxzNeo5AHmMqhGJZdRyQPMZVCNSi6jkgcsfi4jfalUkiRplFi4SZIkDQkLt95avdgB9Mio5AHmMqhGJZdRyQPMZVCNSi6jkgcsci6OcZMkSRoS9rhJkiQNCQs3SZKkIWHh1oUkpye5KckVW7Ht9klWJ/l2km8m+a1+xNhlLFuVR5Kdklza8bo5yTv6FGa3Mc3nnLwkydeTXJ7kY0l26UeMc4hnPrm8uM3jyiRv7Ud8sxx/PrGvSnJ9kjsntT8oyVlJrk7y5SRjPQt45nj6kcuvJvlqkp8lObp30c4aTz9yeW2Sq9rvt08nWdG7iGeMpx+5nND+Drg0yeeTPL53EU8bS8/z6Hj/6CSVZEGmqOjTOTkuyQ87/s68oncRzxhPX85Lkhe1Py9XJnl/b6JtWLh1Zw1wxFZueyJwU1XtAzweuKBXQW2FNWxFHlV1R1U9eeJFM5nxh3oc21ytYStySbId8A/AoVX1JOBy4DW9DW3O1rB1uTwc+L/As6vqCcCuSZ7d49hms4at/9k4D3jaFO3HA7dW1V7A3wN/u5X7n6s19D6X64DjgJ7+4u7CGnqfy9eAA9ufm3OAhfqPwhp6n8v7q+qJ7e+ztwJv38r9z8Uaep8HSXYC/gD48lbue2usoQ+5AGd1/K05bSv3P1dr6HEuSfYG3gA8s/3d/EdbG9xULNy6UFWfBX7U2ZZkz7a35pIkn0uy7zSbvxx4S7ufe6tq0WaOnmceE+vvDTwS+FwfQ53VPHJJ+9oxSYCdge/3P+LpzSOXPYBvV9UP2+VPAQvaozuf76mq+lJV3TjFWy8A3tN+fQ7w7PZc9VU/cqmq9VV1OXBvf6KeWp9y+UxVbWoXvwQ8pueBTx1PP3K5vWNxR6Dvd+n16WcF4K9pis8f9zbi6fUxlwXXp1xeCZxaVbe2693U66B9dfECxoArOpY/Dezdfv104PwptnkIcD3N/+a+CpwN7DpseUza/iTgbYt9PuaTC3A0cDtwI/BZYMkw5gI8FLih3XY74IPAecMQ+6Tt75y0fAXwmI7l7wK7DGMuHe1rgKOH+bxMeu8U4I3DnAvw6vZ76/qJfQ1bHsAvAx9sv15H0yM6lOeEpmf6RpqrIOcAuw1xLh+mKaa/QPOfnCN6Ge92aM6SLAMOAs7u6Ah40BSrbkfzv9IvVNVrk7wWeBtw7IIEOos55NHpGAYk/k7d5pLkgcCraH7hXQP8I02X9psXJtLZdZtLVd2a5FXAWTQ9Ol+k6YVbNFv5PbXFbqZoW/B5i3qUy0DoZS5Jfhs4EDi4N9HN+fg9yaWqTgVOTfJS4I3Ay3oWZBfmm0eSB9AMJTiu58HNUY/OyXnAGVV1T5ITaHrdD+tdlN3pUS7bAXsDh9DUAJ9Lsl9V3daLGC3cts4DgNuqGR9xnyRLgEvaxXOBvwQ2Af/Rtp1NM35nUHSVR1Wd1LbvD2xXVZcweLo9J+cBVNV32/c/ALx+4cLsStfnparOo80pyUrg5wsZ6BTm9D01jRuA3YAb2jGJy5l0KWOB9CKXQdGTXJIcTjNu9+CquqcfgXah1+flTOCfexde1+abx07AfsC6tsB4FHBukudX1cX9CXla8z4nVXVLx+K7WLixrZP16nfYl6rqp8C1Sb5FU8hd1IsALdy2QlXdnuTaJC+sqrPb8TdPqqrLgCd3rpvkPJqq+3zg2cBVCx3vdOaSR+slwBkLGmSXus0lyaOBxyd5RDVjw54DfGNxop7aHL+/HllVNyV5KPB7wIsWIeT7bMX31FTOpen9uJDmsvb51V5/WEg9ymUg9CKXJL8M/CvNZZ/ejtmZgx7lsndVfaddfB7wnZnW74f55lFVG4H77ohPsg543SIUbb06J79Q948Xez6L9Hu5Rz/3H6b5e7kmzawF+9Bc4elZkL5mv/59Bs2195/SVNLHA48FPgZcRlOMnTTNtitoxlFdTnPdfPdhzKPd/hpg38U+Hz04JyfQ/FK4nKa36uFDnMsZ7ftXAccMWexvbbe5t/335LZ9B5re6auBrwB7DHEuT22X7wJuAa4c4lw+Bfw3cGn7OneIc/kH4Mo2j88ATxjGPCats44FGuPWp3PylvacXNaekwX5W9OnXEIztv0q4Ov0+Hezj7ySJEkaEk4HIkmSNCQs3CRJkoaEhZskSdKQsHCTJEkaEhZukiRJQ8LCTdJIS/LwJJe2rx8k+V7H8vaT1l2X5MDFilWSZuMEvJJGWjUzsj8ZIMnJNM8VfFsv9p1kSVUt9tMqJG1D7HGTtM1J8uwkX0vy9SSnJ5nquba/luTCJF9Ncnb7DEOSrE9yUpLPAy9M8sokFyW5LMkHkyxt11uT5J1JvpjkmiRHd+z7z9pjX5bkb9q2PZN8LMklST6XZN8F+jgkDRELN0nbmh2ANcCLq+qJNFceXtW5QvuYmjcCh1fVU4CLgdd2rPLjqnpWVZ0JfKiqnlpV+9M8kaPzecS/ADwLOBKYKNB+HfgN4OntNm9t110N/H5VHQC8DvinnmUsaWR4qVTStmYJcG1Vfbtdfg/wauAdHev8CvB44AvtA7y3p3l26oSzOr7eL8mbgYcAy4CPd7z34aq6F7gqya5t2+HAu6tqE0BV/ajtzTsIOLs9HsAWvYCSZOEmaVtzVxfrBPhkVb2ki32sAX6jqi5LchxwSMd790za58S/k581+ADgtqp6chexSdqGealU0rZmB2AsyV7t8rHABZPW+RLwzIl1kixNss80+9sJuDHJA4HxLo7/CeDlHWPhHlZVtwPXJnlh25Yk+88pK0nbBAs3SduaHwO/S3NZ8uvAvcC/dK5QVT8EjgPOSHI5TSE33c0CfwF8Gfgk8M3ZDl5VHwPOBS5OcinNeDZoir7jk1wGXAm8YE5ZSdompGpyj70kSZIGkT1ukiRJQ8LCTZIkaUhYuEmSJA0JCzdJkqQhYeEmSZI0JCzcJEmShoSFmyRJ0pD4/+5X1PKGfw19AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "errors = []\n",
    "options.display = 0\n",
    "tolerances = [1e-6, 1e-7, 1e-8, 1e-9, 1e-10, 1e-11, 1e-12, 1e-13, 1e-14, 1e-15, 1e-16]\n",
    "tolerances_str = ['1e-6','1e-7','1e-8','1e-9','1e-10','1e-11','1e-12','1e-13','1e-14','1e-15','1e-16']\n",
    "\n",
    "for tol in tolerances:\n",
    "    options.maxiter = 500\n",
    "    options.tol = tol\n",
    "    options.tol_step = tol\n",
    "    options.tol_improv = tol\n",
    "    options.tol_grad = tol\n",
    "    factors, output = tfx.cpd(T, R, options)\n",
    "    errors.append(output.rel_error)\n",
    "    \n",
    "plt.figure(figsize=[10,4])\n",
    "plt.plot(tolerances_str, errors, 'ro')\n",
    "plt.title('Tolerance x Precision in Log10 scale')\n",
    "plt.xlabel('Tolerance')\n",
    "plt.ylabel('Relative error')\n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
