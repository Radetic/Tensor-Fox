{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse tensors\n",
    "\n",
    "A new functionality of Tensor Fox is the support for sparse tensors. Let $T \\in \\mathbb{R}^{I_1 \\times \\ldots \\times I_L}$ ba a sparse tensor with *nnz* nonzero entries. The tensor is represented as a triple *[data, idxs, dims]*, where *data* is an array of size $nnz$ such that *data[i]* is the $i$-th nonzero entry of $T$, with corresponding index *idxs[i]*. It is necessary to pass *idxs* also as an array, which will be of shape $nnz \\times L$. Finally, we have that *dims* $= [I_1, \\ldots, I_L]$. \n",
    "\n",
    "Below we create a fourth order sparse tensor $10 \\times 10 \\times 10 \\times 10$ with only 6 nonzero entries. These entries are random and are located in random places of $T$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import TensorFox as tfx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial variables.\n",
    "nnz = 6\n",
    "n = 10\n",
    "dims = (n, n, n, n)\n",
    "L = len(dims)\n",
    "\n",
    "# Create the nonzero values of T.\n",
    "data = np.random.randn(nnz)\n",
    "\n",
    "# Create indexes.\n",
    "idxs = np.zeros((nnz, L), dtype=np.int64) \n",
    "for l in range(L):\n",
    "    idxs[:, l] = np.random.randint(0, dims[l], size=(nnz))\n",
    "\n",
    "# Define sparse tensor.\n",
    "T = [data, idxs, dims]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we prefer the damped Gauss-Newton method to compute the CPD, but the Tensor Train CPD also works. We remark that it is not possible to use $\\verb|tol| \\_ \\verb|mlsvd| = 0$ or $-1$ since these options doesn't perform compression and to work with large tensors as $T$ we must always to compress. In the case the user sets $\\verb|display| = 3$ or $4$, the program computes only the error associated with the nonzero entries, otherwise we would face memory issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------\n",
      "Computing MLSVD\n",
      "    Compression detected\n",
      "    Compressing from (10, 10, 10, 10) to (4, 4, 5, 4)\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Type of initialization: random\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing CPD\n",
      "===============================================================================================\n",
      "Final results\n",
      "    Number of steps = 16\n",
      "    Relative error = 0.10687570830242643\n",
      "    Accuracy =  89.31243 %\n"
     ]
    }
   ],
   "source": [
    "class options:\n",
    "    display = 1\n",
    "    method = 'dGN'\n",
    "    \n",
    "R = 6\n",
    "factors, output = tfx.cpd(T, R, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relative error showed above only take in account the nonzero entries of $T$ (the same is valid for the compression error which is equal to zero in this example). The factor matrices of the decomposition may introduce small errors when approximating the zeros, and this small errors summed together does increase the actual error of the CPD. Thus the relative error showed above is a lower bound to the actual error, but usually it is close enough.\n",
    "\n",
    "Since this is a small example we can put everything in dense format and verify what is the actual error. However this won't be possible for really large tensors. Regardless, the main point of the CPD is to approximate the nonzero entries, which is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10687681769528798"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the coordinate (dense) format of the approximation.\n",
    "T_approx = tfx.cpd2tens(factors)\n",
    "\n",
    "# Generate the dense format from the sparse representation.\n",
    "T_dense = tfx.sparse2dense(data, idxs, dims)\n",
    "\n",
    "# Compute the error.\n",
    "np.linalg.norm(T_dense - T_approx)/np.linalg.norm(T_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a big reduction in memory cost when working with sparse representations. Below we show a graph with the maximum memory cost attained in the computation of the CPDs of sparse $n \\times n \\times n$ tensors (blue curve) vs. the cost to store these tensors in dense format. As we can see, the difference is substantial, Tensor Fox does avoid the intermediate memory explosion problem. For instance, the sparse approach requires $3648$ megabytes when $n = 30000$, whereas the dense approach requires approximately $205$ terabytes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sparse](sparse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of generating random coordinates of the tensor to be nonzero, we can generate sparse factor matrices and then the associated sparse tensor. For this all we need is to call the function *gen_rand_sparse_tensor* with inputs **dims**, **R**, **nnz** as showed below. In this example we consider a fourth order tensor with dimensions $100 \\times 100 \\times 100 \\times 100$, rank $R = 5$ and $nnz = 40$ nonzero entries in each factor matrix. We remark that the sparsity of the tensor is not necessarily equal to the sparsity of its factor matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factor matrix nonzero entries = 8.0 %\n",
      "Tensor nonzero entries = 0.92 %\n"
     ]
    }
   ],
   "source": [
    "# Rank of the tensor.\n",
    "R = 5\n",
    "\n",
    "# Dimensions of the tensor.\n",
    "dims = [100, 100, 100, 100]\n",
    "\n",
    "# Number of nonzero entries of each factor matrix. \n",
    "nnz = 40\n",
    "\n",
    "# Generate sparse tensor.\n",
    "print('Factor matrix nonzero entries =', round(100 * nnz / np.prod(dims[0] * R), 2), '%')\n",
    "data, idxs, dims, factors = tfx.gen_rand_sparse_tensor(dims, R, nnz)\n",
    "print('Tensor nonzero entries =', round(100 * len(data) / np.prod(dims), 2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we know the rank in advance so it is not necessary to guess. Note how the CPD is able to find a good approximation with less effort now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------\n",
      "Computing MLSVD\n",
      "    Compression detected\n",
      "    Compressing from [100 100 100 100] to (5, 5, 5, 5)\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Type of initialization: random\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing CPD\n",
      "===============================================================================================\n",
      "Final results\n",
      "    Number of steps = 169\n",
      "    Relative error = 1.3678985301695948e-09\n",
      "    Accuracy =  100.0 %\n"
     ]
    }
   ],
   "source": [
    "# Define sparse tensor.\n",
    "T = [data, idxs, dims]\n",
    "\n",
    "options.display = 1\n",
    "factors, output = tfx.cpd(T, R, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can convert everything to dense format and compute the exactly error. This example is not so big that won't fit in the computer memory, but usually one wants to avoid dense representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3816644777860003e-09"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the coordinate (dense) format of the approximation.\n",
    "T_approx = tfx.cpd2tens(factors)\n",
    "\n",
    "# Generate the dense format from the sparse representation.\n",
    "T_dense = tfx.sparse2dense(data, idxs, dims)\n",
    "\n",
    "# Compute the error.\n",
    "np.linalg.norm(T_dense - T_approx)/np.linalg.norm(T_dense)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
